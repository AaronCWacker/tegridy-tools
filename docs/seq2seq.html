<!doctype html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />

  
  <title>seq2seq API documentation</title>
  <meta name="description" content="" />


  <link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,300' rel='stylesheet' type='text/css'>
  
  <style type="text/css">
* {
  box-sizing: border-box;
}
/*! normalize.css v1.1.1 | MIT License | git.io/normalize */

/* ==========================================================================
   HTML5 display definitions
   ========================================================================== */

/**
 * Correct `block` display not defined in IE 6/7/8/9 and Firefox 3.
 */

article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
nav,
section,
summary {
    display: block;
}

/**
 * Correct `inline-block` display not defined in IE 6/7/8/9 and Firefox 3.
 */

audio,
canvas,
video {
    display: inline-block;
    *display: inline;
    *zoom: 1;
}

/**
 * Prevent modern browsers from displaying `audio` without controls.
 * Remove excess height in iOS 5 devices.
 */

audio:not([controls]) {
    display: none;
    height: 0;
}

/**
 * Address styling not present in IE 7/8/9, Firefox 3, and Safari 4.
 * Known issue: no IE 6 support.
 */

[hidden] {
    display: none;
}

/* ==========================================================================
   Base
   ========================================================================== */

/**
 * 1. Prevent system color scheme's background color being used in Firefox, IE,
 *    and Opera.
 * 2. Prevent system color scheme's text color being used in Firefox, IE, and
 *    Opera.
 * 3. Correct text resizing oddly in IE 6/7 when body `font-size` is set using
 *    `em` units.
 * 4. Prevent iOS text size adjust after orientation change, without disabling
 *    user zoom.
 */

html {
    background: #fff; /* 1 */
    color: #000; /* 2 */
    font-size: 100%; /* 3 */
    -webkit-text-size-adjust: 100%; /* 4 */
    -ms-text-size-adjust: 100%; /* 4 */
}

/**
 * Address `font-family` inconsistency between `textarea` and other form
 * elements.
 */

html,
button,
input,
select,
textarea {
    font-family: sans-serif;
}

/**
 * Address margins handled incorrectly in IE 6/7.
 */

body {
    margin: 0;
}

/* ==========================================================================
   Links
   ========================================================================== */

/**
 * Address `outline` inconsistency between Chrome and other browsers.
 */

a:focus {
    outline: thin dotted;
}

/**
 * Improve readability when focused and also mouse hovered in all browsers.
 */

a:active,
a:hover {
    outline: 0;
}

/* ==========================================================================
   Typography
   ========================================================================== */

/**
 * Address font sizes and margins set differently in IE 6/7.
 * Address font sizes within `section` and `article` in Firefox 4+, Safari 5,
 * and Chrome.
 */

h1 {
    font-size: 2em;
    margin: 0.67em 0;
}

h2 {
    font-size: 1.5em;
    margin: 0.83em 0;
}

h3 {
    font-size: 1.17em;
    margin: 1em 0;
}

h4 {
    font-size: 1em;
    margin: 1.33em 0;
}

h5 {
    font-size: 0.83em;
    margin: 1.67em 0;
}

h6 {
    font-size: 0.67em;
    margin: 2.33em 0;
}

/**
 * Address styling not present in IE 7/8/9, Safari 5, and Chrome.
 */

abbr[title] {
    border-bottom: 1px dotted;
}

/**
 * Address style set to `bolder` in Firefox 3+, Safari 4/5, and Chrome.
 */

b,
strong {
    font-weight: bold;
}

blockquote {
    margin: 1em 40px;
}

/**
 * Address styling not present in Safari 5 and Chrome.
 */

dfn {
    font-style: italic;
}

/**
 * Address differences between Firefox and other browsers.
 * Known issue: no IE 6/7 normalization.
 */

hr {
    -moz-box-sizing: content-box;
    box-sizing: content-box;
    height: 0;
}

/**
 * Address styling not present in IE 6/7/8/9.
 */

mark {
    background: #ff0;
    color: #000;
}

/**
 * Address margins set differently in IE 6/7.
 */

p,
pre {
    margin: 1em 0;
}

/**
 * Correct font family set oddly in IE 6, Safari 4/5, and Chrome.
 */

code,
kbd,
pre,
samp {
    font-family: monospace, serif;
    _font-family: 'courier new', monospace;
    font-size: 1em;
}

/**
 * Improve readability of pre-formatted text in all browsers.
 */

pre {
    white-space: pre;
    white-space: pre-wrap;
    word-wrap: break-word;
}

/**
 * Address CSS quotes not supported in IE 6/7.
 */

q {
    quotes: none;
}

/**
 * Address `quotes` property not supported in Safari 4.
 */

q:before,
q:after {
    content: '';
    content: none;
}

/**
 * Address inconsistent and variable font size in all browsers.
 */

small {
    font-size: 80%;
}

/**
 * Prevent `sub` and `sup` affecting `line-height` in all browsers.
 */

sub,
sup {
    font-size: 75%;
    line-height: 0;
    position: relative;
    vertical-align: baseline;
}

sup {
    top: -0.5em;
}

sub {
    bottom: -0.25em;
}

/* ==========================================================================
   Lists
   ========================================================================== */

/**
 * Address margins set differently in IE 6/7.
 */

dl,
menu,
ol,
ul {
    margin: 1em 0;
}

dd {
    margin: 0 0 0 40px;
}

/**
 * Address paddings set differently in IE 6/7.
 */

menu,
ol,
ul {
    padding: 0 0 0 40px;
}

/**
 * Correct list images handled incorrectly in IE 7.
 */

nav ul,
nav ol {
    list-style: none;
    list-style-image: none;
}

/* ==========================================================================
   Embedded content
   ========================================================================== */

/**
 * 1. Remove border when inside `a` element in IE 6/7/8/9 and Firefox 3.
 * 2. Improve image quality when scaled in IE 7.
 */

img {
    border: 0; /* 1 */
    -ms-interpolation-mode: bicubic; /* 2 */
}

/**
 * Correct overflow displayed oddly in IE 9.
 */

svg:not(:root) {
    overflow: hidden;
}

/* ==========================================================================
   Figures
   ========================================================================== */

/**
 * Address margin not present in IE 6/7/8/9, Safari 5, and Opera 11.
 */

figure {
    margin: 0;
}

/* ==========================================================================
   Forms
   ========================================================================== */

/**
 * Correct margin displayed oddly in IE 6/7.
 */

form {
    margin: 0;
}

/**
 * Define consistent border, margin, and padding.
 */

fieldset {
    border: 1px solid #c0c0c0;
    margin: 0 2px;
    padding: 0.35em 0.625em 0.75em;
}

/**
 * 1. Correct color not being inherited in IE 6/7/8/9.
 * 2. Correct text not wrapping in Firefox 3.
 * 3. Correct alignment displayed oddly in IE 6/7.
 */

legend {
    border: 0; /* 1 */
    padding: 0;
    white-space: normal; /* 2 */
    *margin-left: -7px; /* 3 */
}

/**
 * 1. Correct font size not being inherited in all browsers.
 * 2. Address margins set differently in IE 6/7, Firefox 3+, Safari 5,
 *    and Chrome.
 * 3. Improve appearance and consistency in all browsers.
 */

button,
input,
select,
textarea {
    font-size: 100%; /* 1 */
    margin: 0; /* 2 */
    vertical-align: baseline; /* 3 */
    *vertical-align: middle; /* 3 */
}

/**
 * Address Firefox 3+ setting `line-height` on `input` using `!important` in
 * the UA stylesheet.
 */

button,
input {
    line-height: normal;
}

/**
 * Address inconsistent `text-transform` inheritance for `button` and `select`.
 * All other form control elements do not inherit `text-transform` values.
 * Correct `button` style inheritance in Chrome, Safari 5+, and IE 6+.
 * Correct `select` style inheritance in Firefox 4+ and Opera.
 */

button,
select {
    text-transform: none;
}

/**
 * 1. Avoid the WebKit bug in Android 4.0.* where (2) destroys native `audio`
 *    and `video` controls.
 * 2. Correct inability to style clickable `input` types in iOS.
 * 3. Improve usability and consistency of cursor style between image-type
 *    `input` and others.
 * 4. Remove inner spacing in IE 7 without affecting normal text inputs.
 *    Known issue: inner spacing remains in IE 6.
 */

button,
html input[type="button"], /* 1 */
input[type="reset"],
input[type="submit"] {
    -webkit-appearance: button; /* 2 */
    cursor: pointer; /* 3 */
    *overflow: visible;  /* 4 */
}

/**
 * Re-set default cursor for disabled elements.
 */

button[disabled],
html input[disabled] {
    cursor: default;
}

/**
 * 1. Address box sizing set to content-box in IE 8/9.
 * 2. Remove excess padding in IE 8/9.
 * 3. Remove excess padding in IE 7.
 *    Known issue: excess padding remains in IE 6.
 */

input[type="checkbox"],
input[type="radio"] {
    box-sizing: border-box; /* 1 */
    padding: 0; /* 2 */
    *height: 13px; /* 3 */
    *width: 13px; /* 3 */
}

/**
 * 1. Address `appearance` set to `searchfield` in Safari 5 and Chrome.
 * 2. Address `box-sizing` set to `border-box` in Safari 5 and Chrome
 *    (include `-moz` to future-proof).
 */

input[type="search"] {
    -webkit-appearance: textfield; /* 1 */
    -moz-box-sizing: content-box;
    -webkit-box-sizing: content-box; /* 2 */
    box-sizing: content-box;
}

/**
 * Remove inner padding and search cancel button in Safari 5 and Chrome
 * on OS X.
 */

input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
    -webkit-appearance: none;
}

/**
 * Remove inner padding and border in Firefox 3+.
 */

button::-moz-focus-inner,
input::-moz-focus-inner {
    border: 0;
    padding: 0;
}

/**
 * 1. Remove default vertical scrollbar in IE 6/7/8/9.
 * 2. Improve readability and alignment in all browsers.
 */

textarea {
    overflow: auto; /* 1 */
    vertical-align: top; /* 2 */
}

/* ==========================================================================
   Tables
   ========================================================================== */

/**
 * Remove most spacing between table cells.
 */

table {
    border-collapse: collapse;
    border-spacing: 0;
}
</style>
  <style type="text/css">
  html, body {
    margin: 0;
    padding: 0;
    min-height: 100%;
  }
  body {
    background: #fff;
    font-family: "Source Sans Pro", "Helvetica Neueue", Helvetica, sans;
    font-weight: 300;
    font-size: 16px;
    line-height: 1.6em;
  }
  #content {
    width: 70%;
    max-width: 850px;
    float: left;
    padding: 30px 60px;
    border-left: 1px solid #ddd;
  }
  #sidebar {
    width: 25%;
    float: left;
    padding: 30px;
    overflow: hidden;
  }
  #nav {
    font-size: 130%;
    margin: 0 0 15px 0;
  }

  #top {
    display: block;
    position: fixed;
    bottom: 5px;
    left: 5px;
    font-size: .85em;
    text-transform: uppercase;
  }

  #footer {
    font-size: .75em;
    padding: 5px 30px;
    border-top: 1px solid #ddd;
    text-align: right;
  }
    #footer p {
      margin: 0 0 0 30px;
      display: inline-block;
    }

  h1, h2, h3, h4, h5 {
    font-weight: 300;
  }
  h1 {
    font-size: 2.5em;
    line-height: 1.1em;
    margin: 0 0 .50em 0;
  }

  h2 {
    font-size: 1.75em;
    margin: 1em 0 .50em 0;
  }

  h3 {
    margin: 25px 0 10px 0;
  }

  h4 {
    margin: 0;
    font-size: 105%;
  }

  a {
    color: #058;
    text-decoration: none;
    transition: color .3s ease-in-out;
  }

  a:hover {
    color: #e08524;
    transition: color .3s ease-in-out;
  }

  pre, code, .mono, .name {
    font-family: "Ubuntu Mono", "Cousine", "DejaVu Sans Mono", monospace;
  }

  .title .name {
    font-weight: bold;
  }
  .section-title {
    margin-top: 2em;
  }
  .ident {
    color: #900;
  }

  code {
    background: #f9f9f9;
  } 

  pre {
    background: #fefefe;
    border: 1px solid #ddd;
    box-shadow: 2px 2px 0 #f3f3f3;
    margin: 0 30px;
    padding: 15px 30px;
  }

  .codehilite {
    margin: 0 30px 10px 30px;
  }

    .codehilite pre {
      margin: 0;
    }
    .codehilite .err { background: #ff3300; color: #fff !important; } 

  table#module-list {
    font-size: 110%;
  }

    table#module-list tr td:first-child {
      padding-right: 10px;
      white-space: nowrap;
    }

    table#module-list td {
      vertical-align: top;
      padding-bottom: 8px;
    }

      table#module-list td p {
        margin: 0 0 7px 0;
      }

  .def {
    display: table;
  }

    .def p {
      display: table-cell;
      vertical-align: top;
      text-align: left;
    }

    .def p:first-child {
      white-space: nowrap;
    }

    .def p:last-child {
      width: 100%;
    }


  #index {
    list-style-type: none;
    margin: 0;
    padding: 0;
  }
    ul#index .class_name {
      /* font-size: 110%; */
      font-weight: bold;
    }
    #index ul {
      margin: 0;
    }

  .item {
    margin: 0 0 15px 0;
  }

    .item .class {
      margin: 0 0 25px 30px;
    }

      .item .class ul.class_list {
        margin: 0 0 20px 0;
      }

    .item .name {
      background: #fafafa;
      margin: 0;
      font-weight: bold;
      padding: 5px 10px;
      border-radius: 3px;
      display: inline-block;
      min-width: 40%;
    }
      .item .name:hover {
        background: #f6f6f6;
      }

    .item .empty_desc {
      margin: 0 0 5px 0;
      padding: 0;
    }

    .item .inheritance {
      margin: 3px 0 0 30px;
    }

    .item .inherited {
      color: #666;
    }

    .item .desc {
      padding: 0 8px;
      margin: 0;
    }

      .item .desc p {
        margin: 0 0 10px 0;
      }

    .source_cont {
      margin: 0;
      padding: 0;
    }

    .source_link a {
      background: #ffc300;
      font-weight: 400;
      font-size: .75em;
      text-transform: uppercase;
      color: #fff;
      text-shadow: 1px 1px 0 #f4b700;
      
      padding: 3px 8px;
      border-radius: 2px;
      transition: background .3s ease-in-out;
    }
      .source_link a:hover {
        background: #FF7200;
        text-shadow: none;
        transition: background .3s ease-in-out;
      }

    .source {
      display: none;
      max-height: 600px;
      overflow-y: scroll;
      margin-bottom: 15px;
    }

      .source .codehilite {
        margin: 0;
      }

  .desc h1, .desc h2, .desc h3 {
    font-size: 100% !important;
  }
  .clear {
    clear: both;
  }

  @media all and (max-width: 950px) {
    #sidebar {
      width: 35%;
    }
    #content {
      width: 65%;
    }
  }
  @media all and (max-width: 650px) {
    #top {
      display: none;
    }
    #sidebar {
      float: none;
      width: auto;
    }
    #content {
      float: none;
      width: auto;
      padding: 30px;
    }

    #index ul {
      padding: 0;
      margin-bottom: 15px;
    }
    #index ul li {
      display: inline-block;
      margin-right: 30px;
    }
    #footer {
      text-align: left;
    }
    #footer p {
      display: block;
      margin: inherit;
    }
  }

  /*****************************/
</style>
  <style type="text/css">
  pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.codehilite .hll { background-color: #ffffcc }
.codehilite { background: #f8f8f8; }
.codehilite .c { color: #408080; font-style: italic } /* Comment */
.codehilite .err { border: 1px solid #FF0000 } /* Error */
.codehilite .k { color: #008000; font-weight: bold } /* Keyword */
.codehilite .o { color: #666666 } /* Operator */
.codehilite .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.codehilite .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.codehilite .cp { color: #BC7A00 } /* Comment.Preproc */
.codehilite .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.codehilite .c1 { color: #408080; font-style: italic } /* Comment.Single */
.codehilite .cs { color: #408080; font-style: italic } /* Comment.Special */
.codehilite .gd { color: #A00000 } /* Generic.Deleted */
.codehilite .ge { font-style: italic } /* Generic.Emph */
.codehilite .gr { color: #FF0000 } /* Generic.Error */
.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.codehilite .gi { color: #00A000 } /* Generic.Inserted */
.codehilite .go { color: #888888 } /* Generic.Output */
.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.codehilite .gs { font-weight: bold } /* Generic.Strong */
.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.codehilite .gt { color: #0044DD } /* Generic.Traceback */
.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.codehilite .kp { color: #008000 } /* Keyword.Pseudo */
.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.codehilite .kt { color: #B00040 } /* Keyword.Type */
.codehilite .m { color: #666666 } /* Literal.Number */
.codehilite .s { color: #BA2121 } /* Literal.String */
.codehilite .na { color: #7D9029 } /* Name.Attribute */
.codehilite .nb { color: #008000 } /* Name.Builtin */
.codehilite .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.codehilite .no { color: #880000 } /* Name.Constant */
.codehilite .nd { color: #AA22FF } /* Name.Decorator */
.codehilite .ni { color: #999999; font-weight: bold } /* Name.Entity */
.codehilite .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.codehilite .nf { color: #0000FF } /* Name.Function */
.codehilite .nl { color: #A0A000 } /* Name.Label */
.codehilite .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */
.codehilite .nv { color: #19177C } /* Name.Variable */
.codehilite .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.codehilite .w { color: #bbbbbb } /* Text.Whitespace */
.codehilite .mb { color: #666666 } /* Literal.Number.Bin */
.codehilite .mf { color: #666666 } /* Literal.Number.Float */
.codehilite .mh { color: #666666 } /* Literal.Number.Hex */
.codehilite .mi { color: #666666 } /* Literal.Number.Integer */
.codehilite .mo { color: #666666 } /* Literal.Number.Oct */
.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */
.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */
.codehilite .sc { color: #BA2121 } /* Literal.String.Char */
.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */
.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */
.codehilite .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */
.codehilite .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.codehilite .sx { color: #008000 } /* Literal.String.Other */
.codehilite .sr { color: #BB6688 } /* Literal.String.Regex */
.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */
.codehilite .ss { color: #19177C } /* Literal.String.Symbol */
.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */
.codehilite .fm { color: #0000FF } /* Name.Function.Magic */
.codehilite .vc { color: #19177C } /* Name.Variable.Class */
.codehilite .vg { color: #19177C } /* Name.Variable.Global */
.codehilite .vi { color: #19177C } /* Name.Variable.Instance */
.codehilite .vm { color: #19177C } /* Name.Variable.Magic */
.codehilite .il { color: #666666 } /* Literal.Number.Integer.Long */
  </style>
  <style type="text/css">
/* ==========================================================================
   EXAMPLE Media Queries for Responsive Design.
   These examples override the primary ('mobile first') styles.
   Modify as content requires.
   ========================================================================== */

@media only screen and (min-width: 35em) {
    /* Style adjustments for viewports that meet the condition */
}

@media print,
       (-o-min-device-pixel-ratio: 5/4),
       (-webkit-min-device-pixel-ratio: 1.25),
       (min-resolution: 120dpi) {
    /* Style adjustments for high resolution devices */
}

/* ==========================================================================
   Print styles.
   Inlined to avoid required HTTP connection: h5bp.com/r
   ========================================================================== */

@media print {
    * {
        background: transparent !important;
        color: #000 !important; /* Black prints faster: h5bp.com/s */
        box-shadow: none !important;
        text-shadow: none !important;
    }

    a,
    a:visited {
        text-decoration: underline;
    }

    a[href]:after {
        content: " (" attr(href) ")";
    }

    abbr[title]:after {
        content: " (" attr(title) ")";
    }

    /*
     * Don't show links for images, or javascript/internal links
     */

    .ir a:after,
    a[href^="javascript:"]:after,
    a[href^="#"]:after {
        content: "";
    }

    pre,
    blockquote {
        border: 1px solid #999;
        page-break-inside: avoid;
    }

    thead {
        display: table-header-group; /* h5bp.com/t */
    }

    tr,
    img {
        page-break-inside: avoid;
    }

    img {
        max-width: 100% !important;
    }

    @page {
        margin: 0.5cm;
    }

    p,
    h2,
    h3 {
        orphans: 3;
        widows: 3;
    }

    h2,
    h3 {
        page-break-after: avoid;
    }
}
</style>

  <script type="text/javascript">
  function toggle(id, $link) {
    $node = document.getElementById(id);
    if (!$node)
    return;
    if (!$node.style.display || $node.style.display == 'none') {
    $node.style.display = 'block';
    $link.innerHTML = 'Hide source &nequiv;';
    } else {
    $node.style.display = 'none';
    $link.innerHTML = 'Show source &equiv;';
    }
  }
  </script>
</head>
<body>
<a href="#" id="top">Top</a>
<div id="container">
  


















  
  <div id="sidebar">
    <h1>Index</h1>
    <ul id="index">

    <li class="set"><h3><a href="#header-functions">Functions</a></h3>
      
  <ul>
    <li class="mono"><a href="#seq2seq.calculate_bleu">calculate_bleu</a></li>
    <li class="mono"><a href="#seq2seq.display_attention">display_attention</a></li>
    <li class="mono"><a href="#seq2seq.epoch_time">epoch_time</a></li>
    <li class="mono"><a href="#seq2seq.evaluate">evaluate</a></li>
    <li class="mono"><a href="#seq2seq.train">train</a></li>
    <li class="mono"><a href="#seq2seq.translate_sentence">translate_sentence</a></li>
  </ul>

    </li>

    <li class="set"><h3><a href="#header-classes">Classes</a></h3>
      <ul>
        <li class="mono">
        <span class="class_name"><a href="#seq2seq.Decoder">Decoder</a></span>
        
          
  <ul>
    <li class="mono"><a href="#seq2seq.Decoder.add_module">add_module</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.apply">apply</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.bfloat16">bfloat16</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.buffers">buffers</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.children">children</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.cpu">cpu</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.cuda">cuda</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.double">double</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.eval">eval</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.extra_repr">extra_repr</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.float">float</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.forward">forward</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.get_buffer">get_buffer</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.get_extra_state">get_extra_state</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.get_parameter">get_parameter</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.get_submodule">get_submodule</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.half">half</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.load_state_dict">load_state_dict</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.modules">modules</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.named_buffers">named_buffers</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.named_children">named_children</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.named_modules">named_modules</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.named_parameters">named_parameters</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.parameters">parameters</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.register_backward_hook">register_backward_hook</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.register_buffer">register_buffer</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.register_forward_hook">register_forward_hook</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.register_forward_pre_hook">register_forward_pre_hook</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.register_full_backward_hook">register_full_backward_hook</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.register_parameter">register_parameter</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.requires_grad_">requires_grad_</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.set_extra_state">set_extra_state</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.share_memory">share_memory</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.state_dict">state_dict</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.to">to</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.to_empty">to_empty</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.train">train</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.type">type</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.xpu">xpu</a></li>
    <li class="mono"><a href="#seq2seq.Decoder.zero_grad">zero_grad</a></li>
  </ul>

        </li>
        <li class="mono">
        <span class="class_name"><a href="#seq2seq.DecoderLayer">DecoderLayer</a></span>
        
          
  <ul>
    <li class="mono"><a href="#seq2seq.DecoderLayer.add_module">add_module</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.apply">apply</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.bfloat16">bfloat16</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.buffers">buffers</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.children">children</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.cpu">cpu</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.cuda">cuda</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.double">double</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.eval">eval</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.extra_repr">extra_repr</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.float">float</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.forward">forward</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.get_buffer">get_buffer</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.get_extra_state">get_extra_state</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.get_parameter">get_parameter</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.get_submodule">get_submodule</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.half">half</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.load_state_dict">load_state_dict</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.modules">modules</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.named_buffers">named_buffers</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.named_children">named_children</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.named_modules">named_modules</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.named_parameters">named_parameters</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.parameters">parameters</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.register_backward_hook">register_backward_hook</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.register_buffer">register_buffer</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.register_forward_hook">register_forward_hook</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.register_forward_pre_hook">register_forward_pre_hook</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.register_full_backward_hook">register_full_backward_hook</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.register_parameter">register_parameter</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.requires_grad_">requires_grad_</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.set_extra_state">set_extra_state</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.share_memory">share_memory</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.state_dict">state_dict</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.to">to</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.to_empty">to_empty</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.train">train</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.type">type</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.xpu">xpu</a></li>
    <li class="mono"><a href="#seq2seq.DecoderLayer.zero_grad">zero_grad</a></li>
  </ul>

        </li>
        <li class="mono">
        <span class="class_name"><a href="#seq2seq.Encoder">Encoder</a></span>
        
          
  <ul>
    <li class="mono"><a href="#seq2seq.Encoder.add_module">add_module</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.apply">apply</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.bfloat16">bfloat16</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.buffers">buffers</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.children">children</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.cpu">cpu</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.cuda">cuda</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.double">double</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.eval">eval</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.extra_repr">extra_repr</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.float">float</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.forward">forward</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.get_buffer">get_buffer</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.get_extra_state">get_extra_state</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.get_parameter">get_parameter</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.get_submodule">get_submodule</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.half">half</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.load_state_dict">load_state_dict</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.modules">modules</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.named_buffers">named_buffers</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.named_children">named_children</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.named_modules">named_modules</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.named_parameters">named_parameters</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.parameters">parameters</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.register_backward_hook">register_backward_hook</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.register_buffer">register_buffer</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.register_forward_hook">register_forward_hook</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.register_forward_pre_hook">register_forward_pre_hook</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.register_full_backward_hook">register_full_backward_hook</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.register_parameter">register_parameter</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.requires_grad_">requires_grad_</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.set_extra_state">set_extra_state</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.share_memory">share_memory</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.state_dict">state_dict</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.to">to</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.to_empty">to_empty</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.train">train</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.type">type</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.xpu">xpu</a></li>
    <li class="mono"><a href="#seq2seq.Encoder.zero_grad">zero_grad</a></li>
  </ul>

        </li>
        <li class="mono">
        <span class="class_name"><a href="#seq2seq.EncoderLayer">EncoderLayer</a></span>
        
          
  <ul>
    <li class="mono"><a href="#seq2seq.EncoderLayer.add_module">add_module</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.apply">apply</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.bfloat16">bfloat16</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.buffers">buffers</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.children">children</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.cpu">cpu</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.cuda">cuda</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.double">double</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.eval">eval</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.extra_repr">extra_repr</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.float">float</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.forward">forward</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.get_buffer">get_buffer</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.get_extra_state">get_extra_state</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.get_parameter">get_parameter</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.get_submodule">get_submodule</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.half">half</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.load_state_dict">load_state_dict</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.modules">modules</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.named_buffers">named_buffers</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.named_children">named_children</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.named_modules">named_modules</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.named_parameters">named_parameters</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.parameters">parameters</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.register_backward_hook">register_backward_hook</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.register_buffer">register_buffer</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.register_forward_hook">register_forward_hook</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.register_forward_pre_hook">register_forward_pre_hook</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.register_full_backward_hook">register_full_backward_hook</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.register_parameter">register_parameter</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.requires_grad_">requires_grad_</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.set_extra_state">set_extra_state</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.share_memory">share_memory</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.state_dict">state_dict</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.to">to</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.to_empty">to_empty</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.train">train</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.type">type</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.xpu">xpu</a></li>
    <li class="mono"><a href="#seq2seq.EncoderLayer.zero_grad">zero_grad</a></li>
  </ul>

        </li>
        <li class="mono">
        <span class="class_name"><a href="#seq2seq.MultiHeadAttentionLayer">MultiHeadAttentionLayer</a></span>
        
          
  <ul>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.add_module">add_module</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.apply">apply</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.bfloat16">bfloat16</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.buffers">buffers</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.children">children</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.cpu">cpu</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.cuda">cuda</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.double">double</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.eval">eval</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.extra_repr">extra_repr</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.float">float</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.forward">forward</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.get_buffer">get_buffer</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.get_extra_state">get_extra_state</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.get_parameter">get_parameter</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.get_submodule">get_submodule</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.half">half</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.load_state_dict">load_state_dict</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.modules">modules</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.named_buffers">named_buffers</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.named_children">named_children</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.named_modules">named_modules</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.named_parameters">named_parameters</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.parameters">parameters</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.register_backward_hook">register_backward_hook</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.register_buffer">register_buffer</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.register_forward_hook">register_forward_hook</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.register_forward_pre_hook">register_forward_pre_hook</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.register_full_backward_hook">register_full_backward_hook</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.register_parameter">register_parameter</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.requires_grad_">requires_grad_</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.set_extra_state">set_extra_state</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.share_memory">share_memory</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.state_dict">state_dict</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.to">to</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.to_empty">to_empty</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.train">train</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.type">type</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.xpu">xpu</a></li>
    <li class="mono"><a href="#seq2seq.MultiHeadAttentionLayer.zero_grad">zero_grad</a></li>
  </ul>

        </li>
        <li class="mono">
        <span class="class_name"><a href="#seq2seq.PositionwiseFeedforwardLayer">PositionwiseFeedforwardLayer</a></span>
        
          
  <ul>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.add_module">add_module</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.apply">apply</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.bfloat16">bfloat16</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.buffers">buffers</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.children">children</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.cpu">cpu</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.cuda">cuda</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.double">double</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.eval">eval</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.extra_repr">extra_repr</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.float">float</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.forward">forward</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.get_buffer">get_buffer</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.get_extra_state">get_extra_state</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.get_parameter">get_parameter</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.get_submodule">get_submodule</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.half">half</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.load_state_dict">load_state_dict</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.modules">modules</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.named_buffers">named_buffers</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.named_children">named_children</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.named_modules">named_modules</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.named_parameters">named_parameters</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.parameters">parameters</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.register_backward_hook">register_backward_hook</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.register_buffer">register_buffer</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.register_forward_hook">register_forward_hook</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.register_forward_pre_hook">register_forward_pre_hook</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.register_full_backward_hook">register_full_backward_hook</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.register_parameter">register_parameter</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.requires_grad_">requires_grad_</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.set_extra_state">set_extra_state</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.share_memory">share_memory</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.state_dict">state_dict</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.to">to</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.to_empty">to_empty</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.train">train</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.type">type</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.xpu">xpu</a></li>
    <li class="mono"><a href="#seq2seq.PositionwiseFeedforwardLayer.zero_grad">zero_grad</a></li>
  </ul>

        </li>
        <li class="mono">
        <span class="class_name"><a href="#seq2seq.Seq2Seq">Seq2Seq</a></span>
        
          
  <ul>
    <li class="mono"><a href="#seq2seq.Seq2Seq.add_module">add_module</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.apply">apply</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.bfloat16">bfloat16</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.buffers">buffers</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.children">children</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.cpu">cpu</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.cuda">cuda</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.double">double</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.eval">eval</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.extra_repr">extra_repr</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.float">float</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.forward">forward</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.get_buffer">get_buffer</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.get_extra_state">get_extra_state</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.get_parameter">get_parameter</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.get_submodule">get_submodule</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.half">half</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.load_state_dict">load_state_dict</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.make_src_mask">make_src_mask</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.make_trg_mask">make_trg_mask</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.modules">modules</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.named_buffers">named_buffers</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.named_children">named_children</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.named_modules">named_modules</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.named_parameters">named_parameters</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.parameters">parameters</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.register_backward_hook">register_backward_hook</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.register_buffer">register_buffer</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.register_forward_hook">register_forward_hook</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.register_forward_pre_hook">register_forward_pre_hook</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.register_full_backward_hook">register_full_backward_hook</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.register_parameter">register_parameter</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.requires_grad_">requires_grad_</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.set_extra_state">set_extra_state</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.share_memory">share_memory</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.state_dict">state_dict</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.to">to</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.to_empty">to_empty</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.train">train</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.type">type</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.xpu">xpu</a></li>
    <li class="mono"><a href="#seq2seq.Seq2Seq.zero_grad">zero_grad</a></li>
  </ul>

        </li>
      </ul>
    </li>

    </ul>
  </div>

<article id="content">
  

  


  <header id="section-intro">
  <h1 class="title"><span class="name">seq2seq</span> module</h1>
  
  
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq" class="source">
    <div class="codehilite"><pre><span></span><span class="c1"># seq2seq Transformer Implementation</span>

<span class="c1"># Source code is courtesy of https://github.com/bentrevett/pytorch-seq2seq</span>

<span class="c1"># Project Los Angeles</span>
<span class="c1"># Tegridy Code 2022</span>

<span class="c1"># ========================================================================</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="kn">from</span> <span class="nn">torchtext.data.metrics</span> <span class="kn">import</span> <span class="n">bleu_score</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib.ticker</span> <span class="k">as</span> <span class="nn">ticker</span>

<span class="kn">import</span> <span class="nn">spacy</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="c1"># ========================================================================</span>


<span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> 
                 <span class="n">input_dim</span><span class="p">,</span> 
                 <span class="n">hid_dim</span><span class="p">,</span> 
                 <span class="n">n_layers</span><span class="p">,</span> 
                 <span class="n">n_heads</span><span class="p">,</span> 
                 <span class="n">pf_dim</span><span class="p">,</span>
                 <span class="n">dropout</span><span class="p">,</span> 
                 <span class="n">device</span><span class="p">,</span>
                 <span class="n">max_length</span> <span class="o">=</span> <span class="mi">100</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">tok_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">max_length</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">EncoderLayer</span><span class="p">(</span><span class="n">hid_dim</span><span class="p">,</span> 
                                                  <span class="n">n_heads</span><span class="p">,</span> 
                                                  <span class="n">pf_dim</span><span class="p">,</span>
                                                  <span class="n">dropout</span><span class="p">,</span> 
                                                  <span class="n">device</span><span class="p">)</span> 
                                     <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)])</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([</span><span class="n">hid_dim</span><span class="p">]))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">):</span>
        
        <span class="c1">#src = [batch size, src len]</span>
        <span class="c1">#src_mask = [batch size, 1, 1, src len]</span>
        
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">src</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">src_len</span> <span class="o">=</span> <span class="n">src</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        
        <span class="n">pos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        
        <span class="c1">#pos = [batch size, src len]</span>
        
        <span class="n">src</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">tok_embedding</span><span class="p">(</span><span class="n">src</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_embedding</span><span class="p">(</span><span class="n">pos</span><span class="p">))</span>
        
        <span class="c1">#src = [batch size, src len, hid dim]</span>
        
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">src</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
            
        <span class="c1">#src = [batch size, src len, hid dim]</span>
            
        <span class="k">return</span> <span class="n">src</span>
    
<span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> 
                 <span class="n">hid_dim</span><span class="p">,</span> 
                 <span class="n">n_heads</span><span class="p">,</span> 
                 <span class="n">pf_dim</span><span class="p">,</span>  
                 <span class="n">dropout</span><span class="p">,</span> 
                 <span class="n">device</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hid_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ff_layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hid_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttentionLayer</span><span class="p">(</span><span class="n">hid_dim</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">positionwise_feedforward</span> <span class="o">=</span> <span class="n">PositionwiseFeedforwardLayer</span><span class="p">(</span><span class="n">hid_dim</span><span class="p">,</span> 
                                                                     <span class="n">pf_dim</span><span class="p">,</span> 
                                                                     <span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">):</span>
        
        <span class="c1">#src = [batch size, src len, hid dim]</span>
        <span class="c1">#src_mask = [batch size, 1, 1, src len] </span>
                
        <span class="c1">#self attention</span>
        <span class="n">_src</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attention</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
        
        <span class="c1">#dropout, residual connection and layer norm</span>
        <span class="n">src</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_layer_norm</span><span class="p">(</span><span class="n">src</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">_src</span><span class="p">))</span>
        
        <span class="c1">#src = [batch size, src len, hid dim]</span>
        
        <span class="c1">#positionwise feedforward</span>
        <span class="n">_src</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">positionwise_feedforward</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
        
        <span class="c1">#dropout, residual and layer norm</span>
        <span class="n">src</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff_layer_norm</span><span class="p">(</span><span class="n">src</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">_src</span><span class="p">))</span>
        
        <span class="c1">#src = [batch size, src len, hid dim]</span>
        
        <span class="k">return</span> <span class="n">src</span>
    
<span class="k">class</span> <span class="nc">MultiHeadAttentionLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="k">assert</span> <span class="n">hid_dim</span> <span class="o">%</span> <span class="n">n_heads</span> <span class="o">==</span> <span class="mi">0</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">hid_dim</span> <span class="o">=</span> <span class="n">hid_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">hid_dim</span> <span class="o">//</span> <span class="n">n_heads</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hid_dim</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hid_dim</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hid_dim</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_o</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hid_dim</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">]))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1">#query = [batch size, query len, hid dim]</span>
        <span class="c1">#key = [batch size, key len, hid dim]</span>
        <span class="c1">#value = [batch size, value len, hid dim]</span>
                
        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_q</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_k</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_v</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        
        <span class="c1">#Q = [batch size, query len, hid dim]</span>
        <span class="c1">#K = [batch size, key len, hid dim]</span>
        <span class="c1">#V = [batch size, value len, hid dim]</span>
                
        <span class="n">Q</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        
        <span class="c1">#Q = [batch size, n heads, query len, head dim]</span>
        <span class="c1">#K = [batch size, n heads, key len, head dim]</span>
        <span class="c1">#V = [batch size, n heads, value len, head dim]</span>
                
        <span class="n">energy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>
        
        <span class="c1">#energy = [batch size, n heads, query len, key len]</span>
        
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">energy</span> <span class="o">=</span> <span class="n">energy</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e10</span><span class="p">)</span>
        
        <span class="n">attention</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">energy</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                
        <span class="c1">#attention = [batch size, n heads, query len, key len]</span>
                
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attention</span><span class="p">),</span> <span class="n">V</span><span class="p">)</span>
        
        <span class="c1">#x = [batch size, n heads, query len, head dim]</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        
        <span class="c1">#x = [batch size, query len, n heads, head dim]</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hid_dim</span><span class="p">)</span>
        
        <span class="c1">#x = [batch size, query len, hid dim]</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_o</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1">#x = [batch size, query len, hid dim]</span>
        
        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention</span>
    
<span class="k">class</span> <span class="nc">PositionwiseFeedforwardLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">,</span> <span class="n">pf_dim</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hid_dim</span><span class="p">,</span> <span class="n">pf_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">pf_dim</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        
        <span class="c1">#x = [batch size, seq len, hid dim]</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc_1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        
        <span class="c1">#x = [batch size, seq len, pf dim]</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1">#x = [batch size, seq len, hid dim]</span>
        
        <span class="k">return</span> <span class="n">x</span>
    
<span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> 
                 <span class="n">output_dim</span><span class="p">,</span> 
                 <span class="n">hid_dim</span><span class="p">,</span> 
                 <span class="n">n_layers</span><span class="p">,</span> 
                 <span class="n">n_heads</span><span class="p">,</span> 
                 <span class="n">pf_dim</span><span class="p">,</span> 
                 <span class="n">dropout</span><span class="p">,</span> 
                 <span class="n">device</span><span class="p">,</span>
                 <span class="n">max_length</span> <span class="o">=</span> <span class="mi">100</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">tok_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">output_dim</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">max_length</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">DecoderLayer</span><span class="p">(</span><span class="n">hid_dim</span><span class="p">,</span> 
                                                  <span class="n">n_heads</span><span class="p">,</span> 
                                                  <span class="n">pf_dim</span><span class="p">,</span> 
                                                  <span class="n">dropout</span><span class="p">,</span> 
                                                  <span class="n">device</span><span class="p">)</span>
                                     <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)])</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hid_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([</span><span class="n">hid_dim</span><span class="p">]))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trg</span><span class="p">,</span> <span class="n">enc_src</span><span class="p">,</span> <span class="n">trg_mask</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">):</span>
        
        <span class="c1">#trg = [batch size, trg len]</span>
        <span class="c1">#enc_src = [batch size, src len, hid dim]</span>
        <span class="c1">#trg_mask = [batch size, 1, trg len, trg len]</span>
        <span class="c1">#src_mask = [batch size, 1, 1, src len]</span>
                
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">trg</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">trg_len</span> <span class="o">=</span> <span class="n">trg</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        
        <span class="n">pos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">trg_len</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                            
        <span class="c1">#pos = [batch size, trg len]</span>
            
        <span class="n">trg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">tok_embedding</span><span class="p">(</span><span class="n">trg</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_embedding</span><span class="p">(</span><span class="n">pos</span><span class="p">))</span>
                
        <span class="c1">#trg = [batch size, trg len, hid dim]</span>
        
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">trg</span><span class="p">,</span> <span class="n">attention</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">trg</span><span class="p">,</span> <span class="n">enc_src</span><span class="p">,</span> <span class="n">trg_mask</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
        
        <span class="c1">#trg = [batch size, trg len, hid dim]</span>
        <span class="c1">#attention = [batch size, n heads, trg len, src len]</span>
        
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_out</span><span class="p">(</span><span class="n">trg</span><span class="p">)</span>
        
        <span class="c1">#output = [batch size, trg len, output dim]</span>
            
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attention</span>
    
<span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> 
                 <span class="n">hid_dim</span><span class="p">,</span> 
                 <span class="n">n_heads</span><span class="p">,</span> 
                 <span class="n">pf_dim</span><span class="p">,</span> 
                 <span class="n">dropout</span><span class="p">,</span> 
                 <span class="n">device</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hid_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enc_attn_layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hid_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ff_layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hid_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttentionLayer</span><span class="p">(</span><span class="n">hid_dim</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttentionLayer</span><span class="p">(</span><span class="n">hid_dim</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">positionwise_feedforward</span> <span class="o">=</span> <span class="n">PositionwiseFeedforwardLayer</span><span class="p">(</span><span class="n">hid_dim</span><span class="p">,</span> 
                                                                     <span class="n">pf_dim</span><span class="p">,</span> 
                                                                     <span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trg</span><span class="p">,</span> <span class="n">enc_src</span><span class="p">,</span> <span class="n">trg_mask</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">):</span>
        
        <span class="c1">#trg = [batch size, trg len, hid dim]</span>
        <span class="c1">#enc_src = [batch size, src len, hid dim]</span>
        <span class="c1">#trg_mask = [batch size, 1, trg len, trg len]</span>
        <span class="c1">#src_mask = [batch size, 1, 1, src len]</span>
        
        <span class="c1">#self attention</span>
        <span class="n">_trg</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attention</span><span class="p">(</span><span class="n">trg</span><span class="p">,</span> <span class="n">trg</span><span class="p">,</span> <span class="n">trg</span><span class="p">,</span> <span class="n">trg_mask</span><span class="p">)</span>
        
        <span class="c1">#dropout, residual connection and layer norm</span>
        <span class="n">trg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_layer_norm</span><span class="p">(</span><span class="n">trg</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">_trg</span><span class="p">))</span>
            
        <span class="c1">#trg = [batch size, trg len, hid dim]</span>
            
        <span class="c1">#encoder attention</span>
        <span class="n">_trg</span><span class="p">,</span> <span class="n">attention</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_attention</span><span class="p">(</span><span class="n">trg</span><span class="p">,</span> <span class="n">enc_src</span><span class="p">,</span> <span class="n">enc_src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
        
        <span class="c1">#dropout, residual connection and layer norm</span>
        <span class="n">trg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc_attn_layer_norm</span><span class="p">(</span><span class="n">trg</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">_trg</span><span class="p">))</span>
                    
        <span class="c1">#trg = [batch size, trg len, hid dim]</span>
        
        <span class="c1">#positionwise feedforward</span>
        <span class="n">_trg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">positionwise_feedforward</span><span class="p">(</span><span class="n">trg</span><span class="p">)</span>
        
        <span class="c1">#dropout, residual and layer norm</span>
        <span class="n">trg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff_layer_norm</span><span class="p">(</span><span class="n">trg</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">_trg</span><span class="p">))</span>
        
        <span class="c1">#trg = [batch size, trg len, hid dim]</span>
        <span class="c1">#attention = [batch size, n heads, trg len, src len]</span>
        
        <span class="k">return</span> <span class="n">trg</span><span class="p">,</span> <span class="n">attention</span>
    
<span class="k">class</span> <span class="nc">Seq2Seq</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> 
                 <span class="n">encoder</span><span class="p">,</span> 
                 <span class="n">decoder</span><span class="p">,</span> 
                 <span class="n">src_pad_idx</span><span class="p">,</span> 
                 <span class="n">trg_pad_idx</span><span class="p">,</span> 
                 <span class="n">device</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">src_pad_idx</span> <span class="o">=</span> <span class="n">src_pad_idx</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trg_pad_idx</span> <span class="o">=</span> <span class="n">trg_pad_idx</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        
    <span class="k">def</span> <span class="nf">make_src_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">):</span>
        
        <span class="c1">#src = [batch size, src len]</span>
        
        <span class="n">src_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">src</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_pad_idx</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

        <span class="c1">#src_mask = [batch size, 1, 1, src len]</span>

        <span class="k">return</span> <span class="n">src_mask</span>
    
    <span class="k">def</span> <span class="nf">make_trg_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trg</span><span class="p">):</span>
        
        <span class="c1">#trg = [batch size, trg len]</span>
        
        <span class="n">trg_pad_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">trg</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trg_pad_idx</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="c1">#trg_pad_mask = [batch size, 1, 1, trg len]</span>
        
        <span class="n">trg_len</span> <span class="o">=</span> <span class="n">trg</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        
        <span class="n">trg_sub_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">trg_len</span><span class="p">,</span> <span class="n">trg_len</span><span class="p">),</span> <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">))</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
        
        <span class="c1">#trg_sub_mask = [trg len, trg len]</span>
            
        <span class="n">trg_mask</span> <span class="o">=</span> <span class="n">trg_pad_mask</span> <span class="o">&amp;</span> <span class="n">trg_sub_mask</span>
        
        <span class="c1">#trg_mask = [batch size, 1, trg len, trg len]</span>
        
        <span class="k">return</span> <span class="n">trg_mask</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">trg</span><span class="p">):</span>
        
        <span class="c1">#src = [batch size, src len]</span>
        <span class="c1">#trg = [batch size, trg len]</span>
                
        <span class="n">src_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">make_src_mask</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
        <span class="n">trg_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">make_trg_mask</span><span class="p">(</span><span class="n">trg</span><span class="p">)</span>
        
        <span class="c1">#src_mask = [batch size, 1, 1, src len]</span>
        <span class="c1">#trg_mask = [batch size, 1, trg len, trg len]</span>
        
        <span class="n">enc_src</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
        
        <span class="c1">#enc_src = [batch size, src len, hid dim]</span>
                
        <span class="n">output</span><span class="p">,</span> <span class="n">attention</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">trg</span><span class="p">,</span> <span class="n">enc_src</span><span class="p">,</span> <span class="n">trg_mask</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
        
        <span class="c1">#output = [batch size, trg len, output dim]</span>
        <span class="c1">#attention = [batch size, n heads, trg len, src len]</span>
        
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attention</span>
    
<span class="c1"># ========================================================================</span>
    
<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">iterator</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">clip</span><span class="p">):</span>
    
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    
    <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">iterator</span><span class="p">):</span>
        
        <span class="n">src</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">src</span>
        <span class="n">trg</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">trg</span>
        
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        
        <span class="n">output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">trg</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
                
        <span class="c1">#output = [batch size, trg len - 1, output dim]</span>
        <span class="c1">#trg = [batch size, trg len]</span>
            
        <span class="n">output_dim</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
        <span class="n">trg</span> <span class="o">=</span> <span class="n">trg</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                
        <span class="c1">#output = [batch size * trg len - 1, output dim]</span>
        <span class="c1">#trg = [batch size * trg len - 1]</span>
            
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">trg</span><span class="p">)</span>
        
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">clip</span><span class="p">)</span>
        
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        
    <span class="k">return</span> <span class="n">epoch_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">iterator</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">iterator</span><span class="p">,</span> <span class="n">criterion</span><span class="p">):</span>
    
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    
    <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">iterator</span><span class="p">):</span>

            <span class="n">src</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">src</span>
            <span class="n">trg</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">trg</span>

            <span class="n">output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">trg</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            
            <span class="c1">#output = [batch size, trg len - 1, output dim]</span>
            <span class="c1">#trg = [batch size, trg len]</span>
            
            <span class="n">output_dim</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            
            <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
            <span class="n">trg</span> <span class="o">=</span> <span class="n">trg</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            
            <span class="c1">#output = [batch size * trg len - 1, output dim]</span>
            <span class="c1">#trg = [batch size * trg len - 1]</span>
            
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">trg</span><span class="p">)</span>

            <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        
    <span class="k">return</span> <span class="n">epoch_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">iterator</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">epoch_time</span><span class="p">(</span><span class="n">start_time</span><span class="p">,</span> <span class="n">end_time</span><span class="p">):</span>
    <span class="n">elapsed_time</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>
    <span class="n">elapsed_mins</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">elapsed_time</span> <span class="o">/</span> <span class="mi">60</span><span class="p">)</span>
    <span class="n">elapsed_secs</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">elapsed_time</span> <span class="o">-</span> <span class="p">(</span><span class="n">elapsed_mins</span> <span class="o">*</span> <span class="mi">60</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">elapsed_mins</span><span class="p">,</span> <span class="n">elapsed_secs</span>

<span class="c1"># ========================================================================</span>

<span class="k">def</span> <span class="nf">translate_sentence</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">src_field</span><span class="p">,</span> <span class="n">trg_field</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">max_len</span> <span class="o">=</span> <span class="mi">50</span><span class="p">):</span>
    
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;de_core_news_sm&#39;</span><span class="p">)</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">nlp</span><span class="p">(</span><span class="n">sentence</span><span class="p">)]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">]</span>

    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">src_field</span><span class="o">.</span><span class="n">init_token</span><span class="p">]</span> <span class="o">+</span> <span class="n">tokens</span> <span class="o">+</span> <span class="p">[</span><span class="n">src_field</span><span class="o">.</span><span class="n">eos_token</span><span class="p">]</span>
        
    <span class="n">src_indexes</span> <span class="o">=</span> <span class="p">[</span><span class="n">src_field</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">stoi</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>

    <span class="n">src_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">src_indexes</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    
    <span class="n">src_mask</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">make_src_mask</span><span class="p">(</span><span class="n">src_tensor</span><span class="p">)</span>
    
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">enc_src</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">src_tensor</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>

    <span class="n">trg_indexes</span> <span class="o">=</span> <span class="p">[</span><span class="n">trg_field</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">stoi</span><span class="p">[</span><span class="n">trg_field</span><span class="o">.</span><span class="n">init_token</span><span class="p">]]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_len</span><span class="p">):</span>

        <span class="n">trg_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">trg_indexes</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">trg_mask</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">make_trg_mask</span><span class="p">(</span><span class="n">trg_tensor</span><span class="p">)</span>
        
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">output</span><span class="p">,</span> <span class="n">attention</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">trg_tensor</span><span class="p">,</span> <span class="n">enc_src</span><span class="p">,</span> <span class="n">trg_mask</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
        
        <span class="n">pred_token</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">2</span><span class="p">)[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        
        <span class="n">trg_indexes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred_token</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">pred_token</span> <span class="o">==</span> <span class="n">trg_field</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">stoi</span><span class="p">[</span><span class="n">trg_field</span><span class="o">.</span><span class="n">eos_token</span><span class="p">]:</span>
            <span class="k">break</span>
    
    <span class="n">trg_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">trg_field</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">itos</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">trg_indexes</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">trg_tokens</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">attention</span>

<span class="k">def</span> <span class="nf">display_attention</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">translation</span><span class="p">,</span> <span class="n">attention</span><span class="p">,</span> <span class="n">n_heads</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">n_rows</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">n_cols</span> <span class="o">=</span> <span class="mi">2</span><span class="p">):</span>
    
    <span class="k">assert</span> <span class="n">n_rows</span> <span class="o">*</span> <span class="n">n_cols</span> <span class="o">==</span> <span class="n">n_heads</span>
    
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">25</span><span class="p">))</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_heads</span><span class="p">):</span>
        
        <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">n_rows</span><span class="p">,</span> <span class="n">n_cols</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="n">_attention</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="n">cax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">_attention</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;bone&#39;</span><span class="p">)</span>

        <span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">labelsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s1">&#39;&#39;</span><span class="p">]</span><span class="o">+</span><span class="p">[</span><span class="s1">&#39;&lt;sos&gt;&#39;</span><span class="p">]</span><span class="o">+</span><span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">]</span><span class="o">+</span><span class="p">[</span><span class="s1">&#39;&lt;eos&gt;&#39;</span><span class="p">],</span> 
                           <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([</span><span class="s1">&#39;&#39;</span><span class="p">]</span><span class="o">+</span><span class="n">translation</span><span class="p">)</span>

        <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_major_locator</span><span class="p">(</span><span class="n">ticker</span><span class="o">.</span><span class="n">MultipleLocator</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_locator</span><span class="p">(</span><span class="n">ticker</span><span class="o">.</span><span class="n">MultipleLocator</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    
<span class="c1"># ========================================================================</span>
    
<span class="k">def</span> <span class="nf">calculate_bleu</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">src_field</span><span class="p">,</span> <span class="n">trg_field</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">max_len</span> <span class="o">=</span> <span class="mi">50</span><span class="p">):</span>
    
    <span class="n">trgs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">pred_trgs</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">datum</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
        
        <span class="n">src</span> <span class="o">=</span> <span class="nb">vars</span><span class="p">(</span><span class="n">datum</span><span class="p">)[</span><span class="s1">&#39;src&#39;</span><span class="p">]</span>
        <span class="n">trg</span> <span class="o">=</span> <span class="nb">vars</span><span class="p">(</span><span class="n">datum</span><span class="p">)[</span><span class="s1">&#39;trg&#39;</span><span class="p">]</span>
        
        <span class="n">pred_trg</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">translate_sentence</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">src_field</span><span class="p">,</span> <span class="n">trg_field</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">max_len</span><span class="p">)</span>
        
        <span class="c1">#cut off &lt;eos&gt; token</span>
        <span class="n">pred_trg</span> <span class="o">=</span> <span class="n">pred_trg</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        
        <span class="n">pred_trgs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred_trg</span><span class="p">)</span>
        <span class="n">trgs</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">trg</span><span class="p">])</span>
        
    <span class="k">return</span> <span class="n">bleu_score</span><span class="p">(</span><span class="n">pred_trgs</span><span class="p">,</span> <span class="n">trgs</span><span class="p">)</span>

<span class="c1"># ========================================================================</span>
</pre></div>

  </div>

  </header>

  <section id="section-items">

    <h2 class="section-title" id="header-functions">Functions</h2>
      
  <div class="item">
    <div class="name def" id="seq2seq.calculate_bleu">
    <p>def <span class="ident">calculate_bleu</span>(</p><p>data, src_field, trg_field, model, device, max_len=50)</p>
    </div>
    

    
  
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.calculate_bleu', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.calculate_bleu" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">calculate_bleu</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">src_field</span><span class="p">,</span> <span class="n">trg_field</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">max_len</span> <span class="o">=</span> <span class="mi">50</span><span class="p">):</span>
    
    <span class="n">trgs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">pred_trgs</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">datum</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
        
        <span class="n">src</span> <span class="o">=</span> <span class="nb">vars</span><span class="p">(</span><span class="n">datum</span><span class="p">)[</span><span class="s1">&#39;src&#39;</span><span class="p">]</span>
        <span class="n">trg</span> <span class="o">=</span> <span class="nb">vars</span><span class="p">(</span><span class="n">datum</span><span class="p">)[</span><span class="s1">&#39;trg&#39;</span><span class="p">]</span>
        
        <span class="n">pred_trg</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">translate_sentence</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">src_field</span><span class="p">,</span> <span class="n">trg_field</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">max_len</span><span class="p">)</span>
        
        <span class="c1">#cut off &lt;eos&gt; token</span>
        <span class="n">pred_trg</span> <span class="o">=</span> <span class="n">pred_trg</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        
        <span class="n">pred_trgs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred_trg</span><span class="p">)</span>
        <span class="n">trgs</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">trg</span><span class="p">])</span>
        
    <span class="k">return</span> <span class="n">bleu_score</span><span class="p">(</span><span class="n">pred_trgs</span><span class="p">,</span> <span class="n">trgs</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="seq2seq.display_attention">
    <p>def <span class="ident">display_attention</span>(</p><p>sentence, translation, attention, n_heads=8, n_rows=4, n_cols=2)</p>
    </div>
    

    
  
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.display_attention', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.display_attention" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">display_attention</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">translation</span><span class="p">,</span> <span class="n">attention</span><span class="p">,</span> <span class="n">n_heads</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">n_rows</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">n_cols</span> <span class="o">=</span> <span class="mi">2</span><span class="p">):</span>
    
    <span class="k">assert</span> <span class="n">n_rows</span> <span class="o">*</span> <span class="n">n_cols</span> <span class="o">==</span> <span class="n">n_heads</span>
    
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">25</span><span class="p">))</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_heads</span><span class="p">):</span>
        
        <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">n_rows</span><span class="p">,</span> <span class="n">n_cols</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="n">_attention</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="n">cax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">_attention</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;bone&#39;</span><span class="p">)</span>

        <span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">labelsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s1">&#39;&#39;</span><span class="p">]</span><span class="o">+</span><span class="p">[</span><span class="s1">&#39;&lt;sos&gt;&#39;</span><span class="p">]</span><span class="o">+</span><span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">]</span><span class="o">+</span><span class="p">[</span><span class="s1">&#39;&lt;eos&gt;&#39;</span><span class="p">],</span> 
                           <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([</span><span class="s1">&#39;&#39;</span><span class="p">]</span><span class="o">+</span><span class="n">translation</span><span class="p">)</span>

        <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_major_locator</span><span class="p">(</span><span class="n">ticker</span><span class="o">.</span><span class="n">MultipleLocator</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_locator</span><span class="p">(</span><span class="n">ticker</span><span class="o">.</span><span class="n">MultipleLocator</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>

  </div>
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="seq2seq.epoch_time">
    <p>def <span class="ident">epoch_time</span>(</p><p>start_time, end_time)</p>
    </div>
    

    
  
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.epoch_time', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.epoch_time" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">epoch_time</span><span class="p">(</span><span class="n">start_time</span><span class="p">,</span> <span class="n">end_time</span><span class="p">):</span>
    <span class="n">elapsed_time</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>
    <span class="n">elapsed_mins</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">elapsed_time</span> <span class="o">/</span> <span class="mi">60</span><span class="p">)</span>
    <span class="n">elapsed_secs</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">elapsed_time</span> <span class="o">-</span> <span class="p">(</span><span class="n">elapsed_mins</span> <span class="o">*</span> <span class="mi">60</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">elapsed_mins</span><span class="p">,</span> <span class="n">elapsed_secs</span>
</pre></div>

  </div>
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="seq2seq.evaluate">
    <p>def <span class="ident">evaluate</span>(</p><p>model, iterator, criterion)</p>
    </div>
    

    
  
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.evaluate', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.evaluate" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">iterator</span><span class="p">,</span> <span class="n">criterion</span><span class="p">):</span>
    
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    
    <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">iterator</span><span class="p">):</span>

            <span class="n">src</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">src</span>
            <span class="n">trg</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">trg</span>

            <span class="n">output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">trg</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            
            <span class="c1">#output = [batch size, trg len - 1, output dim]</span>
            <span class="c1">#trg = [batch size, trg len]</span>
            
            <span class="n">output_dim</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            
            <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
            <span class="n">trg</span> <span class="o">=</span> <span class="n">trg</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            
            <span class="c1">#output = [batch size * trg len - 1, output dim]</span>
            <span class="c1">#trg = [batch size * trg len - 1]</span>
            
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">trg</span><span class="p">)</span>

            <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        
    <span class="k">return</span> <span class="n">epoch_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">iterator</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="seq2seq.train">
    <p>def <span class="ident">train</span>(</p><p>model, iterator, optimizer, criterion, clip)</p>
    </div>
    

    
  
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.train', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.train" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">iterator</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">clip</span><span class="p">):</span>
    
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    
    <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">iterator</span><span class="p">):</span>
        
        <span class="n">src</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">src</span>
        <span class="n">trg</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">trg</span>
        
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        
        <span class="n">output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">trg</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
                
        <span class="c1">#output = [batch size, trg len - 1, output dim]</span>
        <span class="c1">#trg = [batch size, trg len]</span>
            
        <span class="n">output_dim</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
        <span class="n">trg</span> <span class="o">=</span> <span class="n">trg</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                
        <span class="c1">#output = [batch size * trg len - 1, output dim]</span>
        <span class="c1">#trg = [batch size * trg len - 1]</span>
            
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">trg</span><span class="p">)</span>
        
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">clip</span><span class="p">)</span>
        
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        
    <span class="k">return</span> <span class="n">epoch_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">iterator</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="seq2seq.translate_sentence">
    <p>def <span class="ident">translate_sentence</span>(</p><p>sentence, src_field, trg_field, model, device, max_len=50)</p>
    </div>
    

    
  
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.translate_sentence', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.translate_sentence" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">translate_sentence</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">src_field</span><span class="p">,</span> <span class="n">trg_field</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">max_len</span> <span class="o">=</span> <span class="mi">50</span><span class="p">):</span>
    
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;de_core_news_sm&#39;</span><span class="p">)</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">nlp</span><span class="p">(</span><span class="n">sentence</span><span class="p">)]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">]</span>

    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">src_field</span><span class="o">.</span><span class="n">init_token</span><span class="p">]</span> <span class="o">+</span> <span class="n">tokens</span> <span class="o">+</span> <span class="p">[</span><span class="n">src_field</span><span class="o">.</span><span class="n">eos_token</span><span class="p">]</span>
        
    <span class="n">src_indexes</span> <span class="o">=</span> <span class="p">[</span><span class="n">src_field</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">stoi</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>

    <span class="n">src_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">src_indexes</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    
    <span class="n">src_mask</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">make_src_mask</span><span class="p">(</span><span class="n">src_tensor</span><span class="p">)</span>
    
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">enc_src</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">src_tensor</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>

    <span class="n">trg_indexes</span> <span class="o">=</span> <span class="p">[</span><span class="n">trg_field</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">stoi</span><span class="p">[</span><span class="n">trg_field</span><span class="o">.</span><span class="n">init_token</span><span class="p">]]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_len</span><span class="p">):</span>

        <span class="n">trg_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">trg_indexes</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">trg_mask</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">make_trg_mask</span><span class="p">(</span><span class="n">trg_tensor</span><span class="p">)</span>
        
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">output</span><span class="p">,</span> <span class="n">attention</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">trg_tensor</span><span class="p">,</span> <span class="n">enc_src</span><span class="p">,</span> <span class="n">trg_mask</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
        
        <span class="n">pred_token</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">2</span><span class="p">)[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        
        <span class="n">trg_indexes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred_token</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">pred_token</span> <span class="o">==</span> <span class="n">trg_field</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">stoi</span><span class="p">[</span><span class="n">trg_field</span><span class="o">.</span><span class="n">eos_token</span><span class="p">]:</span>
            <span class="k">break</span>
    
    <span class="n">trg_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">trg_field</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">itos</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">trg_indexes</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">trg_tokens</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">attention</span>
</pre></div>

  </div>
</div>

  </div>
  

    <h2 class="section-title" id="header-classes">Classes</h2>
      
      <div class="item">
      <p id="seq2seq.Decoder" class="name">class <span class="ident">Decoder</span></p>
      
  
    <div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p></div>
  <div class="source_cont">
</div>


      <div class="class">
          <h3>Ancestors (in MRO)</h3>
          <ul class="class_list">
          <li><a href="/torch.nn.modules.module.Module.ext">torch.nn.modules.module.Module</a></li>
          </ul>
          <h3>Class variables</h3>
            <div class="item">
            <p id="seq2seq.Decoder.T_destination" class="name">var <span class="ident">T_destination</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="seq2seq.Decoder.dump_patches" class="name">var <span class="ident">dump_patches</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
          <h3>Methods</h3>
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.add_module">
    <p>def <span class="ident">add_module</span>(</p><p>self, name: str, module: Union[ForwardRef(&#39;Module&#39;), NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Adds a child module to the current module.</p>
<p>The module can be accessed as an attribute using the given name.</p>
<p>Args:
    name (string): name of the child module. The child module can be
        accessed from this module using the given name
    module (Module): child module to be added to the module.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.add_module', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.add_module" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">add_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s1">&#39;Module&#39;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Adds a child module to the current module.</span>
<span class="sd">    The module can be accessed as an attribute using the given name.</span>
<span class="sd">    Args:</span>
<span class="sd">        name (string): name of the child module. The child module can be</span>
<span class="sd">            accessed from this module using the given name</span>
<span class="sd">        module (Module): child module to be added to the module.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">Module</span><span class="p">)</span> <span class="ow">and</span> <span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> is not a Module subclass&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">module</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_six</span><span class="o">.</span><span class="n">string_classes</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;module name should be a string. Got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">name</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span> <span class="ow">and</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;attribute &#39;</span><span class="si">{}</span><span class="s2">&#39; already exists&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">elif</span> <span class="s1">&#39;.&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;module name can&#39;t contain </span><span class="se">\&quot;</span><span class="s2">.</span><span class="se">\&quot;</span><span class="s2">, got: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;module name can&#39;t be empty string </span><span class="se">\&quot;\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.apply">
    <p>def <span class="ident">apply</span>(</p><p>self: ~T, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>)
as well as self. Typical use includes initializing the parameters of a model
(see also :ref:<code>nn-init-doc</code>).</p>
<p>Args:
    fn (:class:<code>Module</code> -&gt; None): function to be applied to each submodule</p>
<p>Returns:
    Module: self</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; @torch.no_grad()
&gt;&gt;&gt; def init_weights(m):
&gt;&gt;&gt;     print(m)
&gt;&gt;&gt;     if type(m) == nn.Linear:
&gt;&gt;&gt;         m.weight.fill_(1.0)
&gt;&gt;&gt;         print(m.weight)
&gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))
&gt;&gt;&gt; net.apply(init_weights)
Linear(in_features=2, out_features=2, bias=True)
Parameter containing:
tensor([[ 1.,  1.],
        [ 1.,  1.]])
Linear(in_features=2, out_features=2, bias=True)
Parameter containing:
tensor([[ 1.,  1.],
        [ 1.,  1.]])
Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.apply', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.apply" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="s1">&#39;Module&#39;</span><span class="p">],</span> <span class="kc">None</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies ``fn`` recursively to every submodule (as returned by ``.children()``)</span>
<span class="sd">    as well as self. Typical use includes initializing the parameters of a model</span>
<span class="sd">    (see also :ref:`nn-init-doc`).</span>
<span class="sd">    Args:</span>
<span class="sd">        fn (:class:`Module` -&gt; None): function to be applied to each submodule</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; @torch.no_grad()</span>
<span class="sd">        &gt;&gt;&gt; def init_weights(m):</span>
<span class="sd">        &gt;&gt;&gt;     print(m)</span>
<span class="sd">        &gt;&gt;&gt;     if type(m) == nn.Linear:</span>
<span class="sd">        &gt;&gt;&gt;         m.weight.fill_(1.0)</span>
<span class="sd">        &gt;&gt;&gt;         print(m.weight)</span>
<span class="sd">        &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))</span>
<span class="sd">        &gt;&gt;&gt; net.apply(init_weights)</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 1.,  1.],</span>
<span class="sd">                [ 1.,  1.]])</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 1.,  1.],</span>
<span class="sd">                [ 1.,  1.]])</span>
<span class="sd">        Sequential(</span>
<span class="sd">          (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">          (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        )</span>
<span class="sd">        Sequential(</span>
<span class="sd">          (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">          (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        )</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">():</span>
        <span class="n">module</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
    <span class="n">fn</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.bfloat16">
    <p>def <span class="ident">bfloat16</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.bfloat16', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.bfloat16" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">bfloat16</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all floating point parameters and buffers to ``bfloat16`` datatype.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">()</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="k">else</span> <span class="n">t</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.buffers">
    <p>def <span class="ident">buffers</span>(</p><p>self, recurse: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over module buffers.</p>
<p>Args:
    recurse (bool): if True, then yields buffers of this module
        and all submodules. Otherwise, yields only buffers that
        are direct members of this module.</p>
<p>Yields:
    torch.Tensor: module buffer</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for buf in model.buffers():
&gt;&gt;&gt;     print(type(buf), buf.size())
&lt;class 'torch.Tensor'&gt; (20L,)
&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.buffers', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.buffers" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over module buffers.</span>
<span class="sd">    Args:</span>
<span class="sd">        recurse (bool): if True, then yields buffers of this module</span>
<span class="sd">            and all submodules. Otherwise, yields only buffers that</span>
<span class="sd">            are direct members of this module.</span>
<span class="sd">    Yields:</span>
<span class="sd">        torch.Tensor: module buffer</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for buf in model.buffers():</span>
<span class="sd">        &gt;&gt;&gt;     print(type(buf), buf.size())</span>
<span class="sd">        &lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="sd">        &lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">buf</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">buf</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.children">
    <p>def <span class="ident">children</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over immediate children modules.</p>
<p>Yields:
    Module: a child module</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.children', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.children" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">children</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="s1">&#39;Module&#39;</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over immediate children modules.</span>
<span class="sd">    Yields:</span>
<span class="sd">        Module: a child module</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
        <span class="k">yield</span> <span class="n">module</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.cpu">
    <p>def <span class="ident">cpu</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Moves all model parameters and buffers to the CPU.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.cpu', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.cpu" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves all model parameters and buffers to the CPU.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.cuda">
    <p>def <span class="ident">cuda</span>(</p><p>self: ~T, device: Union[int, torch.device, NoneType] = None)</p>
    </div>
    

    
  
    <div class="desc"><p>Moves all model parameters and buffers to the GPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on GPU while being optimized.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Args:
    device (int, optional): if specified, all parameters will be
        copied to that device</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.cuda', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.cuda" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">cuda</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves all model parameters and buffers to the GPU.</span>
<span class="sd">    This also makes associated parameters and buffers different objects. So</span>
<span class="sd">    it should be called before constructing optimizer if the module will</span>
<span class="sd">    live on GPU while being optimized.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Args:</span>
<span class="sd">        device (int, optional): if specified, all parameters will be</span>
<span class="sd">            copied to that device</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.double">
    <p>def <span class="ident">double</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.double', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.double" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">double</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all floating point parameters and buffers to ``double`` datatype.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">double</span><span class="p">()</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="k">else</span> <span class="n">t</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.eval">
    <p>def <span class="ident">eval</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Sets the module in evaluation mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>,
etc.</p>
<p>This is equivalent with :meth:<code>self.train(False) &lt;torch.nn.Module.train&gt;</code>.</p>
<p>See :ref:<code>locally-disable-grad-doc</code> for a comparison between
<code>.eval()</code> and several similar mechanisms that may be confused with it.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.eval', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.eval" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets the module in evaluation mode.</span>
<span class="sd">    This has any effect only on certain modules. See documentations of</span>
<span class="sd">    particular modules for details of their behaviors in training/evaluation</span>
<span class="sd">    mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,</span>
<span class="sd">    etc.</span>
<span class="sd">    This is equivalent with :meth:`self.train(False) &lt;torch.nn.Module.train&gt;`.</span>
<span class="sd">    See :ref:`locally-disable-grad-doc` for a comparison between</span>
<span class="sd">    `.eval()` and several similar mechanisms that may be confused with it.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.extra_repr">
    <p>def <span class="ident">extra_repr</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.extra_repr', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.extra_repr" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Set the extra representation of the module</span>
<span class="sd">    To print customized extra information, you should re-implement</span>
<span class="sd">    this method in your own modules. Both single-line and multi-line</span>
<span class="sd">    strings are acceptable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="s1">&#39;&#39;</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.float">
    <p>def <span class="ident">float</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.float', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.float" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">float</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all floating point parameters and buffers to ``float`` datatype.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="k">else</span> <span class="n">t</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.forward">
    <p>def <span class="ident">forward</span>(</p><p>self, trg, enc_src, trg_mask, src_mask)</p>
    </div>
    

    
  
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.forward', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.forward" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trg</span><span class="p">,</span> <span class="n">enc_src</span><span class="p">,</span> <span class="n">trg_mask</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">):</span>
    
    <span class="c1">#trg = [batch size, trg len]</span>
    <span class="c1">#enc_src = [batch size, src len, hid dim]</span>
    <span class="c1">#trg_mask = [batch size, 1, trg len, trg len]</span>
    <span class="c1">#src_mask = [batch size, 1, 1, src len]</span>
            
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">trg</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">trg_len</span> <span class="o">=</span> <span class="n">trg</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="n">pos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">trg_len</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                        
    <span class="c1">#pos = [batch size, trg len]</span>
        
    <span class="n">trg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">tok_embedding</span><span class="p">(</span><span class="n">trg</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_embedding</span><span class="p">(</span><span class="n">pos</span><span class="p">))</span>
            
    <span class="c1">#trg = [batch size, trg len, hid dim]</span>
    
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
        <span class="n">trg</span><span class="p">,</span> <span class="n">attention</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">trg</span><span class="p">,</span> <span class="n">enc_src</span><span class="p">,</span> <span class="n">trg_mask</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
    
    <span class="c1">#trg = [batch size, trg len, hid dim]</span>
    <span class="c1">#attention = [batch size, n heads, trg len, src len]</span>
    
    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_out</span><span class="p">(</span><span class="n">trg</span><span class="p">)</span>
    
    <span class="c1">#output = [batch size, trg len, output dim]</span>
        
    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attention</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.get_buffer">
    <p>def <span class="ident">get_buffer</span>(</p><p>self, target: str)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the buffer given by <code>target</code> if it exists,
otherwise throws an error.</p>
<p>See the docstring for <code>get_submodule</code> for a more detailed
explanation of this method's functionality as well as how to
correctly specify <code>target</code>.</p>
<p>Args:
    target: The fully-qualified string name of the buffer
        to look for. (See <code>get_submodule</code> for how to specify a
        fully-qualified string.)</p>
<p>Returns:
    torch.Tensor: The buffer referenced by <code>target</code></p>
<p>Raises:
    AttributeError: If the target string references an invalid
        path or resolves to something that is not a
        buffer</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.get_buffer', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.get_buffer" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_buffer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the buffer given by ``target`` if it exists,</span>
<span class="sd">    otherwise throws an error.</span>
<span class="sd">    See the docstring for ``get_submodule`` for a more detailed</span>
<span class="sd">    explanation of this method&#39;s functionality as well as how to</span>
<span class="sd">    correctly specify ``target``.</span>
<span class="sd">    Args:</span>
<span class="sd">        target: The fully-qualified string name of the buffer</span>
<span class="sd">            to look for. (See ``get_submodule`` for how to specify a</span>
<span class="sd">            fully-qualified string.)</span>
<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: The buffer referenced by ``target``</span>
<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: If the target string references an invalid</span>
<span class="sd">            path or resolves to something that is not a</span>
<span class="sd">            buffer</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">module_path</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">buffer_name</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">rpartition</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
    <span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_submodule</span><span class="p">(</span><span class="n">module_path</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">buffer_name</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">_get_name</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot; has no attribute `&quot;</span>
                             <span class="o">+</span> <span class="n">buffer_name</span> <span class="o">+</span> <span class="s2">&quot;`&quot;</span><span class="p">)</span>
    <span class="n">buffer</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">buffer_name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">buffer_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">mod</span><span class="o">.</span><span class="n">_buffers</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s2">&quot;`&quot;</span> <span class="o">+</span> <span class="n">buffer_name</span> <span class="o">+</span> <span class="s2">&quot;` is not a buffer&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">buffer</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.get_extra_state">
    <p>def <span class="ident">get_extra_state</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns any extra state to include in the module's state_dict.
Implement this and a corresponding :func:<code>set_extra_state</code> for your module
if you need to store extra state. This function is called when building the
module's <code>state_dict()</code>.</p>
<p>Note that extra state should be pickleable to ensure working serialization
of the state_dict. We only provide provide backwards compatibility guarantees
for serializing Tensors; other objects may break backwards compatibility if
their serialized pickled form changes.</p>
<p>Returns:
    object: Any extra state to store in the module's state_dict</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.get_extra_state', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.get_extra_state" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_extra_state</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns any extra state to include in the module&#39;s state_dict.</span>
<span class="sd">    Implement this and a corresponding :func:`set_extra_state` for your module</span>
<span class="sd">    if you need to store extra state. This function is called when building the</span>
<span class="sd">    module&#39;s `state_dict()`.</span>
<span class="sd">    Note that extra state should be pickleable to ensure working serialization</span>
<span class="sd">    of the state_dict. We only provide provide backwards compatibility guarantees</span>
<span class="sd">    for serializing Tensors; other objects may break backwards compatibility if</span>
<span class="sd">    their serialized pickled form changes.</span>
<span class="sd">    Returns:</span>
<span class="sd">        object: Any extra state to store in the module&#39;s state_dict</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
        <span class="s2">&quot;Reached a code path in Module.get_extra_state() that should never be called. &quot;</span>
        <span class="s2">&quot;Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md &quot;</span>
        <span class="s2">&quot;to report this bug.&quot;</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.get_parameter">
    <p>def <span class="ident">get_parameter</span>(</p><p>self, target: str)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the parameter given by <code>target</code> if it exists,
otherwise throws an error.</p>
<p>See the docstring for <code>get_submodule</code> for a more detailed
explanation of this method's functionality as well as how to
correctly specify <code>target</code>.</p>
<p>Args:
    target: The fully-qualified string name of the Parameter
        to look for. (See <code>get_submodule</code> for how to specify a
        fully-qualified string.)</p>
<p>Returns:
    torch.nn.Parameter: The Parameter referenced by <code>target</code></p>
<p>Raises:
    AttributeError: If the target string references an invalid
        path or resolves to something that is not an
        <code>nn.Parameter</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.get_parameter', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.get_parameter" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_parameter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Parameter&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the parameter given by ``target`` if it exists,</span>
<span class="sd">    otherwise throws an error.</span>
<span class="sd">    See the docstring for ``get_submodule`` for a more detailed</span>
<span class="sd">    explanation of this method&#39;s functionality as well as how to</span>
<span class="sd">    correctly specify ``target``.</span>
<span class="sd">    Args:</span>
<span class="sd">        target: The fully-qualified string name of the Parameter</span>
<span class="sd">            to look for. (See ``get_submodule`` for how to specify a</span>
<span class="sd">            fully-qualified string.)</span>
<span class="sd">    Returns:</span>
<span class="sd">        torch.nn.Parameter: The Parameter referenced by ``target``</span>
<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: If the target string references an invalid</span>
<span class="sd">            path or resolves to something that is not an</span>
<span class="sd">            ``nn.Parameter``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">module_path</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">param_name</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">rpartition</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
    <span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_submodule</span><span class="p">(</span><span class="n">module_path</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">param_name</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">_get_name</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot; has no attribute `&quot;</span>
                             <span class="o">+</span> <span class="n">param_name</span> <span class="o">+</span> <span class="s2">&quot;`&quot;</span><span class="p">)</span>
    <span class="n">param</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">param_name</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s2">&quot;`&quot;</span> <span class="o">+</span> <span class="n">param_name</span> <span class="o">+</span> <span class="s2">&quot;` is not an &quot;</span>
                             <span class="s2">&quot;nn.Parameter&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">param</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.get_submodule">
    <p>def <span class="ident">get_submodule</span>(</p><p>self, target: str)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the submodule given by <code>target</code> if it exists,
otherwise throws an error.</p>
<p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that
looks like this:</p>
<p>.. code-block::text</p>
<pre><code>A(
    (net_b): Module(
        (net_c): Module(
            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))
        )
        (linear): Linear(in_features=100, out_features=200, bias=True)
    )
)
</code></pre>
<p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested
submodule <code>net_b</code>, which itself has two submodules <code>net_c</code>
and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p>
<p>To check whether or not we have the <code>linear</code> submodule, we
would call <code>get_submodule("net_b.linear")</code>. To check whether
we have the <code>conv</code> submodule, we would call
<code>get_submodule("net_b.net_c.conv")</code>.</p>
<p>The runtime of <code>get_submodule</code> is bounded by the degree
of module nesting in <code>target</code>. A query against
<code>named_modules</code> achieves the same result, but it is O(N) in
the number of transitive modules. So, for a simple check to see
if some submodule exists, <code>get_submodule</code> should always be
used.</p>
<p>Args:
    target: The fully-qualified string name of the submodule
        to look for. (See above example for how to specify a
        fully-qualified string.)</p>
<p>Returns:
    torch.nn.Module: The submodule referenced by <code>target</code></p>
<p>Raises:
    AttributeError: If the target string references an invalid
        path or resolves to something that is not an
        <code>nn.Module</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.get_submodule', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.get_submodule" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_submodule</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Module&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the submodule given by ``target`` if it exists,</span>
<span class="sd">    otherwise throws an error.</span>
<span class="sd">    For example, let&#39;s say you have an ``nn.Module`` ``A`` that</span>
<span class="sd">    looks like this:</span>
<span class="sd">    .. code-block::text</span>
<span class="sd">        A(</span>
<span class="sd">            (net_b): Module(</span>
<span class="sd">                (net_c): Module(</span>
<span class="sd">                    (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))</span>
<span class="sd">                )</span>
<span class="sd">                (linear): Linear(in_features=100, out_features=200, bias=True)</span>
<span class="sd">            )</span>
<span class="sd">        )</span>
<span class="sd">    (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested</span>
<span class="sd">    submodule ``net_b``, which itself has two submodules ``net_c``</span>
<span class="sd">    and ``linear``. ``net_c`` then has a submodule ``conv``.)</span>
<span class="sd">    To check whether or not we have the ``linear`` submodule, we</span>
<span class="sd">    would call ``get_submodule(&quot;net_b.linear&quot;)``. To check whether</span>
<span class="sd">    we have the ``conv`` submodule, we would call</span>
<span class="sd">    ``get_submodule(&quot;net_b.net_c.conv&quot;)``.</span>
<span class="sd">    The runtime of ``get_submodule`` is bounded by the degree</span>
<span class="sd">    of module nesting in ``target``. A query against</span>
<span class="sd">    ``named_modules`` achieves the same result, but it is O(N) in</span>
<span class="sd">    the number of transitive modules. So, for a simple check to see</span>
<span class="sd">    if some submodule exists, ``get_submodule`` should always be</span>
<span class="sd">    used.</span>
<span class="sd">    Args:</span>
<span class="sd">        target: The fully-qualified string name of the submodule</span>
<span class="sd">            to look for. (See above example for how to specify a</span>
<span class="sd">            fully-qualified string.)</span>
<span class="sd">    Returns:</span>
<span class="sd">        torch.nn.Module: The submodule referenced by ``target``</span>
<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: If the target string references an invalid</span>
<span class="sd">            path or resolves to something that is not an</span>
<span class="sd">            ``nn.Module``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">target</span> <span class="o">==</span> <span class="s2">&quot;&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span>
    <span class="n">atoms</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
    <span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="bp">self</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">atoms</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">_get_name</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot; has no &quot;</span>
                                 <span class="s2">&quot;attribute `&quot;</span> <span class="o">+</span> <span class="n">item</span> <span class="o">+</span> <span class="s2">&quot;`&quot;</span><span class="p">)</span>
        <span class="n">mod</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">item</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s2">&quot;`&quot;</span> <span class="o">+</span> <span class="n">item</span> <span class="o">+</span> <span class="s2">&quot;` is not &quot;</span>
                                 <span class="s2">&quot;an nn.Module&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mod</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.half">
    <p>def <span class="ident">half</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.half', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.half" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">half</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all floating point parameters and buffers to ``half`` datatype.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">half</span><span class="p">()</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="k">else</span> <span class="n">t</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.load_state_dict">
    <p>def <span class="ident">load_state_dict</span>(</p><p>self, state_dict: &#39;OrderedDict[str, Tensor]&#39;, strict: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Copies parameters and buffers from :attr:<code>state_dict</code> into
this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then
the keys of :attr:<code>state_dict</code> must exactly match the keys returned
by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.</p>
<p>Args:
    state_dict (dict): a dict containing parameters and
        persistent buffers.
    strict (bool, optional): whether to strictly enforce that the keys
        in :attr:<code>state_dict</code> match the keys returned by this module's
        :meth:<code>~torch.nn.Module.state_dict</code> function. Default: <code>True</code></p>
<p>Returns:
    <code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:
        * <strong>missing_keys</strong> is a list of str containing the missing keys
        * <strong>unexpected_keys</strong> is a list of str containing the unexpected keys</p>
<p>Note:
    If a parameter or buffer is registered as <code>None</code> and its corresponding key
    exists in :attr:<code>state_dict</code>, :meth:<code>load_state_dict</code> will raise a
    <code>RuntimeError</code>.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.load_state_dict', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.load_state_dict" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="s1">&#39;OrderedDict[str, Tensor]&#39;</span><span class="p">,</span>
                    <span class="n">strict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Copies parameters and buffers from :attr:`state_dict` into</span>
<span class="sd">    this module and its descendants. If :attr:`strict` is ``True``, then</span>
<span class="sd">    the keys of :attr:`state_dict` must exactly match the keys returned</span>
<span class="sd">    by this module&#39;s :meth:`~torch.nn.Module.state_dict` function.</span>
<span class="sd">    Args:</span>
<span class="sd">        state_dict (dict): a dict containing parameters and</span>
<span class="sd">            persistent buffers.</span>
<span class="sd">        strict (bool, optional): whether to strictly enforce that the keys</span>
<span class="sd">            in :attr:`state_dict` match the keys returned by this module&#39;s</span>
<span class="sd">            :meth:`~torch.nn.Module.state_dict` function. Default: ``True``</span>
<span class="sd">    Returns:</span>
<span class="sd">        ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:</span>
<span class="sd">            * **missing_keys** is a list of str containing the missing keys</span>
<span class="sd">            * **unexpected_keys** is a list of str containing the unexpected keys</span>
<span class="sd">    Note:</span>
<span class="sd">        If a parameter or buffer is registered as ``None`` and its corresponding key</span>
<span class="sd">        exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a</span>
<span class="sd">        ``RuntimeError``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">missing_keys</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">unexpected_keys</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">error_msgs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># copy state_dict so _load_from_state_dict can modify it</span>
    <span class="n">metadata</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="s1">&#39;_metadata&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># mypy isn&#39;t aware that &quot;_metadata&quot; exists in state_dict</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">_metadata</span> <span class="o">=</span> <span class="n">metadata</span>  <span class="c1"># type: ignore[attr-defined]</span>
    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
        <span class="n">local_metadata</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="n">metadata</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">metadata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">prefix</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">{})</span>
        <span class="n">module</span><span class="o">.</span><span class="n">_load_from_state_dict</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">local_metadata</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">missing_keys</span><span class="p">,</span> <span class="n">unexpected_keys</span><span class="p">,</span> <span class="n">error_msgs</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">child</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">load</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">prefix</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>
    <span class="n">load</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">load</span>
    <span class="k">if</span> <span class="n">strict</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">unexpected_keys</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">error_msgs</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span>
                <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Unexpected key(s) in state_dict: </span><span class="si">{}</span><span class="s1">. &#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;&quot;</span><span class="si">{}</span><span class="s1">&quot;&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">unexpected_keys</span><span class="p">)))</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">missing_keys</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">error_msgs</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span>
                <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Missing key(s) in state_dict: </span><span class="si">{}</span><span class="s1">. &#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;&quot;</span><span class="si">{}</span><span class="s1">&quot;&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">missing_keys</span><span class="p">)))</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">error_msgs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Error(s) in loading state_dict for </span><span class="si">{}</span><span class="s1">:</span><span class="se">\n\t</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                           <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n\t</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">error_msgs</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">_IncompatibleKeys</span><span class="p">(</span><span class="n">missing_keys</span><span class="p">,</span> <span class="n">unexpected_keys</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.modules">
    <p>def <span class="ident">modules</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over all modules in the network.</p>
<p>Yields:
    Module: a module in the network</p>
<p>Note:
    Duplicate modules are returned only once. In the following
    example, <code>l</code> will be returned only once.</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; l = nn.Linear(2, 2)
&gt;&gt;&gt; net = nn.Sequential(l, l)
&gt;&gt;&gt; for idx, m in enumerate(net.modules()):
        print(idx, '-&gt;', m)

0 -&gt; Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
1 -&gt; Linear(in_features=2, out_features=2, bias=True)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.modules', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.modules" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">modules</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="s1">&#39;Module&#39;</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over all modules in the network.</span>
<span class="sd">    Yields:</span>
<span class="sd">        Module: a module in the network</span>
<span class="sd">    Note:</span>
<span class="sd">        Duplicate modules are returned only once. In the following</span>
<span class="sd">        example, ``l`` will be returned only once.</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; l = nn.Linear(2, 2)</span>
<span class="sd">        &gt;&gt;&gt; net = nn.Sequential(l, l)</span>
<span class="sd">        &gt;&gt;&gt; for idx, m in enumerate(net.modules()):</span>
<span class="sd">                print(idx, &#39;-&gt;&#39;, m)</span>
<span class="sd">        0 -&gt; Sequential(</span>
<span class="sd">          (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">          (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        )</span>
<span class="sd">        1 -&gt; Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
        <span class="k">yield</span> <span class="n">module</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.named_buffers">
    <p>def <span class="ident">named_buffers</span>(</p><p>self, prefix: str = &#39;&#39;, recurse: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over module buffers, yielding both the
name of the buffer as well as the buffer itself.</p>
<p>Args:
    prefix (str): prefix to prepend to all buffer names.
    recurse (bool): if True, then yields buffers of this module
        and all submodules. Otherwise, yields only buffers that
        are direct members of this module.</p>
<p>Yields:
    (string, torch.Tensor): Tuple containing the name and buffer</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for name, buf in self.named_buffers():
&gt;&gt;&gt;    if name in ['running_var']:
&gt;&gt;&gt;        print(buf.size())
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.named_buffers', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.named_buffers" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">named_buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over module buffers, yielding both the</span>
<span class="sd">    name of the buffer as well as the buffer itself.</span>
<span class="sd">    Args:</span>
<span class="sd">        prefix (str): prefix to prepend to all buffer names.</span>
<span class="sd">        recurse (bool): if True, then yields buffers of this module</span>
<span class="sd">            and all submodules. Otherwise, yields only buffers that</span>
<span class="sd">            are direct members of this module.</span>
<span class="sd">    Yields:</span>
<span class="sd">        (string, torch.Tensor): Tuple containing the name and buffer</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for name, buf in self.named_buffers():</span>
<span class="sd">        &gt;&gt;&gt;    if name in [&#39;running_var&#39;]:</span>
<span class="sd">        &gt;&gt;&gt;        print(buf.size())</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">gen</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_named_members</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">module</span><span class="p">:</span> <span class="n">module</span><span class="o">.</span><span class="n">_buffers</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span>
        <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span> <span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">gen</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">elem</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.named_children">
    <p>def <span class="ident">named_children</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over immediate children modules, yielding both
the name of the module as well as the module itself.</p>
<p>Yields:
    (string, Module): Tuple containing a name and child module</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for name, module in model.named_children():
&gt;&gt;&gt;     if name in ['conv4', 'conv5']:
&gt;&gt;&gt;         print(module)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.named_children', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.named_children" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">named_children</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s1">&#39;Module&#39;</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over immediate children modules, yielding both</span>
<span class="sd">    the name of the module as well as the module itself.</span>
<span class="sd">    Yields:</span>
<span class="sd">        (string, Module): Tuple containing a name and child module</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for name, module in model.named_children():</span>
<span class="sd">        &gt;&gt;&gt;     if name in [&#39;conv4&#39;, &#39;conv5&#39;]:</span>
<span class="sd">        &gt;&gt;&gt;         print(module)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">memo</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">module</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">memo</span><span class="p">:</span>
            <span class="n">memo</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
            <span class="k">yield</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.named_modules">
    <p>def <span class="ident">named_modules</span>(</p><p>self, memo: Union[Set[ForwardRef(&#39;Module&#39;)], NoneType] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over all modules in the network, yielding
both the name of the module as well as the module itself.</p>
<p>Args:
    memo: a memo to store the set of modules already added to the result
    prefix: a prefix that will be added to the name of the module
    remove_duplicate: whether to remove the duplicated module instances in the result
    or not</p>
<p>Yields:
    (string, Module): Tuple of name and module</p>
<p>Note:
    Duplicate modules are returned only once. In the following
    example, <code>l</code> will be returned only once.</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; l = nn.Linear(2, 2)
&gt;&gt;&gt; net = nn.Sequential(l, l)
&gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):
        print(idx, '-&gt;', m)

0 -&gt; ('', Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
))
1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.named_modules', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.named_modules" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">named_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memo</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Set</span><span class="p">[</span><span class="s1">&#39;Module&#39;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">remove_duplicate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over all modules in the network, yielding</span>
<span class="sd">    both the name of the module as well as the module itself.</span>
<span class="sd">    Args:</span>
<span class="sd">        memo: a memo to store the set of modules already added to the result</span>
<span class="sd">        prefix: a prefix that will be added to the name of the module</span>
<span class="sd">        remove_duplicate: whether to remove the duplicated module instances in the result</span>
<span class="sd">        or not</span>
<span class="sd">    Yields:</span>
<span class="sd">        (string, Module): Tuple of name and module</span>
<span class="sd">    Note:</span>
<span class="sd">        Duplicate modules are returned only once. In the following</span>
<span class="sd">        example, ``l`` will be returned only once.</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; l = nn.Linear(2, 2)</span>
<span class="sd">        &gt;&gt;&gt; net = nn.Sequential(l, l)</span>
<span class="sd">        &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):</span>
<span class="sd">                print(idx, &#39;-&gt;&#39;, m)</span>
<span class="sd">        0 -&gt; (&#39;&#39;, Sequential(</span>
<span class="sd">          (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">          (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        ))</span>
<span class="sd">        1 -&gt; (&#39;0&#39;, Linear(in_features=2, out_features=2, bias=True))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">memo</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">memo</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">if</span> <span class="bp">self</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">memo</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">remove_duplicate</span><span class="p">:</span>
            <span class="n">memo</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">prefix</span><span class="p">,</span> <span class="bp">self</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">submodule_prefix</span> <span class="o">=</span> <span class="n">prefix</span> <span class="o">+</span> <span class="p">(</span><span class="s1">&#39;.&#39;</span> <span class="k">if</span> <span class="n">prefix</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span><span class="p">)</span> <span class="o">+</span> <span class="n">name</span>
            <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_modules</span><span class="p">(</span><span class="n">memo</span><span class="p">,</span> <span class="n">submodule_prefix</span><span class="p">,</span> <span class="n">remove_duplicate</span><span class="p">):</span>
                <span class="k">yield</span> <span class="n">m</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.named_parameters">
    <p>def <span class="ident">named_parameters</span>(</p><p>self, prefix: str = &#39;&#39;, recurse: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over module parameters, yielding both the
name of the parameter as well as the parameter itself.</p>
<p>Args:
    prefix (str): prefix to prepend to all parameter names.
    recurse (bool): if True, then yields parameters of this module
        and all submodules. Otherwise, yields only parameters that
        are direct members of this module.</p>
<p>Yields:
    (string, Parameter): Tuple containing the name and parameter</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for name, param in self.named_parameters():
&gt;&gt;&gt;    if name in ['bias']:
&gt;&gt;&gt;        print(param.size())
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.named_parameters', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.named_parameters" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">named_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over module parameters, yielding both the</span>
<span class="sd">    name of the parameter as well as the parameter itself.</span>
<span class="sd">    Args:</span>
<span class="sd">        prefix (str): prefix to prepend to all parameter names.</span>
<span class="sd">        recurse (bool): if True, then yields parameters of this module</span>
<span class="sd">            and all submodules. Otherwise, yields only parameters that</span>
<span class="sd">            are direct members of this module.</span>
<span class="sd">    Yields:</span>
<span class="sd">        (string, Parameter): Tuple containing the name and parameter</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for name, param in self.named_parameters():</span>
<span class="sd">        &gt;&gt;&gt;    if name in [&#39;bias&#39;]:</span>
<span class="sd">        &gt;&gt;&gt;        print(param.size())</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">gen</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_named_members</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">module</span><span class="p">:</span> <span class="n">module</span><span class="o">.</span><span class="n">_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span>
        <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span> <span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">gen</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">elem</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.parameters">
    <p>def <span class="ident">parameters</span>(</p><p>self, recurse: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over module parameters.</p>
<p>This is typically passed to an optimizer.</p>
<p>Args:
    recurse (bool): if True, then yields parameters of this module
        and all submodules. Otherwise, yields only parameters that
        are direct members of this module.</p>
<p>Yields:
    Parameter: module parameter</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for param in model.parameters():
&gt;&gt;&gt;     print(type(param), param.size())
&lt;class 'torch.Tensor'&gt; (20L,)
&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.parameters', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.parameters" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Parameter</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over module parameters.</span>
<span class="sd">    This is typically passed to an optimizer.</span>
<span class="sd">    Args:</span>
<span class="sd">        recurse (bool): if True, then yields parameters of this module</span>
<span class="sd">            and all submodules. Otherwise, yields only parameters that</span>
<span class="sd">            are direct members of this module.</span>
<span class="sd">    Yields:</span>
<span class="sd">        Parameter: module parameter</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for param in model.parameters():</span>
<span class="sd">        &gt;&gt;&gt;     print(type(param), param.size())</span>
<span class="sd">        &lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="sd">        &lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">param</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.register_backward_hook">
    <p>def <span class="ident">register_backward_hook</span>(</p><p>self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]])</p>
    </div>
    

    
  
    <div class="desc"><p>Registers a backward hook on the module.</p>
<p>This function is deprecated in favor of :meth:<code>~torch.nn.Module.register_full_backward_hook</code> and
the behavior of this function will change in future versions.</p>
<p>Returns:
    :class:<code>torch.utils.hooks.RemovableHandle</code>:
        a handle that can be used to remove the added hook by calling
        <code>handle.remove()</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.register_backward_hook', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.register_backward_hook" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_backward_hook</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="s1">&#39;Module&#39;</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">],</span> <span class="n">Union</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a backward hook on the module.</span>
<span class="sd">    This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and</span>
<span class="sd">    the behavior of this function will change in future versions.</span>
<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_full_backward_hook</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Cannot use both regular backward hooks and full backward hooks on a &quot;</span>
                           <span class="s2">&quot;single Module. Please use only one of them.&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_is_full_backward_hook</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.register_buffer">
    <p>def <span class="ident">register_buffer</span>(</p><p>self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Adds a buffer to the module.</p>
<p>This is typically used to register a buffer that should not to be
considered a model parameter. For example, BatchNorm's <code>running_mean</code>
is not a parameter, but is part of the module's state. Buffers, by
default, are persistent and will be saved alongside parameters. This
behavior can be changed by setting :attr:<code>persistent</code> to <code>False</code>. The
only difference between a persistent buffer and a non-persistent buffer
is that the latter will not be a part of this module's
:attr:<code>state_dict</code>.</p>
<p>Buffers can be accessed as attributes using given names.</p>
<p>Args:
    name (string): name of the buffer. The buffer can be accessed
        from this module using the given name
    tensor (Tensor or None): buffer to be registered. If <code>None</code>, then operations
        that run on buffers, such as :attr:<code>cuda</code>, are ignored. If <code>None</code>,
        the buffer is <strong>not</strong> included in the module's :attr:<code>state_dict</code>.
    persistent (bool): whether the buffer is part of this module's
        :attr:<code>state_dict</code>.</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.register_buffer', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.register_buffer" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_buffer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">persistent</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Adds a buffer to the module.</span>
<span class="sd">    This is typically used to register a buffer that should not to be</span>
<span class="sd">    considered a model parameter. For example, BatchNorm&#39;s ``running_mean``</span>
<span class="sd">    is not a parameter, but is part of the module&#39;s state. Buffers, by</span>
<span class="sd">    default, are persistent and will be saved alongside parameters. This</span>
<span class="sd">    behavior can be changed by setting :attr:`persistent` to ``False``. The</span>
<span class="sd">    only difference between a persistent buffer and a non-persistent buffer</span>
<span class="sd">    is that the latter will not be a part of this module&#39;s</span>
<span class="sd">    :attr:`state_dict`.</span>
<span class="sd">    Buffers can be accessed as attributes using given names.</span>
<span class="sd">    Args:</span>
<span class="sd">        name (string): name of the buffer. The buffer can be accessed</span>
<span class="sd">            from this module using the given name</span>
<span class="sd">        tensor (Tensor or None): buffer to be registered. If ``None``, then operations</span>
<span class="sd">            that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,</span>
<span class="sd">            the buffer is **not** included in the module&#39;s :attr:`state_dict`.</span>
<span class="sd">        persistent (bool): whether the buffer is part of this module&#39;s</span>
<span class="sd">            :attr:`state_dict`.</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; self.register_buffer(&#39;running_mean&#39;, torch.zeros(num_features))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">persistent</span> <span class="ow">is</span> <span class="kc">False</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ScriptModule</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;ScriptModule does not support non-persistent buffers&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="s1">&#39;_buffers&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
            <span class="s2">&quot;cannot assign buffer before Module.__init__() call&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_six</span><span class="o">.</span><span class="n">string_classes</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;buffer name should be a string. &quot;</span>
                        <span class="s2">&quot;Got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">name</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="s1">&#39;.&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;buffer name can&#39;t contain </span><span class="se">\&quot;</span><span class="s2">.</span><span class="se">\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;buffer name can&#39;t be empty string </span><span class="se">\&quot;\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span> <span class="ow">and</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffers</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;attribute &#39;</span><span class="si">{}</span><span class="s2">&#39; already exists&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;cannot assign &#39;</span><span class="si">{}</span><span class="s2">&#39; object to buffer &#39;</span><span class="si">{}</span><span class="s2">&#39; &quot;</span>
                        <span class="s2">&quot;(torch Tensor or None required)&quot;</span>
                        <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">tensor</span><span class="p">),</span> <span class="n">name</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_buffers</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">tensor</span>
        <span class="k">if</span> <span class="n">persistent</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_non_persistent_buffers_set</span><span class="o">.</span><span class="n">discard</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_non_persistent_buffers_set</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.register_forward_hook">
    <p>def <span class="ident">register_forward_hook</span>(</p><p>self, hook: Callable[..., NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Registers a forward hook on the module.</p>
<p>The hook will be called every time after :func:<code>forward</code> has computed an output.
It should have the following signature::</p>
<pre><code>hook(module, input, output) -&gt; None or modified output
</code></pre>
<p>The input contains only the positional arguments given to the module.
Keyword arguments won't be passed to the hooks and only to the <code>forward</code>.
The hook can modify the output. It can modify the input inplace but
it will not have effect on forward since this is called after
:func:<code>forward</code> is called.</p>
<p>Returns:
    :class:<code>torch.utils.hooks.RemovableHandle</code>:
        a handle that can be used to remove the added hook by calling
        <code>handle.remove()</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.register_forward_hook', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.register_forward_hook" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_forward_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a forward hook on the module.</span>
<span class="sd">    The hook will be called every time after :func:`forward` has computed an output.</span>
<span class="sd">    It should have the following signature::</span>
<span class="sd">        hook(module, input, output) -&gt; None or modified output</span>
<span class="sd">    The input contains only the positional arguments given to the module.</span>
<span class="sd">    Keyword arguments won&#39;t be passed to the hooks and only to the ``forward``.</span>
<span class="sd">    The hook can modify the output. It can modify the input inplace but</span>
<span class="sd">    it will not have effect on forward since this is called after</span>
<span class="sd">    :func:`forward` is called.</span>
<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.register_forward_pre_hook">
    <p>def <span class="ident">register_forward_pre_hook</span>(</p><p>self, hook: Callable[..., NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Registers a forward pre-hook on the module.</p>
<p>The hook will be called every time before :func:<code>forward</code> is invoked.
It should have the following signature::</p>
<pre><code>hook(module, input) -&gt; None or modified input
</code></pre>
<p>The input contains only the positional arguments given to the module.
Keyword arguments won't be passed to the hooks and only to the <code>forward</code>.
The hook can modify the input. User can either return a tuple or a
single modified value in the hook. We will wrap the value into a tuple
if a single value is returned(unless that value is already a tuple).</p>
<p>Returns:
    :class:<code>torch.utils.hooks.RemovableHandle</code>:
        a handle that can be used to remove the added hook by calling
        <code>handle.remove()</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.register_forward_pre_hook', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.register_forward_pre_hook" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_forward_pre_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a forward pre-hook on the module.</span>
<span class="sd">    The hook will be called every time before :func:`forward` is invoked.</span>
<span class="sd">    It should have the following signature::</span>
<span class="sd">        hook(module, input) -&gt; None or modified input</span>
<span class="sd">    The input contains only the positional arguments given to the module.</span>
<span class="sd">    Keyword arguments won&#39;t be passed to the hooks and only to the ``forward``.</span>
<span class="sd">    The hook can modify the input. User can either return a tuple or a</span>
<span class="sd">    single modified value in the hook. We will wrap the value into a tuple</span>
<span class="sd">    if a single value is returned(unless that value is already a tuple).</span>
<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.register_full_backward_hook">
    <p>def <span class="ident">register_full_backward_hook</span>(</p><p>self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]])</p>
    </div>
    

    
  
    <div class="desc"><p>Registers a backward hook on the module.</p>
<p>The hook will be called every time the gradients with respect to module
inputs are computed. The hook should have the following signature::</p>
<pre><code>hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None
</code></pre>
<p>The :attr:<code>grad_input</code> and :attr:<code>grad_output</code> are tuples that contain the gradients
with respect to the inputs and outputs respectively. The hook should
not modify its arguments, but it can optionally return a new gradient with
respect to the input that will be used in place of :attr:<code>grad_input</code> in
subsequent computations. :attr:<code>grad_input</code> will only correspond to the inputs given
as positional arguments and all kwarg arguments are ignored. Entries
in :attr:<code>grad_input</code> and :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor
arguments.</p>
<p>For technical reasons, when this hook is applied to a Module, its forward function will
receive a view of each Tensor passed to the Module. Similarly the caller will receive a view
of each Tensor returned by the Module's forward function.</p>
<p>.. warning ::
    Modifying inputs or outputs inplace is not allowed when using backward hooks and
    will raise an error.</p>
<p>Returns:
    :class:<code>torch.utils.hooks.RemovableHandle</code>:
        a handle that can be used to remove the added hook by calling
        <code>handle.remove()</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.register_full_backward_hook', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.register_full_backward_hook" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_full_backward_hook</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="s1">&#39;Module&#39;</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">],</span> <span class="n">Union</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a backward hook on the module.</span>
<span class="sd">    The hook will be called every time the gradients with respect to module</span>
<span class="sd">    inputs are computed. The hook should have the following signature::</span>
<span class="sd">        hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None</span>
<span class="sd">    The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients</span>
<span class="sd">    with respect to the inputs and outputs respectively. The hook should</span>
<span class="sd">    not modify its arguments, but it can optionally return a new gradient with</span>
<span class="sd">    respect to the input that will be used in place of :attr:`grad_input` in</span>
<span class="sd">    subsequent computations. :attr:`grad_input` will only correspond to the inputs given</span>
<span class="sd">    as positional arguments and all kwarg arguments are ignored. Entries</span>
<span class="sd">    in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor</span>
<span class="sd">    arguments.</span>
<span class="sd">    For technical reasons, when this hook is applied to a Module, its forward function will</span>
<span class="sd">    receive a view of each Tensor passed to the Module. Similarly the caller will receive a view</span>
<span class="sd">    of each Tensor returned by the Module&#39;s forward function.</span>
<span class="sd">    .. warning ::</span>
<span class="sd">        Modifying inputs or outputs inplace is not allowed when using backward hooks and</span>
<span class="sd">        will raise an error.</span>
<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_full_backward_hook</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Cannot use both regular backward hooks and full backward hooks on a &quot;</span>
                           <span class="s2">&quot;single Module. Please use only one of them.&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_is_full_backward_hook</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.register_parameter">
    <p>def <span class="ident">register_parameter</span>(</p><p>self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Adds a parameter to the module.</p>
<p>The parameter can be accessed as an attribute using given name.</p>
<p>Args:
    name (string): name of the parameter. The parameter can be accessed
        from this module using the given name
    param (Parameter or None): parameter to be added to the module. If
        <code>None</code>, then operations that run on parameters, such as :attr:<code>cuda</code>,
        are ignored. If <code>None</code>, the parameter is <strong>not</strong> included in the
        module's :attr:<code>state_dict</code>.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.register_parameter', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.register_parameter" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_parameter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">param</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Parameter</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Adds a parameter to the module.</span>
<span class="sd">    The parameter can be accessed as an attribute using given name.</span>
<span class="sd">    Args:</span>
<span class="sd">        name (string): name of the parameter. The parameter can be accessed</span>
<span class="sd">            from this module using the given name</span>
<span class="sd">        param (Parameter or None): parameter to be added to the module. If</span>
<span class="sd">            ``None``, then operations that run on parameters, such as :attr:`cuda`,</span>
<span class="sd">            are ignored. If ``None``, the parameter is **not** included in the</span>
<span class="sd">            module&#39;s :attr:`state_dict`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="s1">&#39;_parameters&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
            <span class="s2">&quot;cannot assign parameter before Module.__init__() call&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_six</span><span class="o">.</span><span class="n">string_classes</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;parameter name should be a string. &quot;</span>
                        <span class="s2">&quot;Got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">name</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="s1">&#39;.&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;parameter name can&#39;t contain </span><span class="se">\&quot;</span><span class="s2">.</span><span class="se">\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;parameter name can&#39;t be empty string </span><span class="se">\&quot;\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span> <span class="ow">and</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parameters</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;attribute &#39;</span><span class="si">{}</span><span class="s2">&#39; already exists&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">param</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_parameters</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;cannot assign &#39;</span><span class="si">{}</span><span class="s2">&#39; object to parameter &#39;</span><span class="si">{}</span><span class="s2">&#39; &quot;</span>
                        <span class="s2">&quot;(torch.nn.Parameter or None required)&quot;</span>
                        <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">param</span><span class="p">),</span> <span class="n">name</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">param</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Cannot assign non-leaf Tensor to parameter &#39;</span><span class="si">{0}</span><span class="s2">&#39;. Model &quot;</span>
            <span class="s2">&quot;parameters must be created explicitly. To express &#39;</span><span class="si">{0}</span><span class="s2">&#39; &quot;</span>
            <span class="s2">&quot;as a function of another Tensor, compute the value in &quot;</span>
            <span class="s2">&quot;the forward() method.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_parameters</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">param</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.requires_grad_">
    <p>def <span class="ident">requires_grad_</span>(</p><p>self: ~T, requires_grad: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Change if autograd should record operations on parameters in this
module.</p>
<p>This method sets the parameters' :attr:<code>requires_grad</code> attributes
in-place.</p>
<p>This method is helpful for freezing part of the module for finetuning
or training parts of a model individually (e.g., GAN training).</p>
<p>See :ref:<code>locally-disable-grad-doc</code> for a comparison between
<code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.</p>
<p>Args:
    requires_grad (bool): whether autograd should record operations on
                          parameters in this module. Default: <code>True</code>.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.requires_grad_', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.requires_grad_" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Change if autograd should record operations on parameters in this</span>
<span class="sd">    module.</span>
<span class="sd">    This method sets the parameters&#39; :attr:`requires_grad` attributes</span>
<span class="sd">    in-place.</span>
<span class="sd">    This method is helpful for freezing part of the module for finetuning</span>
<span class="sd">    or training parts of a model individually (e.g., GAN training).</span>
<span class="sd">    See :ref:`locally-disable-grad-doc` for a comparison between</span>
<span class="sd">    `.requires_grad_()` and several similar mechanisms that may be confused with it.</span>
<span class="sd">    Args:</span>
<span class="sd">        requires_grad (bool): whether autograd should record operations on</span>
<span class="sd">                              parameters in this module. Default: ``True``.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="n">p</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="n">requires_grad</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.set_extra_state">
    <p>def <span class="ident">set_extra_state</span>(</p><p>self, state: Any)</p>
    </div>
    

    
  
    <div class="desc"><p>This function is called from :func:<code>load_state_dict</code> to handle any extra state
found within the <code>state_dict</code>. Implement this function and a corresponding
:func:<code>get_extra_state</code> for your module if you need to store extra state within its
<code>state_dict</code>.</p>
<p>Args:
    state (dict): Extra state from the <code>state_dict</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.set_extra_state', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.set_extra_state" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">set_extra_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function is called from :func:`load_state_dict` to handle any extra state</span>
<span class="sd">    found within the `state_dict`. Implement this function and a corresponding</span>
<span class="sd">    :func:`get_extra_state` for your module if you need to store extra state within its</span>
<span class="sd">    `state_dict`.</span>
<span class="sd">    Args:</span>
<span class="sd">        state (dict): Extra state from the `state_dict`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
        <span class="s2">&quot;Reached a code path in Module.set_extra_state() that should never be called. &quot;</span>
        <span class="s2">&quot;Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md &quot;</span>
        <span class="s2">&quot;to report this bug.&quot;</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.share_memory">
    <p>def <span class="ident">share_memory</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>See :meth:<code>torch.Tensor.share_memory_</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.share_memory', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.share_memory" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">share_memory</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;See :meth:`torch.Tensor.share_memory_`&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">share_memory_</span><span class="p">())</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.state_dict">
    <p>def <span class="ident">state_dict</span>(</p><p>self, destination=None, prefix=&#39;&#39;, keep_vars=False)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns a dictionary containing a whole state of the module.</p>
<p>Both parameters and persistent buffers (e.g. running averages) are
included. Keys are corresponding parameter and buffer names.
Parameters and buffers set to <code>None</code> are not included.</p>
<p>Returns:
    dict:
        a dictionary containing a whole state of the module</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; module.state_dict().keys()
['bias', 'weight']
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.state_dict', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.state_dict" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">destination</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">keep_vars</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns a dictionary containing a whole state of the module.</span>
<span class="sd">    Both parameters and persistent buffers (e.g. running averages) are</span>
<span class="sd">    included. Keys are corresponding parameter and buffer names.</span>
<span class="sd">    Parameters and buffers set to ``None`` are not included.</span>
<span class="sd">    Returns:</span>
<span class="sd">        dict:</span>
<span class="sd">            a dictionary containing a whole state of the module</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; module.state_dict().keys()</span>
<span class="sd">        [&#39;bias&#39;, &#39;weight&#39;]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">destination</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">destination</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="n">destination</span><span class="o">.</span><span class="n">_metadata</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
    <span class="n">destination</span><span class="o">.</span><span class="n">_metadata</span><span class="p">[</span><span class="n">prefix</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">local_metadata</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">version</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_version</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_save_to_state_dict</span><span class="p">(</span><span class="n">destination</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">keep_vars</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(</span><span class="n">destination</span><span class="p">,</span> <span class="n">prefix</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">keep_vars</span><span class="o">=</span><span class="n">keep_vars</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">hook</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_dict_hooks</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
        <span class="n">hook_result</span> <span class="o">=</span> <span class="n">hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">destination</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">local_metadata</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">hook_result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">destination</span> <span class="o">=</span> <span class="n">hook_result</span>
    <span class="k">return</span> <span class="n">destination</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.to">
    <p>def <span class="ident">to</span>(</p><p>self, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>Moves and/or casts the parameters and buffers.</p>
<p>This can be called as</p>
<p>.. function:: to(device=None, dtype=None, non_blocking=False)
   :noindex:</p>
<p>.. function:: to(dtype, non_blocking=False)
   :noindex:</p>
<p>.. function:: to(tensor, non_blocking=False)
   :noindex:</p>
<p>.. function:: to(memory_format=torch.channels_last)
   :noindex:</p>
<p>Its signature is similar to :meth:<code>torch.Tensor.to</code>, but only accepts
floating point or complex :attr:<code>dtype</code>\ s. In addition, this method will
only cast the floating point or complex parameters and buffers to :attr:<code>dtype</code>
(if given). The integral parameters and buffers will be moved
:attr:<code>device</code>, if that is given, but with dtypes unchanged. When
:attr:<code>non_blocking</code> is set, it tries to convert/move asynchronously
with respect to the host if possible, e.g., moving CPU Tensors with
pinned memory to CUDA devices.</p>
<p>See below for examples.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Args:
    device (:class:<code>torch.device</code>): the desired device of the parameters
        and buffers in this module
    dtype (:class:<code>torch.dtype</code>): the desired floating point or complex dtype of
        the parameters and buffers in this module
    tensor (torch.Tensor): Tensor whose dtype and device are the desired
        dtype and device for all parameters and buffers in this module
    memory_format (:class:<code>torch.memory_format</code>): the desired memory
        format for 4D parameters and buffers in this module (keyword
        only argument)</p>
<p>Returns:
    Module: self</p>
<p>Examples::</p>
<pre><code>&gt;&gt;&gt; linear = nn.Linear(2, 2)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1913, -0.3420],
        [-0.5113, -0.2325]])
&gt;&gt;&gt; linear.to(torch.double)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1913, -0.3420],
        [-0.5113, -0.2325]], dtype=torch.float64)
&gt;&gt;&gt; gpu1 = torch.device("cuda:1")
&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1914, -0.3420],
        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')
&gt;&gt;&gt; cpu = torch.device("cpu")
&gt;&gt;&gt; linear.to(cpu)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1914, -0.3420],
        [-0.5112, -0.2324]], dtype=torch.float16)

&gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.3741+0.j,  0.2382+0.j],
        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)
&gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))
tensor([[0.6122+0.j, 0.1150+0.j],
        [0.6122+0.j, 0.1150+0.j],
        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.to', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.to" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves and/or casts the parameters and buffers.</span>
<span class="sd">    This can be called as</span>
<span class="sd">    .. function:: to(device=None, dtype=None, non_blocking=False)</span>
<span class="sd">       :noindex:</span>
<span class="sd">    .. function:: to(dtype, non_blocking=False)</span>
<span class="sd">       :noindex:</span>
<span class="sd">    .. function:: to(tensor, non_blocking=False)</span>
<span class="sd">       :noindex:</span>
<span class="sd">    .. function:: to(memory_format=torch.channels_last)</span>
<span class="sd">       :noindex:</span>
<span class="sd">    Its signature is similar to :meth:`torch.Tensor.to`, but only accepts</span>
<span class="sd">    floating point or complex :attr:`dtype`\ s. In addition, this method will</span>
<span class="sd">    only cast the floating point or complex parameters and buffers to :attr:`dtype`</span>
<span class="sd">    (if given). The integral parameters and buffers will be moved</span>
<span class="sd">    :attr:`device`, if that is given, but with dtypes unchanged. When</span>
<span class="sd">    :attr:`non_blocking` is set, it tries to convert/move asynchronously</span>
<span class="sd">    with respect to the host if possible, e.g., moving CPU Tensors with</span>
<span class="sd">    pinned memory to CUDA devices.</span>
<span class="sd">    See below for examples.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Args:</span>
<span class="sd">        device (:class:`torch.device`): the desired device of the parameters</span>
<span class="sd">            and buffers in this module</span>
<span class="sd">        dtype (:class:`torch.dtype`): the desired floating point or complex dtype of</span>
<span class="sd">            the parameters and buffers in this module</span>
<span class="sd">        tensor (torch.Tensor): Tensor whose dtype and device are the desired</span>
<span class="sd">            dtype and device for all parameters and buffers in this module</span>
<span class="sd">        memory_format (:class:`torch.memory_format`): the desired memory</span>
<span class="sd">            format for 4D parameters and buffers in this module (keyword</span>
<span class="sd">            only argument)</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    Examples::</span>
<span class="sd">        &gt;&gt;&gt; linear = nn.Linear(2, 2)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.1913, -0.3420],</span>
<span class="sd">                [-0.5113, -0.2325]])</span>
<span class="sd">        &gt;&gt;&gt; linear.to(torch.double)</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.1913, -0.3420],</span>
<span class="sd">                [-0.5113, -0.2325]], dtype=torch.float64)</span>
<span class="sd">        &gt;&gt;&gt; gpu1 = torch.device(&quot;cuda:1&quot;)</span>
<span class="sd">        &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.1914, -0.3420],</span>
<span class="sd">                [-0.5112, -0.2324]], dtype=torch.float16, device=&#39;cuda:1&#39;)</span>
<span class="sd">        &gt;&gt;&gt; cpu = torch.device(&quot;cpu&quot;)</span>
<span class="sd">        &gt;&gt;&gt; linear.to(cpu)</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.1914, -0.3420],</span>
<span class="sd">                [-0.5112, -0.2324]], dtype=torch.float16)</span>
<span class="sd">        &gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.3741+0.j,  0.2382+0.j],</span>
<span class="sd">                [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)</span>
<span class="sd">        &gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))</span>
<span class="sd">        tensor([[0.6122+0.j, 0.1150+0.j],</span>
<span class="sd">                [0.6122+0.j, 0.1150+0.j],</span>
<span class="sd">                [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">,</span> <span class="n">convert_to_format</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">_parse_to</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_floating_point</span> <span class="ow">or</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;nn.Module.to only accepts floating point or complex &#39;</span>
                            <span class="s1">&#39;dtypes, but got desired dtype=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dtype</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Complex modules are a new feature under active development whose design may change, &quot;</span>
                <span class="s2">&quot;and some modules might not work as expected when using complex tensors as parameters or buffers. &quot;</span>
                <span class="s2">&quot;Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md &quot;</span>
                <span class="s2">&quot;if a complex module does not work as expected.&quot;</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">convert</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">convert_to_format</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">t</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">t</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="ow">or</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
                        <span class="n">non_blocking</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">convert_to_format</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">t</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="ow">or</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="n">convert</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.to_empty">
    <p>def <span class="ident">to_empty</span>(</p><p>self: ~T, *, device: Union[str, torch.device])</p>
    </div>
    

    
  
    <div class="desc"><p>Moves the parameters and buffers to the specified device without copying storage.</p>
<p>Args:
    device (:class:<code>torch.device</code>): The desired device of the parameters
        and buffers in this module.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.to_empty', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.to_empty" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">to_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves the parameters and buffers to the specified device without copying storage.</span>
<span class="sd">    Args:</span>
<span class="sd">        device (:class:`torch.device`): The desired device of the parameters</span>
<span class="sd">            and buffers in this module.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">))</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.train">
    <p>def <span class="ident">train</span>(</p><p>self: ~T, mode: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Sets the module in training mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>,
etc.</p>
<p>Args:
    mode (bool): whether to set training mode (<code>True</code>) or evaluation
                 mode (<code>False</code>). Default: <code>True</code>.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.train', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.train" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets the module in training mode.</span>
<span class="sd">    This has any effect only on certain modules. See documentations of</span>
<span class="sd">    particular modules for details of their behaviors in training/evaluation</span>
<span class="sd">    mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,</span>
<span class="sd">    etc.</span>
<span class="sd">    Args:</span>
<span class="sd">        mode (bool): whether to set training mode (``True``) or evaluation</span>
<span class="sd">                     mode (``False``). Default: ``True``.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;training mode is expected to be boolean&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="o">=</span> <span class="n">mode</span>
    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">():</span>
        <span class="n">module</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.type">
    <p>def <span class="ident">type</span>(</p><p>self: ~T, dst_type: Union[torch.dtype, str])</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Args:
    dst_type (type or string): the desired type</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.type', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.type" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">type</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">dst_type</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all parameters and buffers to :attr:`dst_type`.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Args:</span>
<span class="sd">        dst_type (type or string): the desired type</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">dst_type</span><span class="p">))</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.xpu">
    <p>def <span class="ident">xpu</span>(</p><p>self: ~T, device: Union[int, torch.device, NoneType] = None)</p>
    </div>
    

    
  
    <div class="desc"><p>Moves all model parameters and buffers to the XPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on XPU while being optimized.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Arguments:
    device (int, optional): if specified, all parameters will be
        copied to that device</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.xpu', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.xpu" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">xpu</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves all model parameters and buffers to the XPU.</span>
<span class="sd">    This also makes associated parameters and buffers different objects. So</span>
<span class="sd">    it should be called before constructing optimizer if the module will</span>
<span class="sd">    live on XPU while being optimized.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Arguments:</span>
<span class="sd">        device (int, optional): if specified, all parameters will be</span>
<span class="sd">            copied to that device</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">xpu</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Decoder.zero_grad">
    <p>def <span class="ident">zero_grad</span>(</p><p>self, set_to_none: bool = False)</p>
    </div>
    

    
  
    <div class="desc"><p>Sets gradients of all model parameters to zero. See similar function
under :class:<code>torch.optim.Optimizer</code> for more context.</p>
<p>Args:
    set_to_none (bool): instead of setting to zero, set the grads to None.
        See :meth:<code>torch.optim.Optimizer.zero_grad</code> for details.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Decoder.zero_grad', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Decoder.zero_grad" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">set_to_none</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets gradients of all model parameters to zero. See similar function</span>
<span class="sd">    under :class:`torch.optim.Optimizer` for more context.</span>
<span class="sd">    Args:</span>
<span class="sd">        set_to_none (bool): instead of setting to zero, set the grads to None.</span>
<span class="sd">            See :meth:`torch.optim.Optimizer.zero_grad` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_is_replica&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;Calling .zero_grad() from a module created with nn.DataParallel() has no effect. &quot;</span>
            <span class="s2">&quot;The parameters are copied (in a differentiable manner) from the original module. &quot;</span>
            <span class="s2">&quot;This means they are not leaf nodes in autograd and so don&#39;t accumulate gradients. &quot;</span>
            <span class="s2">&quot;If you need gradients in your forward method, consider using autograd.grad instead.&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">set_to_none</span><span class="p">:</span>
                <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">grad_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">detach_</span><span class="p">()</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
                <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</pre></div>

  </div>
</div>

  </div>
  
      </div>
      </div>
      
      <div class="item">
      <p id="seq2seq.DecoderLayer" class="name">class <span class="ident">DecoderLayer</span></p>
      
  
    <div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p></div>
  <div class="source_cont">
</div>


      <div class="class">
          <h3>Ancestors (in MRO)</h3>
          <ul class="class_list">
          <li><a href="/torch.nn.modules.module.Module.ext">torch.nn.modules.module.Module</a></li>
          </ul>
          <h3>Class variables</h3>
            <div class="item">
            <p id="seq2seq.DecoderLayer.T_destination" class="name">var <span class="ident">T_destination</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="seq2seq.DecoderLayer.dump_patches" class="name">var <span class="ident">dump_patches</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
          <h3>Methods</h3>
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.add_module">
    <p>def <span class="ident">add_module</span>(</p><p>self, name: str, module: Union[ForwardRef(&#39;Module&#39;), NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Adds a child module to the current module.</p>
<p>The module can be accessed as an attribute using the given name.</p>
<p>Args:
    name (string): name of the child module. The child module can be
        accessed from this module using the given name
    module (Module): child module to be added to the module.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.add_module', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.add_module" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">add_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s1">&#39;Module&#39;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Adds a child module to the current module.</span>
<span class="sd">    The module can be accessed as an attribute using the given name.</span>
<span class="sd">    Args:</span>
<span class="sd">        name (string): name of the child module. The child module can be</span>
<span class="sd">            accessed from this module using the given name</span>
<span class="sd">        module (Module): child module to be added to the module.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">Module</span><span class="p">)</span> <span class="ow">and</span> <span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> is not a Module subclass&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">module</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_six</span><span class="o">.</span><span class="n">string_classes</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;module name should be a string. Got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">name</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span> <span class="ow">and</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;attribute &#39;</span><span class="si">{}</span><span class="s2">&#39; already exists&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">elif</span> <span class="s1">&#39;.&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;module name can&#39;t contain </span><span class="se">\&quot;</span><span class="s2">.</span><span class="se">\&quot;</span><span class="s2">, got: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;module name can&#39;t be empty string </span><span class="se">\&quot;\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.apply">
    <p>def <span class="ident">apply</span>(</p><p>self: ~T, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>)
as well as self. Typical use includes initializing the parameters of a model
(see also :ref:<code>nn-init-doc</code>).</p>
<p>Args:
    fn (:class:<code>Module</code> -&gt; None): function to be applied to each submodule</p>
<p>Returns:
    Module: self</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; @torch.no_grad()
&gt;&gt;&gt; def init_weights(m):
&gt;&gt;&gt;     print(m)
&gt;&gt;&gt;     if type(m) == nn.Linear:
&gt;&gt;&gt;         m.weight.fill_(1.0)
&gt;&gt;&gt;         print(m.weight)
&gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))
&gt;&gt;&gt; net.apply(init_weights)
Linear(in_features=2, out_features=2, bias=True)
Parameter containing:
tensor([[ 1.,  1.],
        [ 1.,  1.]])
Linear(in_features=2, out_features=2, bias=True)
Parameter containing:
tensor([[ 1.,  1.],
        [ 1.,  1.]])
Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.apply', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.apply" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="s1">&#39;Module&#39;</span><span class="p">],</span> <span class="kc">None</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies ``fn`` recursively to every submodule (as returned by ``.children()``)</span>
<span class="sd">    as well as self. Typical use includes initializing the parameters of a model</span>
<span class="sd">    (see also :ref:`nn-init-doc`).</span>
<span class="sd">    Args:</span>
<span class="sd">        fn (:class:`Module` -&gt; None): function to be applied to each submodule</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; @torch.no_grad()</span>
<span class="sd">        &gt;&gt;&gt; def init_weights(m):</span>
<span class="sd">        &gt;&gt;&gt;     print(m)</span>
<span class="sd">        &gt;&gt;&gt;     if type(m) == nn.Linear:</span>
<span class="sd">        &gt;&gt;&gt;         m.weight.fill_(1.0)</span>
<span class="sd">        &gt;&gt;&gt;         print(m.weight)</span>
<span class="sd">        &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))</span>
<span class="sd">        &gt;&gt;&gt; net.apply(init_weights)</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 1.,  1.],</span>
<span class="sd">                [ 1.,  1.]])</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 1.,  1.],</span>
<span class="sd">                [ 1.,  1.]])</span>
<span class="sd">        Sequential(</span>
<span class="sd">          (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">          (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        )</span>
<span class="sd">        Sequential(</span>
<span class="sd">          (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">          (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        )</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">():</span>
        <span class="n">module</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
    <span class="n">fn</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.bfloat16">
    <p>def <span class="ident">bfloat16</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.bfloat16', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.bfloat16" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">bfloat16</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all floating point parameters and buffers to ``bfloat16`` datatype.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">()</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="k">else</span> <span class="n">t</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.buffers">
    <p>def <span class="ident">buffers</span>(</p><p>self, recurse: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over module buffers.</p>
<p>Args:
    recurse (bool): if True, then yields buffers of this module
        and all submodules. Otherwise, yields only buffers that
        are direct members of this module.</p>
<p>Yields:
    torch.Tensor: module buffer</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for buf in model.buffers():
&gt;&gt;&gt;     print(type(buf), buf.size())
&lt;class 'torch.Tensor'&gt; (20L,)
&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.buffers', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.buffers" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over module buffers.</span>
<span class="sd">    Args:</span>
<span class="sd">        recurse (bool): if True, then yields buffers of this module</span>
<span class="sd">            and all submodules. Otherwise, yields only buffers that</span>
<span class="sd">            are direct members of this module.</span>
<span class="sd">    Yields:</span>
<span class="sd">        torch.Tensor: module buffer</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for buf in model.buffers():</span>
<span class="sd">        &gt;&gt;&gt;     print(type(buf), buf.size())</span>
<span class="sd">        &lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="sd">        &lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">buf</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">buf</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.children">
    <p>def <span class="ident">children</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over immediate children modules.</p>
<p>Yields:
    Module: a child module</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.children', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.children" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">children</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="s1">&#39;Module&#39;</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over immediate children modules.</span>
<span class="sd">    Yields:</span>
<span class="sd">        Module: a child module</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
        <span class="k">yield</span> <span class="n">module</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.cpu">
    <p>def <span class="ident">cpu</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Moves all model parameters and buffers to the CPU.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.cpu', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.cpu" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves all model parameters and buffers to the CPU.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.cuda">
    <p>def <span class="ident">cuda</span>(</p><p>self: ~T, device: Union[int, torch.device, NoneType] = None)</p>
    </div>
    

    
  
    <div class="desc"><p>Moves all model parameters and buffers to the GPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on GPU while being optimized.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Args:
    device (int, optional): if specified, all parameters will be
        copied to that device</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.cuda', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.cuda" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">cuda</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves all model parameters and buffers to the GPU.</span>
<span class="sd">    This also makes associated parameters and buffers different objects. So</span>
<span class="sd">    it should be called before constructing optimizer if the module will</span>
<span class="sd">    live on GPU while being optimized.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Args:</span>
<span class="sd">        device (int, optional): if specified, all parameters will be</span>
<span class="sd">            copied to that device</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.double">
    <p>def <span class="ident">double</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.double', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.double" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">double</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all floating point parameters and buffers to ``double`` datatype.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">double</span><span class="p">()</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="k">else</span> <span class="n">t</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.eval">
    <p>def <span class="ident">eval</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Sets the module in evaluation mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>,
etc.</p>
<p>This is equivalent with :meth:<code>self.train(False) &lt;torch.nn.Module.train&gt;</code>.</p>
<p>See :ref:<code>locally-disable-grad-doc</code> for a comparison between
<code>.eval()</code> and several similar mechanisms that may be confused with it.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.eval', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.eval" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets the module in evaluation mode.</span>
<span class="sd">    This has any effect only on certain modules. See documentations of</span>
<span class="sd">    particular modules for details of their behaviors in training/evaluation</span>
<span class="sd">    mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,</span>
<span class="sd">    etc.</span>
<span class="sd">    This is equivalent with :meth:`self.train(False) &lt;torch.nn.Module.train&gt;`.</span>
<span class="sd">    See :ref:`locally-disable-grad-doc` for a comparison between</span>
<span class="sd">    `.eval()` and several similar mechanisms that may be confused with it.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.extra_repr">
    <p>def <span class="ident">extra_repr</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.extra_repr', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.extra_repr" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Set the extra representation of the module</span>
<span class="sd">    To print customized extra information, you should re-implement</span>
<span class="sd">    this method in your own modules. Both single-line and multi-line</span>
<span class="sd">    strings are acceptable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="s1">&#39;&#39;</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.float">
    <p>def <span class="ident">float</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.float', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.float" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">float</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all floating point parameters and buffers to ``float`` datatype.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="k">else</span> <span class="n">t</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.forward">
    <p>def <span class="ident">forward</span>(</p><p>self, trg, enc_src, trg_mask, src_mask)</p>
    </div>
    

    
  
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.forward', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.forward" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trg</span><span class="p">,</span> <span class="n">enc_src</span><span class="p">,</span> <span class="n">trg_mask</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">):</span>
    
    <span class="c1">#trg = [batch size, trg len, hid dim]</span>
    <span class="c1">#enc_src = [batch size, src len, hid dim]</span>
    <span class="c1">#trg_mask = [batch size, 1, trg len, trg len]</span>
    <span class="c1">#src_mask = [batch size, 1, 1, src len]</span>
    
    <span class="c1">#self attention</span>
    <span class="n">_trg</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attention</span><span class="p">(</span><span class="n">trg</span><span class="p">,</span> <span class="n">trg</span><span class="p">,</span> <span class="n">trg</span><span class="p">,</span> <span class="n">trg_mask</span><span class="p">)</span>
    
    <span class="c1">#dropout, residual connection and layer norm</span>
    <span class="n">trg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_layer_norm</span><span class="p">(</span><span class="n">trg</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">_trg</span><span class="p">))</span>
        
    <span class="c1">#trg = [batch size, trg len, hid dim]</span>
        
    <span class="c1">#encoder attention</span>
    <span class="n">_trg</span><span class="p">,</span> <span class="n">attention</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_attention</span><span class="p">(</span><span class="n">trg</span><span class="p">,</span> <span class="n">enc_src</span><span class="p">,</span> <span class="n">enc_src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
    
    <span class="c1">#dropout, residual connection and layer norm</span>
    <span class="n">trg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc_attn_layer_norm</span><span class="p">(</span><span class="n">trg</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">_trg</span><span class="p">))</span>
                
    <span class="c1">#trg = [batch size, trg len, hid dim]</span>
    
    <span class="c1">#positionwise feedforward</span>
    <span class="n">_trg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">positionwise_feedforward</span><span class="p">(</span><span class="n">trg</span><span class="p">)</span>
    
    <span class="c1">#dropout, residual and layer norm</span>
    <span class="n">trg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff_layer_norm</span><span class="p">(</span><span class="n">trg</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">_trg</span><span class="p">))</span>
    
    <span class="c1">#trg = [batch size, trg len, hid dim]</span>
    <span class="c1">#attention = [batch size, n heads, trg len, src len]</span>
    
    <span class="k">return</span> <span class="n">trg</span><span class="p">,</span> <span class="n">attention</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.get_buffer">
    <p>def <span class="ident">get_buffer</span>(</p><p>self, target: str)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the buffer given by <code>target</code> if it exists,
otherwise throws an error.</p>
<p>See the docstring for <code>get_submodule</code> for a more detailed
explanation of this method's functionality as well as how to
correctly specify <code>target</code>.</p>
<p>Args:
    target: The fully-qualified string name of the buffer
        to look for. (See <code>get_submodule</code> for how to specify a
        fully-qualified string.)</p>
<p>Returns:
    torch.Tensor: The buffer referenced by <code>target</code></p>
<p>Raises:
    AttributeError: If the target string references an invalid
        path or resolves to something that is not a
        buffer</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.get_buffer', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.get_buffer" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_buffer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the buffer given by ``target`` if it exists,</span>
<span class="sd">    otherwise throws an error.</span>
<span class="sd">    See the docstring for ``get_submodule`` for a more detailed</span>
<span class="sd">    explanation of this method&#39;s functionality as well as how to</span>
<span class="sd">    correctly specify ``target``.</span>
<span class="sd">    Args:</span>
<span class="sd">        target: The fully-qualified string name of the buffer</span>
<span class="sd">            to look for. (See ``get_submodule`` for how to specify a</span>
<span class="sd">            fully-qualified string.)</span>
<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: The buffer referenced by ``target``</span>
<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: If the target string references an invalid</span>
<span class="sd">            path or resolves to something that is not a</span>
<span class="sd">            buffer</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">module_path</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">buffer_name</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">rpartition</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
    <span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_submodule</span><span class="p">(</span><span class="n">module_path</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">buffer_name</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">_get_name</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot; has no attribute `&quot;</span>
                             <span class="o">+</span> <span class="n">buffer_name</span> <span class="o">+</span> <span class="s2">&quot;`&quot;</span><span class="p">)</span>
    <span class="n">buffer</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">buffer_name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">buffer_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">mod</span><span class="o">.</span><span class="n">_buffers</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s2">&quot;`&quot;</span> <span class="o">+</span> <span class="n">buffer_name</span> <span class="o">+</span> <span class="s2">&quot;` is not a buffer&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">buffer</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.get_extra_state">
    <p>def <span class="ident">get_extra_state</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns any extra state to include in the module's state_dict.
Implement this and a corresponding :func:<code>set_extra_state</code> for your module
if you need to store extra state. This function is called when building the
module's <code>state_dict()</code>.</p>
<p>Note that extra state should be pickleable to ensure working serialization
of the state_dict. We only provide provide backwards compatibility guarantees
for serializing Tensors; other objects may break backwards compatibility if
their serialized pickled form changes.</p>
<p>Returns:
    object: Any extra state to store in the module's state_dict</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.get_extra_state', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.get_extra_state" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_extra_state</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns any extra state to include in the module&#39;s state_dict.</span>
<span class="sd">    Implement this and a corresponding :func:`set_extra_state` for your module</span>
<span class="sd">    if you need to store extra state. This function is called when building the</span>
<span class="sd">    module&#39;s `state_dict()`.</span>
<span class="sd">    Note that extra state should be pickleable to ensure working serialization</span>
<span class="sd">    of the state_dict. We only provide provide backwards compatibility guarantees</span>
<span class="sd">    for serializing Tensors; other objects may break backwards compatibility if</span>
<span class="sd">    their serialized pickled form changes.</span>
<span class="sd">    Returns:</span>
<span class="sd">        object: Any extra state to store in the module&#39;s state_dict</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
        <span class="s2">&quot;Reached a code path in Module.get_extra_state() that should never be called. &quot;</span>
        <span class="s2">&quot;Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md &quot;</span>
        <span class="s2">&quot;to report this bug.&quot;</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.get_parameter">
    <p>def <span class="ident">get_parameter</span>(</p><p>self, target: str)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the parameter given by <code>target</code> if it exists,
otherwise throws an error.</p>
<p>See the docstring for <code>get_submodule</code> for a more detailed
explanation of this method's functionality as well as how to
correctly specify <code>target</code>.</p>
<p>Args:
    target: The fully-qualified string name of the Parameter
        to look for. (See <code>get_submodule</code> for how to specify a
        fully-qualified string.)</p>
<p>Returns:
    torch.nn.Parameter: The Parameter referenced by <code>target</code></p>
<p>Raises:
    AttributeError: If the target string references an invalid
        path or resolves to something that is not an
        <code>nn.Parameter</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.get_parameter', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.get_parameter" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_parameter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Parameter&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the parameter given by ``target`` if it exists,</span>
<span class="sd">    otherwise throws an error.</span>
<span class="sd">    See the docstring for ``get_submodule`` for a more detailed</span>
<span class="sd">    explanation of this method&#39;s functionality as well as how to</span>
<span class="sd">    correctly specify ``target``.</span>
<span class="sd">    Args:</span>
<span class="sd">        target: The fully-qualified string name of the Parameter</span>
<span class="sd">            to look for. (See ``get_submodule`` for how to specify a</span>
<span class="sd">            fully-qualified string.)</span>
<span class="sd">    Returns:</span>
<span class="sd">        torch.nn.Parameter: The Parameter referenced by ``target``</span>
<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: If the target string references an invalid</span>
<span class="sd">            path or resolves to something that is not an</span>
<span class="sd">            ``nn.Parameter``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">module_path</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">param_name</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">rpartition</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
    <span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_submodule</span><span class="p">(</span><span class="n">module_path</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">param_name</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">_get_name</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot; has no attribute `&quot;</span>
                             <span class="o">+</span> <span class="n">param_name</span> <span class="o">+</span> <span class="s2">&quot;`&quot;</span><span class="p">)</span>
    <span class="n">param</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">param_name</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s2">&quot;`&quot;</span> <span class="o">+</span> <span class="n">param_name</span> <span class="o">+</span> <span class="s2">&quot;` is not an &quot;</span>
                             <span class="s2">&quot;nn.Parameter&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">param</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.get_submodule">
    <p>def <span class="ident">get_submodule</span>(</p><p>self, target: str)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the submodule given by <code>target</code> if it exists,
otherwise throws an error.</p>
<p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that
looks like this:</p>
<p>.. code-block::text</p>
<pre><code>A(
    (net_b): Module(
        (net_c): Module(
            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))
        )
        (linear): Linear(in_features=100, out_features=200, bias=True)
    )
)
</code></pre>
<p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested
submodule <code>net_b</code>, which itself has two submodules <code>net_c</code>
and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p>
<p>To check whether or not we have the <code>linear</code> submodule, we
would call <code>get_submodule("net_b.linear")</code>. To check whether
we have the <code>conv</code> submodule, we would call
<code>get_submodule("net_b.net_c.conv")</code>.</p>
<p>The runtime of <code>get_submodule</code> is bounded by the degree
of module nesting in <code>target</code>. A query against
<code>named_modules</code> achieves the same result, but it is O(N) in
the number of transitive modules. So, for a simple check to see
if some submodule exists, <code>get_submodule</code> should always be
used.</p>
<p>Args:
    target: The fully-qualified string name of the submodule
        to look for. (See above example for how to specify a
        fully-qualified string.)</p>
<p>Returns:
    torch.nn.Module: The submodule referenced by <code>target</code></p>
<p>Raises:
    AttributeError: If the target string references an invalid
        path or resolves to something that is not an
        <code>nn.Module</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.get_submodule', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.get_submodule" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_submodule</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Module&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the submodule given by ``target`` if it exists,</span>
<span class="sd">    otherwise throws an error.</span>
<span class="sd">    For example, let&#39;s say you have an ``nn.Module`` ``A`` that</span>
<span class="sd">    looks like this:</span>
<span class="sd">    .. code-block::text</span>
<span class="sd">        A(</span>
<span class="sd">            (net_b): Module(</span>
<span class="sd">                (net_c): Module(</span>
<span class="sd">                    (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))</span>
<span class="sd">                )</span>
<span class="sd">                (linear): Linear(in_features=100, out_features=200, bias=True)</span>
<span class="sd">            )</span>
<span class="sd">        )</span>
<span class="sd">    (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested</span>
<span class="sd">    submodule ``net_b``, which itself has two submodules ``net_c``</span>
<span class="sd">    and ``linear``. ``net_c`` then has a submodule ``conv``.)</span>
<span class="sd">    To check whether or not we have the ``linear`` submodule, we</span>
<span class="sd">    would call ``get_submodule(&quot;net_b.linear&quot;)``. To check whether</span>
<span class="sd">    we have the ``conv`` submodule, we would call</span>
<span class="sd">    ``get_submodule(&quot;net_b.net_c.conv&quot;)``.</span>
<span class="sd">    The runtime of ``get_submodule`` is bounded by the degree</span>
<span class="sd">    of module nesting in ``target``. A query against</span>
<span class="sd">    ``named_modules`` achieves the same result, but it is O(N) in</span>
<span class="sd">    the number of transitive modules. So, for a simple check to see</span>
<span class="sd">    if some submodule exists, ``get_submodule`` should always be</span>
<span class="sd">    used.</span>
<span class="sd">    Args:</span>
<span class="sd">        target: The fully-qualified string name of the submodule</span>
<span class="sd">            to look for. (See above example for how to specify a</span>
<span class="sd">            fully-qualified string.)</span>
<span class="sd">    Returns:</span>
<span class="sd">        torch.nn.Module: The submodule referenced by ``target``</span>
<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: If the target string references an invalid</span>
<span class="sd">            path or resolves to something that is not an</span>
<span class="sd">            ``nn.Module``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">target</span> <span class="o">==</span> <span class="s2">&quot;&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span>
    <span class="n">atoms</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
    <span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="bp">self</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">atoms</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">_get_name</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot; has no &quot;</span>
                                 <span class="s2">&quot;attribute `&quot;</span> <span class="o">+</span> <span class="n">item</span> <span class="o">+</span> <span class="s2">&quot;`&quot;</span><span class="p">)</span>
        <span class="n">mod</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">item</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s2">&quot;`&quot;</span> <span class="o">+</span> <span class="n">item</span> <span class="o">+</span> <span class="s2">&quot;` is not &quot;</span>
                                 <span class="s2">&quot;an nn.Module&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mod</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.half">
    <p>def <span class="ident">half</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.half', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.half" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">half</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all floating point parameters and buffers to ``half`` datatype.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">half</span><span class="p">()</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="k">else</span> <span class="n">t</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.load_state_dict">
    <p>def <span class="ident">load_state_dict</span>(</p><p>self, state_dict: &#39;OrderedDict[str, Tensor]&#39;, strict: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Copies parameters and buffers from :attr:<code>state_dict</code> into
this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then
the keys of :attr:<code>state_dict</code> must exactly match the keys returned
by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.</p>
<p>Args:
    state_dict (dict): a dict containing parameters and
        persistent buffers.
    strict (bool, optional): whether to strictly enforce that the keys
        in :attr:<code>state_dict</code> match the keys returned by this module's
        :meth:<code>~torch.nn.Module.state_dict</code> function. Default: <code>True</code></p>
<p>Returns:
    <code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:
        * <strong>missing_keys</strong> is a list of str containing the missing keys
        * <strong>unexpected_keys</strong> is a list of str containing the unexpected keys</p>
<p>Note:
    If a parameter or buffer is registered as <code>None</code> and its corresponding key
    exists in :attr:<code>state_dict</code>, :meth:<code>load_state_dict</code> will raise a
    <code>RuntimeError</code>.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.load_state_dict', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.load_state_dict" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="s1">&#39;OrderedDict[str, Tensor]&#39;</span><span class="p">,</span>
                    <span class="n">strict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Copies parameters and buffers from :attr:`state_dict` into</span>
<span class="sd">    this module and its descendants. If :attr:`strict` is ``True``, then</span>
<span class="sd">    the keys of :attr:`state_dict` must exactly match the keys returned</span>
<span class="sd">    by this module&#39;s :meth:`~torch.nn.Module.state_dict` function.</span>
<span class="sd">    Args:</span>
<span class="sd">        state_dict (dict): a dict containing parameters and</span>
<span class="sd">            persistent buffers.</span>
<span class="sd">        strict (bool, optional): whether to strictly enforce that the keys</span>
<span class="sd">            in :attr:`state_dict` match the keys returned by this module&#39;s</span>
<span class="sd">            :meth:`~torch.nn.Module.state_dict` function. Default: ``True``</span>
<span class="sd">    Returns:</span>
<span class="sd">        ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:</span>
<span class="sd">            * **missing_keys** is a list of str containing the missing keys</span>
<span class="sd">            * **unexpected_keys** is a list of str containing the unexpected keys</span>
<span class="sd">    Note:</span>
<span class="sd">        If a parameter or buffer is registered as ``None`` and its corresponding key</span>
<span class="sd">        exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a</span>
<span class="sd">        ``RuntimeError``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">missing_keys</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">unexpected_keys</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">error_msgs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># copy state_dict so _load_from_state_dict can modify it</span>
    <span class="n">metadata</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="s1">&#39;_metadata&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># mypy isn&#39;t aware that &quot;_metadata&quot; exists in state_dict</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">_metadata</span> <span class="o">=</span> <span class="n">metadata</span>  <span class="c1"># type: ignore[attr-defined]</span>
    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
        <span class="n">local_metadata</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="n">metadata</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">metadata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">prefix</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">{})</span>
        <span class="n">module</span><span class="o">.</span><span class="n">_load_from_state_dict</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">local_metadata</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">missing_keys</span><span class="p">,</span> <span class="n">unexpected_keys</span><span class="p">,</span> <span class="n">error_msgs</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">child</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">load</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">prefix</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>
    <span class="n">load</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">load</span>
    <span class="k">if</span> <span class="n">strict</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">unexpected_keys</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">error_msgs</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span>
                <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Unexpected key(s) in state_dict: </span><span class="si">{}</span><span class="s1">. &#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;&quot;</span><span class="si">{}</span><span class="s1">&quot;&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">unexpected_keys</span><span class="p">)))</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">missing_keys</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">error_msgs</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span>
                <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Missing key(s) in state_dict: </span><span class="si">{}</span><span class="s1">. &#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;&quot;</span><span class="si">{}</span><span class="s1">&quot;&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">missing_keys</span><span class="p">)))</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">error_msgs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Error(s) in loading state_dict for </span><span class="si">{}</span><span class="s1">:</span><span class="se">\n\t</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                           <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n\t</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">error_msgs</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">_IncompatibleKeys</span><span class="p">(</span><span class="n">missing_keys</span><span class="p">,</span> <span class="n">unexpected_keys</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.modules">
    <p>def <span class="ident">modules</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over all modules in the network.</p>
<p>Yields:
    Module: a module in the network</p>
<p>Note:
    Duplicate modules are returned only once. In the following
    example, <code>l</code> will be returned only once.</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; l = nn.Linear(2, 2)
&gt;&gt;&gt; net = nn.Sequential(l, l)
&gt;&gt;&gt; for idx, m in enumerate(net.modules()):
        print(idx, '-&gt;', m)

0 -&gt; Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
1 -&gt; Linear(in_features=2, out_features=2, bias=True)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.modules', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.modules" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">modules</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="s1">&#39;Module&#39;</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over all modules in the network.</span>
<span class="sd">    Yields:</span>
<span class="sd">        Module: a module in the network</span>
<span class="sd">    Note:</span>
<span class="sd">        Duplicate modules are returned only once. In the following</span>
<span class="sd">        example, ``l`` will be returned only once.</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; l = nn.Linear(2, 2)</span>
<span class="sd">        &gt;&gt;&gt; net = nn.Sequential(l, l)</span>
<span class="sd">        &gt;&gt;&gt; for idx, m in enumerate(net.modules()):</span>
<span class="sd">                print(idx, &#39;-&gt;&#39;, m)</span>
<span class="sd">        0 -&gt; Sequential(</span>
<span class="sd">          (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">          (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        )</span>
<span class="sd">        1 -&gt; Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
        <span class="k">yield</span> <span class="n">module</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.named_buffers">
    <p>def <span class="ident">named_buffers</span>(</p><p>self, prefix: str = &#39;&#39;, recurse: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over module buffers, yielding both the
name of the buffer as well as the buffer itself.</p>
<p>Args:
    prefix (str): prefix to prepend to all buffer names.
    recurse (bool): if True, then yields buffers of this module
        and all submodules. Otherwise, yields only buffers that
        are direct members of this module.</p>
<p>Yields:
    (string, torch.Tensor): Tuple containing the name and buffer</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for name, buf in self.named_buffers():
&gt;&gt;&gt;    if name in ['running_var']:
&gt;&gt;&gt;        print(buf.size())
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.named_buffers', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.named_buffers" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">named_buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over module buffers, yielding both the</span>
<span class="sd">    name of the buffer as well as the buffer itself.</span>
<span class="sd">    Args:</span>
<span class="sd">        prefix (str): prefix to prepend to all buffer names.</span>
<span class="sd">        recurse (bool): if True, then yields buffers of this module</span>
<span class="sd">            and all submodules. Otherwise, yields only buffers that</span>
<span class="sd">            are direct members of this module.</span>
<span class="sd">    Yields:</span>
<span class="sd">        (string, torch.Tensor): Tuple containing the name and buffer</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for name, buf in self.named_buffers():</span>
<span class="sd">        &gt;&gt;&gt;    if name in [&#39;running_var&#39;]:</span>
<span class="sd">        &gt;&gt;&gt;        print(buf.size())</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">gen</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_named_members</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">module</span><span class="p">:</span> <span class="n">module</span><span class="o">.</span><span class="n">_buffers</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span>
        <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span> <span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">gen</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">elem</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.named_children">
    <p>def <span class="ident">named_children</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over immediate children modules, yielding both
the name of the module as well as the module itself.</p>
<p>Yields:
    (string, Module): Tuple containing a name and child module</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for name, module in model.named_children():
&gt;&gt;&gt;     if name in ['conv4', 'conv5']:
&gt;&gt;&gt;         print(module)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.named_children', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.named_children" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">named_children</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s1">&#39;Module&#39;</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over immediate children modules, yielding both</span>
<span class="sd">    the name of the module as well as the module itself.</span>
<span class="sd">    Yields:</span>
<span class="sd">        (string, Module): Tuple containing a name and child module</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for name, module in model.named_children():</span>
<span class="sd">        &gt;&gt;&gt;     if name in [&#39;conv4&#39;, &#39;conv5&#39;]:</span>
<span class="sd">        &gt;&gt;&gt;         print(module)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">memo</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">module</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">memo</span><span class="p">:</span>
            <span class="n">memo</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
            <span class="k">yield</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.named_modules">
    <p>def <span class="ident">named_modules</span>(</p><p>self, memo: Union[Set[ForwardRef(&#39;Module&#39;)], NoneType] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over all modules in the network, yielding
both the name of the module as well as the module itself.</p>
<p>Args:
    memo: a memo to store the set of modules already added to the result
    prefix: a prefix that will be added to the name of the module
    remove_duplicate: whether to remove the duplicated module instances in the result
    or not</p>
<p>Yields:
    (string, Module): Tuple of name and module</p>
<p>Note:
    Duplicate modules are returned only once. In the following
    example, <code>l</code> will be returned only once.</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; l = nn.Linear(2, 2)
&gt;&gt;&gt; net = nn.Sequential(l, l)
&gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):
        print(idx, '-&gt;', m)

0 -&gt; ('', Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
))
1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.named_modules', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.named_modules" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">named_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memo</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Set</span><span class="p">[</span><span class="s1">&#39;Module&#39;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">remove_duplicate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over all modules in the network, yielding</span>
<span class="sd">    both the name of the module as well as the module itself.</span>
<span class="sd">    Args:</span>
<span class="sd">        memo: a memo to store the set of modules already added to the result</span>
<span class="sd">        prefix: a prefix that will be added to the name of the module</span>
<span class="sd">        remove_duplicate: whether to remove the duplicated module instances in the result</span>
<span class="sd">        or not</span>
<span class="sd">    Yields:</span>
<span class="sd">        (string, Module): Tuple of name and module</span>
<span class="sd">    Note:</span>
<span class="sd">        Duplicate modules are returned only once. In the following</span>
<span class="sd">        example, ``l`` will be returned only once.</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; l = nn.Linear(2, 2)</span>
<span class="sd">        &gt;&gt;&gt; net = nn.Sequential(l, l)</span>
<span class="sd">        &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):</span>
<span class="sd">                print(idx, &#39;-&gt;&#39;, m)</span>
<span class="sd">        0 -&gt; (&#39;&#39;, Sequential(</span>
<span class="sd">          (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">          (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        ))</span>
<span class="sd">        1 -&gt; (&#39;0&#39;, Linear(in_features=2, out_features=2, bias=True))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">memo</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">memo</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">if</span> <span class="bp">self</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">memo</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">remove_duplicate</span><span class="p">:</span>
            <span class="n">memo</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">prefix</span><span class="p">,</span> <span class="bp">self</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">submodule_prefix</span> <span class="o">=</span> <span class="n">prefix</span> <span class="o">+</span> <span class="p">(</span><span class="s1">&#39;.&#39;</span> <span class="k">if</span> <span class="n">prefix</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span><span class="p">)</span> <span class="o">+</span> <span class="n">name</span>
            <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_modules</span><span class="p">(</span><span class="n">memo</span><span class="p">,</span> <span class="n">submodule_prefix</span><span class="p">,</span> <span class="n">remove_duplicate</span><span class="p">):</span>
                <span class="k">yield</span> <span class="n">m</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.named_parameters">
    <p>def <span class="ident">named_parameters</span>(</p><p>self, prefix: str = &#39;&#39;, recurse: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over module parameters, yielding both the
name of the parameter as well as the parameter itself.</p>
<p>Args:
    prefix (str): prefix to prepend to all parameter names.
    recurse (bool): if True, then yields parameters of this module
        and all submodules. Otherwise, yields only parameters that
        are direct members of this module.</p>
<p>Yields:
    (string, Parameter): Tuple containing the name and parameter</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for name, param in self.named_parameters():
&gt;&gt;&gt;    if name in ['bias']:
&gt;&gt;&gt;        print(param.size())
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.named_parameters', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.named_parameters" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">named_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over module parameters, yielding both the</span>
<span class="sd">    name of the parameter as well as the parameter itself.</span>
<span class="sd">    Args:</span>
<span class="sd">        prefix (str): prefix to prepend to all parameter names.</span>
<span class="sd">        recurse (bool): if True, then yields parameters of this module</span>
<span class="sd">            and all submodules. Otherwise, yields only parameters that</span>
<span class="sd">            are direct members of this module.</span>
<span class="sd">    Yields:</span>
<span class="sd">        (string, Parameter): Tuple containing the name and parameter</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for name, param in self.named_parameters():</span>
<span class="sd">        &gt;&gt;&gt;    if name in [&#39;bias&#39;]:</span>
<span class="sd">        &gt;&gt;&gt;        print(param.size())</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">gen</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_named_members</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">module</span><span class="p">:</span> <span class="n">module</span><span class="o">.</span><span class="n">_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span>
        <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span> <span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">gen</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">elem</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.parameters">
    <p>def <span class="ident">parameters</span>(</p><p>self, recurse: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over module parameters.</p>
<p>This is typically passed to an optimizer.</p>
<p>Args:
    recurse (bool): if True, then yields parameters of this module
        and all submodules. Otherwise, yields only parameters that
        are direct members of this module.</p>
<p>Yields:
    Parameter: module parameter</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for param in model.parameters():
&gt;&gt;&gt;     print(type(param), param.size())
&lt;class 'torch.Tensor'&gt; (20L,)
&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.parameters', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.parameters" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Parameter</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over module parameters.</span>
<span class="sd">    This is typically passed to an optimizer.</span>
<span class="sd">    Args:</span>
<span class="sd">        recurse (bool): if True, then yields parameters of this module</span>
<span class="sd">            and all submodules. Otherwise, yields only parameters that</span>
<span class="sd">            are direct members of this module.</span>
<span class="sd">    Yields:</span>
<span class="sd">        Parameter: module parameter</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for param in model.parameters():</span>
<span class="sd">        &gt;&gt;&gt;     print(type(param), param.size())</span>
<span class="sd">        &lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="sd">        &lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">param</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.register_backward_hook">
    <p>def <span class="ident">register_backward_hook</span>(</p><p>self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]])</p>
    </div>
    

    
  
    <div class="desc"><p>Registers a backward hook on the module.</p>
<p>This function is deprecated in favor of :meth:<code>~torch.nn.Module.register_full_backward_hook</code> and
the behavior of this function will change in future versions.</p>
<p>Returns:
    :class:<code>torch.utils.hooks.RemovableHandle</code>:
        a handle that can be used to remove the added hook by calling
        <code>handle.remove()</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.register_backward_hook', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.register_backward_hook" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_backward_hook</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="s1">&#39;Module&#39;</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">],</span> <span class="n">Union</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a backward hook on the module.</span>
<span class="sd">    This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and</span>
<span class="sd">    the behavior of this function will change in future versions.</span>
<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_full_backward_hook</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Cannot use both regular backward hooks and full backward hooks on a &quot;</span>
                           <span class="s2">&quot;single Module. Please use only one of them.&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_is_full_backward_hook</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.register_buffer">
    <p>def <span class="ident">register_buffer</span>(</p><p>self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Adds a buffer to the module.</p>
<p>This is typically used to register a buffer that should not to be
considered a model parameter. For example, BatchNorm's <code>running_mean</code>
is not a parameter, but is part of the module's state. Buffers, by
default, are persistent and will be saved alongside parameters. This
behavior can be changed by setting :attr:<code>persistent</code> to <code>False</code>. The
only difference between a persistent buffer and a non-persistent buffer
is that the latter will not be a part of this module's
:attr:<code>state_dict</code>.</p>
<p>Buffers can be accessed as attributes using given names.</p>
<p>Args:
    name (string): name of the buffer. The buffer can be accessed
        from this module using the given name
    tensor (Tensor or None): buffer to be registered. If <code>None</code>, then operations
        that run on buffers, such as :attr:<code>cuda</code>, are ignored. If <code>None</code>,
        the buffer is <strong>not</strong> included in the module's :attr:<code>state_dict</code>.
    persistent (bool): whether the buffer is part of this module's
        :attr:<code>state_dict</code>.</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.register_buffer', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.register_buffer" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_buffer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">persistent</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Adds a buffer to the module.</span>
<span class="sd">    This is typically used to register a buffer that should not to be</span>
<span class="sd">    considered a model parameter. For example, BatchNorm&#39;s ``running_mean``</span>
<span class="sd">    is not a parameter, but is part of the module&#39;s state. Buffers, by</span>
<span class="sd">    default, are persistent and will be saved alongside parameters. This</span>
<span class="sd">    behavior can be changed by setting :attr:`persistent` to ``False``. The</span>
<span class="sd">    only difference between a persistent buffer and a non-persistent buffer</span>
<span class="sd">    is that the latter will not be a part of this module&#39;s</span>
<span class="sd">    :attr:`state_dict`.</span>
<span class="sd">    Buffers can be accessed as attributes using given names.</span>
<span class="sd">    Args:</span>
<span class="sd">        name (string): name of the buffer. The buffer can be accessed</span>
<span class="sd">            from this module using the given name</span>
<span class="sd">        tensor (Tensor or None): buffer to be registered. If ``None``, then operations</span>
<span class="sd">            that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,</span>
<span class="sd">            the buffer is **not** included in the module&#39;s :attr:`state_dict`.</span>
<span class="sd">        persistent (bool): whether the buffer is part of this module&#39;s</span>
<span class="sd">            :attr:`state_dict`.</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; self.register_buffer(&#39;running_mean&#39;, torch.zeros(num_features))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">persistent</span> <span class="ow">is</span> <span class="kc">False</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ScriptModule</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;ScriptModule does not support non-persistent buffers&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="s1">&#39;_buffers&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
            <span class="s2">&quot;cannot assign buffer before Module.__init__() call&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_six</span><span class="o">.</span><span class="n">string_classes</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;buffer name should be a string. &quot;</span>
                        <span class="s2">&quot;Got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">name</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="s1">&#39;.&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;buffer name can&#39;t contain </span><span class="se">\&quot;</span><span class="s2">.</span><span class="se">\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;buffer name can&#39;t be empty string </span><span class="se">\&quot;\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span> <span class="ow">and</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffers</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;attribute &#39;</span><span class="si">{}</span><span class="s2">&#39; already exists&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;cannot assign &#39;</span><span class="si">{}</span><span class="s2">&#39; object to buffer &#39;</span><span class="si">{}</span><span class="s2">&#39; &quot;</span>
                        <span class="s2">&quot;(torch Tensor or None required)&quot;</span>
                        <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">tensor</span><span class="p">),</span> <span class="n">name</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_buffers</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">tensor</span>
        <span class="k">if</span> <span class="n">persistent</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_non_persistent_buffers_set</span><span class="o">.</span><span class="n">discard</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_non_persistent_buffers_set</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.register_forward_hook">
    <p>def <span class="ident">register_forward_hook</span>(</p><p>self, hook: Callable[..., NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Registers a forward hook on the module.</p>
<p>The hook will be called every time after :func:<code>forward</code> has computed an output.
It should have the following signature::</p>
<pre><code>hook(module, input, output) -&gt; None or modified output
</code></pre>
<p>The input contains only the positional arguments given to the module.
Keyword arguments won't be passed to the hooks and only to the <code>forward</code>.
The hook can modify the output. It can modify the input inplace but
it will not have effect on forward since this is called after
:func:<code>forward</code> is called.</p>
<p>Returns:
    :class:<code>torch.utils.hooks.RemovableHandle</code>:
        a handle that can be used to remove the added hook by calling
        <code>handle.remove()</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.register_forward_hook', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.register_forward_hook" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_forward_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a forward hook on the module.</span>
<span class="sd">    The hook will be called every time after :func:`forward` has computed an output.</span>
<span class="sd">    It should have the following signature::</span>
<span class="sd">        hook(module, input, output) -&gt; None or modified output</span>
<span class="sd">    The input contains only the positional arguments given to the module.</span>
<span class="sd">    Keyword arguments won&#39;t be passed to the hooks and only to the ``forward``.</span>
<span class="sd">    The hook can modify the output. It can modify the input inplace but</span>
<span class="sd">    it will not have effect on forward since this is called after</span>
<span class="sd">    :func:`forward` is called.</span>
<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.register_forward_pre_hook">
    <p>def <span class="ident">register_forward_pre_hook</span>(</p><p>self, hook: Callable[..., NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Registers a forward pre-hook on the module.</p>
<p>The hook will be called every time before :func:<code>forward</code> is invoked.
It should have the following signature::</p>
<pre><code>hook(module, input) -&gt; None or modified input
</code></pre>
<p>The input contains only the positional arguments given to the module.
Keyword arguments won't be passed to the hooks and only to the <code>forward</code>.
The hook can modify the input. User can either return a tuple or a
single modified value in the hook. We will wrap the value into a tuple
if a single value is returned(unless that value is already a tuple).</p>
<p>Returns:
    :class:<code>torch.utils.hooks.RemovableHandle</code>:
        a handle that can be used to remove the added hook by calling
        <code>handle.remove()</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.register_forward_pre_hook', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.register_forward_pre_hook" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_forward_pre_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a forward pre-hook on the module.</span>
<span class="sd">    The hook will be called every time before :func:`forward` is invoked.</span>
<span class="sd">    It should have the following signature::</span>
<span class="sd">        hook(module, input) -&gt; None or modified input</span>
<span class="sd">    The input contains only the positional arguments given to the module.</span>
<span class="sd">    Keyword arguments won&#39;t be passed to the hooks and only to the ``forward``.</span>
<span class="sd">    The hook can modify the input. User can either return a tuple or a</span>
<span class="sd">    single modified value in the hook. We will wrap the value into a tuple</span>
<span class="sd">    if a single value is returned(unless that value is already a tuple).</span>
<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.register_full_backward_hook">
    <p>def <span class="ident">register_full_backward_hook</span>(</p><p>self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]])</p>
    </div>
    

    
  
    <div class="desc"><p>Registers a backward hook on the module.</p>
<p>The hook will be called every time the gradients with respect to module
inputs are computed. The hook should have the following signature::</p>
<pre><code>hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None
</code></pre>
<p>The :attr:<code>grad_input</code> and :attr:<code>grad_output</code> are tuples that contain the gradients
with respect to the inputs and outputs respectively. The hook should
not modify its arguments, but it can optionally return a new gradient with
respect to the input that will be used in place of :attr:<code>grad_input</code> in
subsequent computations. :attr:<code>grad_input</code> will only correspond to the inputs given
as positional arguments and all kwarg arguments are ignored. Entries
in :attr:<code>grad_input</code> and :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor
arguments.</p>
<p>For technical reasons, when this hook is applied to a Module, its forward function will
receive a view of each Tensor passed to the Module. Similarly the caller will receive a view
of each Tensor returned by the Module's forward function.</p>
<p>.. warning ::
    Modifying inputs or outputs inplace is not allowed when using backward hooks and
    will raise an error.</p>
<p>Returns:
    :class:<code>torch.utils.hooks.RemovableHandle</code>:
        a handle that can be used to remove the added hook by calling
        <code>handle.remove()</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.register_full_backward_hook', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.register_full_backward_hook" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_full_backward_hook</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="s1">&#39;Module&#39;</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">],</span> <span class="n">Union</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a backward hook on the module.</span>
<span class="sd">    The hook will be called every time the gradients with respect to module</span>
<span class="sd">    inputs are computed. The hook should have the following signature::</span>
<span class="sd">        hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None</span>
<span class="sd">    The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients</span>
<span class="sd">    with respect to the inputs and outputs respectively. The hook should</span>
<span class="sd">    not modify its arguments, but it can optionally return a new gradient with</span>
<span class="sd">    respect to the input that will be used in place of :attr:`grad_input` in</span>
<span class="sd">    subsequent computations. :attr:`grad_input` will only correspond to the inputs given</span>
<span class="sd">    as positional arguments and all kwarg arguments are ignored. Entries</span>
<span class="sd">    in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor</span>
<span class="sd">    arguments.</span>
<span class="sd">    For technical reasons, when this hook is applied to a Module, its forward function will</span>
<span class="sd">    receive a view of each Tensor passed to the Module. Similarly the caller will receive a view</span>
<span class="sd">    of each Tensor returned by the Module&#39;s forward function.</span>
<span class="sd">    .. warning ::</span>
<span class="sd">        Modifying inputs or outputs inplace is not allowed when using backward hooks and</span>
<span class="sd">        will raise an error.</span>
<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_full_backward_hook</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Cannot use both regular backward hooks and full backward hooks on a &quot;</span>
                           <span class="s2">&quot;single Module. Please use only one of them.&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_is_full_backward_hook</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.register_parameter">
    <p>def <span class="ident">register_parameter</span>(</p><p>self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Adds a parameter to the module.</p>
<p>The parameter can be accessed as an attribute using given name.</p>
<p>Args:
    name (string): name of the parameter. The parameter can be accessed
        from this module using the given name
    param (Parameter or None): parameter to be added to the module. If
        <code>None</code>, then operations that run on parameters, such as :attr:<code>cuda</code>,
        are ignored. If <code>None</code>, the parameter is <strong>not</strong> included in the
        module's :attr:<code>state_dict</code>.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.register_parameter', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.register_parameter" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_parameter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">param</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Parameter</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Adds a parameter to the module.</span>
<span class="sd">    The parameter can be accessed as an attribute using given name.</span>
<span class="sd">    Args:</span>
<span class="sd">        name (string): name of the parameter. The parameter can be accessed</span>
<span class="sd">            from this module using the given name</span>
<span class="sd">        param (Parameter or None): parameter to be added to the module. If</span>
<span class="sd">            ``None``, then operations that run on parameters, such as :attr:`cuda`,</span>
<span class="sd">            are ignored. If ``None``, the parameter is **not** included in the</span>
<span class="sd">            module&#39;s :attr:`state_dict`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="s1">&#39;_parameters&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
            <span class="s2">&quot;cannot assign parameter before Module.__init__() call&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_six</span><span class="o">.</span><span class="n">string_classes</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;parameter name should be a string. &quot;</span>
                        <span class="s2">&quot;Got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">name</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="s1">&#39;.&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;parameter name can&#39;t contain </span><span class="se">\&quot;</span><span class="s2">.</span><span class="se">\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;parameter name can&#39;t be empty string </span><span class="se">\&quot;\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span> <span class="ow">and</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parameters</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;attribute &#39;</span><span class="si">{}</span><span class="s2">&#39; already exists&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">param</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_parameters</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;cannot assign &#39;</span><span class="si">{}</span><span class="s2">&#39; object to parameter &#39;</span><span class="si">{}</span><span class="s2">&#39; &quot;</span>
                        <span class="s2">&quot;(torch.nn.Parameter or None required)&quot;</span>
                        <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">param</span><span class="p">),</span> <span class="n">name</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">param</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Cannot assign non-leaf Tensor to parameter &#39;</span><span class="si">{0}</span><span class="s2">&#39;. Model &quot;</span>
            <span class="s2">&quot;parameters must be created explicitly. To express &#39;</span><span class="si">{0}</span><span class="s2">&#39; &quot;</span>
            <span class="s2">&quot;as a function of another Tensor, compute the value in &quot;</span>
            <span class="s2">&quot;the forward() method.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_parameters</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">param</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.requires_grad_">
    <p>def <span class="ident">requires_grad_</span>(</p><p>self: ~T, requires_grad: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Change if autograd should record operations on parameters in this
module.</p>
<p>This method sets the parameters' :attr:<code>requires_grad</code> attributes
in-place.</p>
<p>This method is helpful for freezing part of the module for finetuning
or training parts of a model individually (e.g., GAN training).</p>
<p>See :ref:<code>locally-disable-grad-doc</code> for a comparison between
<code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.</p>
<p>Args:
    requires_grad (bool): whether autograd should record operations on
                          parameters in this module. Default: <code>True</code>.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.requires_grad_', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.requires_grad_" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Change if autograd should record operations on parameters in this</span>
<span class="sd">    module.</span>
<span class="sd">    This method sets the parameters&#39; :attr:`requires_grad` attributes</span>
<span class="sd">    in-place.</span>
<span class="sd">    This method is helpful for freezing part of the module for finetuning</span>
<span class="sd">    or training parts of a model individually (e.g., GAN training).</span>
<span class="sd">    See :ref:`locally-disable-grad-doc` for a comparison between</span>
<span class="sd">    `.requires_grad_()` and several similar mechanisms that may be confused with it.</span>
<span class="sd">    Args:</span>
<span class="sd">        requires_grad (bool): whether autograd should record operations on</span>
<span class="sd">                              parameters in this module. Default: ``True``.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="n">p</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="n">requires_grad</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.set_extra_state">
    <p>def <span class="ident">set_extra_state</span>(</p><p>self, state: Any)</p>
    </div>
    

    
  
    <div class="desc"><p>This function is called from :func:<code>load_state_dict</code> to handle any extra state
found within the <code>state_dict</code>. Implement this function and a corresponding
:func:<code>get_extra_state</code> for your module if you need to store extra state within its
<code>state_dict</code>.</p>
<p>Args:
    state (dict): Extra state from the <code>state_dict</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.set_extra_state', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.set_extra_state" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">set_extra_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function is called from :func:`load_state_dict` to handle any extra state</span>
<span class="sd">    found within the `state_dict`. Implement this function and a corresponding</span>
<span class="sd">    :func:`get_extra_state` for your module if you need to store extra state within its</span>
<span class="sd">    `state_dict`.</span>
<span class="sd">    Args:</span>
<span class="sd">        state (dict): Extra state from the `state_dict`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
        <span class="s2">&quot;Reached a code path in Module.set_extra_state() that should never be called. &quot;</span>
        <span class="s2">&quot;Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md &quot;</span>
        <span class="s2">&quot;to report this bug.&quot;</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.share_memory">
    <p>def <span class="ident">share_memory</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>See :meth:<code>torch.Tensor.share_memory_</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.share_memory', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.share_memory" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">share_memory</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;See :meth:`torch.Tensor.share_memory_`&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">share_memory_</span><span class="p">())</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.state_dict">
    <p>def <span class="ident">state_dict</span>(</p><p>self, destination=None, prefix=&#39;&#39;, keep_vars=False)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns a dictionary containing a whole state of the module.</p>
<p>Both parameters and persistent buffers (e.g. running averages) are
included. Keys are corresponding parameter and buffer names.
Parameters and buffers set to <code>None</code> are not included.</p>
<p>Returns:
    dict:
        a dictionary containing a whole state of the module</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; module.state_dict().keys()
['bias', 'weight']
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.state_dict', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.state_dict" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">destination</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">keep_vars</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns a dictionary containing a whole state of the module.</span>
<span class="sd">    Both parameters and persistent buffers (e.g. running averages) are</span>
<span class="sd">    included. Keys are corresponding parameter and buffer names.</span>
<span class="sd">    Parameters and buffers set to ``None`` are not included.</span>
<span class="sd">    Returns:</span>
<span class="sd">        dict:</span>
<span class="sd">            a dictionary containing a whole state of the module</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; module.state_dict().keys()</span>
<span class="sd">        [&#39;bias&#39;, &#39;weight&#39;]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">destination</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">destination</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="n">destination</span><span class="o">.</span><span class="n">_metadata</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
    <span class="n">destination</span><span class="o">.</span><span class="n">_metadata</span><span class="p">[</span><span class="n">prefix</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">local_metadata</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">version</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_version</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_save_to_state_dict</span><span class="p">(</span><span class="n">destination</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">keep_vars</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(</span><span class="n">destination</span><span class="p">,</span> <span class="n">prefix</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">keep_vars</span><span class="o">=</span><span class="n">keep_vars</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">hook</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_dict_hooks</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
        <span class="n">hook_result</span> <span class="o">=</span> <span class="n">hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">destination</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">local_metadata</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">hook_result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">destination</span> <span class="o">=</span> <span class="n">hook_result</span>
    <span class="k">return</span> <span class="n">destination</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.to">
    <p>def <span class="ident">to</span>(</p><p>self, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>Moves and/or casts the parameters and buffers.</p>
<p>This can be called as</p>
<p>.. function:: to(device=None, dtype=None, non_blocking=False)
   :noindex:</p>
<p>.. function:: to(dtype, non_blocking=False)
   :noindex:</p>
<p>.. function:: to(tensor, non_blocking=False)
   :noindex:</p>
<p>.. function:: to(memory_format=torch.channels_last)
   :noindex:</p>
<p>Its signature is similar to :meth:<code>torch.Tensor.to</code>, but only accepts
floating point or complex :attr:<code>dtype</code>\ s. In addition, this method will
only cast the floating point or complex parameters and buffers to :attr:<code>dtype</code>
(if given). The integral parameters and buffers will be moved
:attr:<code>device</code>, if that is given, but with dtypes unchanged. When
:attr:<code>non_blocking</code> is set, it tries to convert/move asynchronously
with respect to the host if possible, e.g., moving CPU Tensors with
pinned memory to CUDA devices.</p>
<p>See below for examples.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Args:
    device (:class:<code>torch.device</code>): the desired device of the parameters
        and buffers in this module
    dtype (:class:<code>torch.dtype</code>): the desired floating point or complex dtype of
        the parameters and buffers in this module
    tensor (torch.Tensor): Tensor whose dtype and device are the desired
        dtype and device for all parameters and buffers in this module
    memory_format (:class:<code>torch.memory_format</code>): the desired memory
        format for 4D parameters and buffers in this module (keyword
        only argument)</p>
<p>Returns:
    Module: self</p>
<p>Examples::</p>
<pre><code>&gt;&gt;&gt; linear = nn.Linear(2, 2)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1913, -0.3420],
        [-0.5113, -0.2325]])
&gt;&gt;&gt; linear.to(torch.double)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1913, -0.3420],
        [-0.5113, -0.2325]], dtype=torch.float64)
&gt;&gt;&gt; gpu1 = torch.device("cuda:1")
&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1914, -0.3420],
        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')
&gt;&gt;&gt; cpu = torch.device("cpu")
&gt;&gt;&gt; linear.to(cpu)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1914, -0.3420],
        [-0.5112, -0.2324]], dtype=torch.float16)

&gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.3741+0.j,  0.2382+0.j],
        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)
&gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))
tensor([[0.6122+0.j, 0.1150+0.j],
        [0.6122+0.j, 0.1150+0.j],
        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.to', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.to" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves and/or casts the parameters and buffers.</span>
<span class="sd">    This can be called as</span>
<span class="sd">    .. function:: to(device=None, dtype=None, non_blocking=False)</span>
<span class="sd">       :noindex:</span>
<span class="sd">    .. function:: to(dtype, non_blocking=False)</span>
<span class="sd">       :noindex:</span>
<span class="sd">    .. function:: to(tensor, non_blocking=False)</span>
<span class="sd">       :noindex:</span>
<span class="sd">    .. function:: to(memory_format=torch.channels_last)</span>
<span class="sd">       :noindex:</span>
<span class="sd">    Its signature is similar to :meth:`torch.Tensor.to`, but only accepts</span>
<span class="sd">    floating point or complex :attr:`dtype`\ s. In addition, this method will</span>
<span class="sd">    only cast the floating point or complex parameters and buffers to :attr:`dtype`</span>
<span class="sd">    (if given). The integral parameters and buffers will be moved</span>
<span class="sd">    :attr:`device`, if that is given, but with dtypes unchanged. When</span>
<span class="sd">    :attr:`non_blocking` is set, it tries to convert/move asynchronously</span>
<span class="sd">    with respect to the host if possible, e.g., moving CPU Tensors with</span>
<span class="sd">    pinned memory to CUDA devices.</span>
<span class="sd">    See below for examples.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Args:</span>
<span class="sd">        device (:class:`torch.device`): the desired device of the parameters</span>
<span class="sd">            and buffers in this module</span>
<span class="sd">        dtype (:class:`torch.dtype`): the desired floating point or complex dtype of</span>
<span class="sd">            the parameters and buffers in this module</span>
<span class="sd">        tensor (torch.Tensor): Tensor whose dtype and device are the desired</span>
<span class="sd">            dtype and device for all parameters and buffers in this module</span>
<span class="sd">        memory_format (:class:`torch.memory_format`): the desired memory</span>
<span class="sd">            format for 4D parameters and buffers in this module (keyword</span>
<span class="sd">            only argument)</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    Examples::</span>
<span class="sd">        &gt;&gt;&gt; linear = nn.Linear(2, 2)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.1913, -0.3420],</span>
<span class="sd">                [-0.5113, -0.2325]])</span>
<span class="sd">        &gt;&gt;&gt; linear.to(torch.double)</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.1913, -0.3420],</span>
<span class="sd">                [-0.5113, -0.2325]], dtype=torch.float64)</span>
<span class="sd">        &gt;&gt;&gt; gpu1 = torch.device(&quot;cuda:1&quot;)</span>
<span class="sd">        &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.1914, -0.3420],</span>
<span class="sd">                [-0.5112, -0.2324]], dtype=torch.float16, device=&#39;cuda:1&#39;)</span>
<span class="sd">        &gt;&gt;&gt; cpu = torch.device(&quot;cpu&quot;)</span>
<span class="sd">        &gt;&gt;&gt; linear.to(cpu)</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.1914, -0.3420],</span>
<span class="sd">                [-0.5112, -0.2324]], dtype=torch.float16)</span>
<span class="sd">        &gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.3741+0.j,  0.2382+0.j],</span>
<span class="sd">                [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)</span>
<span class="sd">        &gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))</span>
<span class="sd">        tensor([[0.6122+0.j, 0.1150+0.j],</span>
<span class="sd">                [0.6122+0.j, 0.1150+0.j],</span>
<span class="sd">                [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">,</span> <span class="n">convert_to_format</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">_parse_to</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_floating_point</span> <span class="ow">or</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;nn.Module.to only accepts floating point or complex &#39;</span>
                            <span class="s1">&#39;dtypes, but got desired dtype=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dtype</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Complex modules are a new feature under active development whose design may change, &quot;</span>
                <span class="s2">&quot;and some modules might not work as expected when using complex tensors as parameters or buffers. &quot;</span>
                <span class="s2">&quot;Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md &quot;</span>
                <span class="s2">&quot;if a complex module does not work as expected.&quot;</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">convert</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">convert_to_format</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">t</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">t</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="ow">or</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
                        <span class="n">non_blocking</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">convert_to_format</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">t</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="ow">or</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="n">convert</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.to_empty">
    <p>def <span class="ident">to_empty</span>(</p><p>self: ~T, *, device: Union[str, torch.device])</p>
    </div>
    

    
  
    <div class="desc"><p>Moves the parameters and buffers to the specified device without copying storage.</p>
<p>Args:
    device (:class:<code>torch.device</code>): The desired device of the parameters
        and buffers in this module.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.to_empty', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.to_empty" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">to_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves the parameters and buffers to the specified device without copying storage.</span>
<span class="sd">    Args:</span>
<span class="sd">        device (:class:`torch.device`): The desired device of the parameters</span>
<span class="sd">            and buffers in this module.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">))</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.train">
    <p>def <span class="ident">train</span>(</p><p>self: ~T, mode: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Sets the module in training mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>,
etc.</p>
<p>Args:
    mode (bool): whether to set training mode (<code>True</code>) or evaluation
                 mode (<code>False</code>). Default: <code>True</code>.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.train', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.train" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets the module in training mode.</span>
<span class="sd">    This has any effect only on certain modules. See documentations of</span>
<span class="sd">    particular modules for details of their behaviors in training/evaluation</span>
<span class="sd">    mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,</span>
<span class="sd">    etc.</span>
<span class="sd">    Args:</span>
<span class="sd">        mode (bool): whether to set training mode (``True``) or evaluation</span>
<span class="sd">                     mode (``False``). Default: ``True``.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;training mode is expected to be boolean&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="o">=</span> <span class="n">mode</span>
    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">():</span>
        <span class="n">module</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.type">
    <p>def <span class="ident">type</span>(</p><p>self: ~T, dst_type: Union[torch.dtype, str])</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Args:
    dst_type (type or string): the desired type</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.type', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.type" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">type</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">dst_type</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all parameters and buffers to :attr:`dst_type`.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Args:</span>
<span class="sd">        dst_type (type or string): the desired type</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">dst_type</span><span class="p">))</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.xpu">
    <p>def <span class="ident">xpu</span>(</p><p>self: ~T, device: Union[int, torch.device, NoneType] = None)</p>
    </div>
    

    
  
    <div class="desc"><p>Moves all model parameters and buffers to the XPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on XPU while being optimized.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Arguments:
    device (int, optional): if specified, all parameters will be
        copied to that device</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.xpu', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.xpu" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">xpu</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves all model parameters and buffers to the XPU.</span>
<span class="sd">    This also makes associated parameters and buffers different objects. So</span>
<span class="sd">    it should be called before constructing optimizer if the module will</span>
<span class="sd">    live on XPU while being optimized.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Arguments:</span>
<span class="sd">        device (int, optional): if specified, all parameters will be</span>
<span class="sd">            copied to that device</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">xpu</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.DecoderLayer.zero_grad">
    <p>def <span class="ident">zero_grad</span>(</p><p>self, set_to_none: bool = False)</p>
    </div>
    

    
  
    <div class="desc"><p>Sets gradients of all model parameters to zero. See similar function
under :class:<code>torch.optim.Optimizer</code> for more context.</p>
<p>Args:
    set_to_none (bool): instead of setting to zero, set the grads to None.
        See :meth:<code>torch.optim.Optimizer.zero_grad</code> for details.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.DecoderLayer.zero_grad', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.DecoderLayer.zero_grad" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">set_to_none</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets gradients of all model parameters to zero. See similar function</span>
<span class="sd">    under :class:`torch.optim.Optimizer` for more context.</span>
<span class="sd">    Args:</span>
<span class="sd">        set_to_none (bool): instead of setting to zero, set the grads to None.</span>
<span class="sd">            See :meth:`torch.optim.Optimizer.zero_grad` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_is_replica&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;Calling .zero_grad() from a module created with nn.DataParallel() has no effect. &quot;</span>
            <span class="s2">&quot;The parameters are copied (in a differentiable manner) from the original module. &quot;</span>
            <span class="s2">&quot;This means they are not leaf nodes in autograd and so don&#39;t accumulate gradients. &quot;</span>
            <span class="s2">&quot;If you need gradients in your forward method, consider using autograd.grad instead.&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">set_to_none</span><span class="p">:</span>
                <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">grad_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">detach_</span><span class="p">()</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
                <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</pre></div>

  </div>
</div>

  </div>
  
      </div>
      </div>
      
      <div class="item">
      <p id="seq2seq.Encoder" class="name">class <span class="ident">Encoder</span></p>
      
  
    <div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p></div>
  <div class="source_cont">
</div>


      <div class="class">
          <h3>Ancestors (in MRO)</h3>
          <ul class="class_list">
          <li><a href="/torch.nn.modules.module.Module.ext">torch.nn.modules.module.Module</a></li>
          </ul>
          <h3>Class variables</h3>
            <div class="item">
            <p id="seq2seq.Encoder.T_destination" class="name">var <span class="ident">T_destination</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="seq2seq.Encoder.dump_patches" class="name">var <span class="ident">dump_patches</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
          <h3>Methods</h3>
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.add_module">
    <p>def <span class="ident">add_module</span>(</p><p>self, name: str, module: Union[ForwardRef(&#39;Module&#39;), NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Adds a child module to the current module.</p>
<p>The module can be accessed as an attribute using the given name.</p>
<p>Args:
    name (string): name of the child module. The child module can be
        accessed from this module using the given name
    module (Module): child module to be added to the module.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.add_module', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.add_module" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">add_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s1">&#39;Module&#39;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Adds a child module to the current module.</span>
<span class="sd">    The module can be accessed as an attribute using the given name.</span>
<span class="sd">    Args:</span>
<span class="sd">        name (string): name of the child module. The child module can be</span>
<span class="sd">            accessed from this module using the given name</span>
<span class="sd">        module (Module): child module to be added to the module.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">Module</span><span class="p">)</span> <span class="ow">and</span> <span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> is not a Module subclass&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">module</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_six</span><span class="o">.</span><span class="n">string_classes</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;module name should be a string. Got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">name</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span> <span class="ow">and</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;attribute &#39;</span><span class="si">{}</span><span class="s2">&#39; already exists&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">elif</span> <span class="s1">&#39;.&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;module name can&#39;t contain </span><span class="se">\&quot;</span><span class="s2">.</span><span class="se">\&quot;</span><span class="s2">, got: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;module name can&#39;t be empty string </span><span class="se">\&quot;\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.apply">
    <p>def <span class="ident">apply</span>(</p><p>self: ~T, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>)
as well as self. Typical use includes initializing the parameters of a model
(see also :ref:<code>nn-init-doc</code>).</p>
<p>Args:
    fn (:class:<code>Module</code> -&gt; None): function to be applied to each submodule</p>
<p>Returns:
    Module: self</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; @torch.no_grad()
&gt;&gt;&gt; def init_weights(m):
&gt;&gt;&gt;     print(m)
&gt;&gt;&gt;     if type(m) == nn.Linear:
&gt;&gt;&gt;         m.weight.fill_(1.0)
&gt;&gt;&gt;         print(m.weight)
&gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))
&gt;&gt;&gt; net.apply(init_weights)
Linear(in_features=2, out_features=2, bias=True)
Parameter containing:
tensor([[ 1.,  1.],
        [ 1.,  1.]])
Linear(in_features=2, out_features=2, bias=True)
Parameter containing:
tensor([[ 1.,  1.],
        [ 1.,  1.]])
Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.apply', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.apply" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="s1">&#39;Module&#39;</span><span class="p">],</span> <span class="kc">None</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies ``fn`` recursively to every submodule (as returned by ``.children()``)</span>
<span class="sd">    as well as self. Typical use includes initializing the parameters of a model</span>
<span class="sd">    (see also :ref:`nn-init-doc`).</span>
<span class="sd">    Args:</span>
<span class="sd">        fn (:class:`Module` -&gt; None): function to be applied to each submodule</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; @torch.no_grad()</span>
<span class="sd">        &gt;&gt;&gt; def init_weights(m):</span>
<span class="sd">        &gt;&gt;&gt;     print(m)</span>
<span class="sd">        &gt;&gt;&gt;     if type(m) == nn.Linear:</span>
<span class="sd">        &gt;&gt;&gt;         m.weight.fill_(1.0)</span>
<span class="sd">        &gt;&gt;&gt;         print(m.weight)</span>
<span class="sd">        &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))</span>
<span class="sd">        &gt;&gt;&gt; net.apply(init_weights)</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 1.,  1.],</span>
<span class="sd">                [ 1.,  1.]])</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 1.,  1.],</span>
<span class="sd">                [ 1.,  1.]])</span>
<span class="sd">        Sequential(</span>
<span class="sd">          (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">          (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        )</span>
<span class="sd">        Sequential(</span>
<span class="sd">          (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">          (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        )</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">():</span>
        <span class="n">module</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
    <span class="n">fn</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.bfloat16">
    <p>def <span class="ident">bfloat16</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.bfloat16', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.bfloat16" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">bfloat16</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all floating point parameters and buffers to ``bfloat16`` datatype.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">()</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="k">else</span> <span class="n">t</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.buffers">
    <p>def <span class="ident">buffers</span>(</p><p>self, recurse: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over module buffers.</p>
<p>Args:
    recurse (bool): if True, then yields buffers of this module
        and all submodules. Otherwise, yields only buffers that
        are direct members of this module.</p>
<p>Yields:
    torch.Tensor: module buffer</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for buf in model.buffers():
&gt;&gt;&gt;     print(type(buf), buf.size())
&lt;class 'torch.Tensor'&gt; (20L,)
&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.buffers', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.buffers" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over module buffers.</span>
<span class="sd">    Args:</span>
<span class="sd">        recurse (bool): if True, then yields buffers of this module</span>
<span class="sd">            and all submodules. Otherwise, yields only buffers that</span>
<span class="sd">            are direct members of this module.</span>
<span class="sd">    Yields:</span>
<span class="sd">        torch.Tensor: module buffer</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for buf in model.buffers():</span>
<span class="sd">        &gt;&gt;&gt;     print(type(buf), buf.size())</span>
<span class="sd">        &lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="sd">        &lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">buf</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">buf</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.children">
    <p>def <span class="ident">children</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over immediate children modules.</p>
<p>Yields:
    Module: a child module</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.children', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.children" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">children</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="s1">&#39;Module&#39;</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over immediate children modules.</span>
<span class="sd">    Yields:</span>
<span class="sd">        Module: a child module</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
        <span class="k">yield</span> <span class="n">module</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.cpu">
    <p>def <span class="ident">cpu</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Moves all model parameters and buffers to the CPU.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.cpu', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.cpu" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves all model parameters and buffers to the CPU.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.cuda">
    <p>def <span class="ident">cuda</span>(</p><p>self: ~T, device: Union[int, torch.device, NoneType] = None)</p>
    </div>
    

    
  
    <div class="desc"><p>Moves all model parameters and buffers to the GPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on GPU while being optimized.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Args:
    device (int, optional): if specified, all parameters will be
        copied to that device</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.cuda', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.cuda" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">cuda</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves all model parameters and buffers to the GPU.</span>
<span class="sd">    This also makes associated parameters and buffers different objects. So</span>
<span class="sd">    it should be called before constructing optimizer if the module will</span>
<span class="sd">    live on GPU while being optimized.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Args:</span>
<span class="sd">        device (int, optional): if specified, all parameters will be</span>
<span class="sd">            copied to that device</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.double">
    <p>def <span class="ident">double</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.double', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.double" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">double</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all floating point parameters and buffers to ``double`` datatype.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">double</span><span class="p">()</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="k">else</span> <span class="n">t</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.eval">
    <p>def <span class="ident">eval</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Sets the module in evaluation mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>,
etc.</p>
<p>This is equivalent with :meth:<code>self.train(False) &lt;torch.nn.Module.train&gt;</code>.</p>
<p>See :ref:<code>locally-disable-grad-doc</code> for a comparison between
<code>.eval()</code> and several similar mechanisms that may be confused with it.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.eval', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.eval" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets the module in evaluation mode.</span>
<span class="sd">    This has any effect only on certain modules. See documentations of</span>
<span class="sd">    particular modules for details of their behaviors in training/evaluation</span>
<span class="sd">    mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,</span>
<span class="sd">    etc.</span>
<span class="sd">    This is equivalent with :meth:`self.train(False) &lt;torch.nn.Module.train&gt;`.</span>
<span class="sd">    See :ref:`locally-disable-grad-doc` for a comparison between</span>
<span class="sd">    `.eval()` and several similar mechanisms that may be confused with it.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.extra_repr">
    <p>def <span class="ident">extra_repr</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.extra_repr', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.extra_repr" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Set the extra representation of the module</span>
<span class="sd">    To print customized extra information, you should re-implement</span>
<span class="sd">    this method in your own modules. Both single-line and multi-line</span>
<span class="sd">    strings are acceptable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="s1">&#39;&#39;</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.float">
    <p>def <span class="ident">float</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.float', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.float" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">float</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all floating point parameters and buffers to ``float`` datatype.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="k">else</span> <span class="n">t</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.forward">
    <p>def <span class="ident">forward</span>(</p><p>self, src, src_mask)</p>
    </div>
    

    
  
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.forward', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.forward" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">):</span>
    
    <span class="c1">#src = [batch size, src len]</span>
    <span class="c1">#src_mask = [batch size, 1, 1, src len]</span>
    
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">src</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">src_len</span> <span class="o">=</span> <span class="n">src</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="n">pos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    
    <span class="c1">#pos = [batch size, src len]</span>
    
    <span class="n">src</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">tok_embedding</span><span class="p">(</span><span class="n">src</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_embedding</span><span class="p">(</span><span class="n">pos</span><span class="p">))</span>
    
    <span class="c1">#src = [batch size, src len, hid dim]</span>
    
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
        <span class="n">src</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
        
    <span class="c1">#src = [batch size, src len, hid dim]</span>
        
    <span class="k">return</span> <span class="n">src</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.get_buffer">
    <p>def <span class="ident">get_buffer</span>(</p><p>self, target: str)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the buffer given by <code>target</code> if it exists,
otherwise throws an error.</p>
<p>See the docstring for <code>get_submodule</code> for a more detailed
explanation of this method's functionality as well as how to
correctly specify <code>target</code>.</p>
<p>Args:
    target: The fully-qualified string name of the buffer
        to look for. (See <code>get_submodule</code> for how to specify a
        fully-qualified string.)</p>
<p>Returns:
    torch.Tensor: The buffer referenced by <code>target</code></p>
<p>Raises:
    AttributeError: If the target string references an invalid
        path or resolves to something that is not a
        buffer</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.get_buffer', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.get_buffer" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_buffer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the buffer given by ``target`` if it exists,</span>
<span class="sd">    otherwise throws an error.</span>
<span class="sd">    See the docstring for ``get_submodule`` for a more detailed</span>
<span class="sd">    explanation of this method&#39;s functionality as well as how to</span>
<span class="sd">    correctly specify ``target``.</span>
<span class="sd">    Args:</span>
<span class="sd">        target: The fully-qualified string name of the buffer</span>
<span class="sd">            to look for. (See ``get_submodule`` for how to specify a</span>
<span class="sd">            fully-qualified string.)</span>
<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: The buffer referenced by ``target``</span>
<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: If the target string references an invalid</span>
<span class="sd">            path or resolves to something that is not a</span>
<span class="sd">            buffer</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">module_path</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">buffer_name</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">rpartition</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
    <span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_submodule</span><span class="p">(</span><span class="n">module_path</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">buffer_name</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">_get_name</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot; has no attribute `&quot;</span>
                             <span class="o">+</span> <span class="n">buffer_name</span> <span class="o">+</span> <span class="s2">&quot;`&quot;</span><span class="p">)</span>
    <span class="n">buffer</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">buffer_name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">buffer_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">mod</span><span class="o">.</span><span class="n">_buffers</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s2">&quot;`&quot;</span> <span class="o">+</span> <span class="n">buffer_name</span> <span class="o">+</span> <span class="s2">&quot;` is not a buffer&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">buffer</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.get_extra_state">
    <p>def <span class="ident">get_extra_state</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns any extra state to include in the module's state_dict.
Implement this and a corresponding :func:<code>set_extra_state</code> for your module
if you need to store extra state. This function is called when building the
module's <code>state_dict()</code>.</p>
<p>Note that extra state should be pickleable to ensure working serialization
of the state_dict. We only provide provide backwards compatibility guarantees
for serializing Tensors; other objects may break backwards compatibility if
their serialized pickled form changes.</p>
<p>Returns:
    object: Any extra state to store in the module's state_dict</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.get_extra_state', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.get_extra_state" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_extra_state</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns any extra state to include in the module&#39;s state_dict.</span>
<span class="sd">    Implement this and a corresponding :func:`set_extra_state` for your module</span>
<span class="sd">    if you need to store extra state. This function is called when building the</span>
<span class="sd">    module&#39;s `state_dict()`.</span>
<span class="sd">    Note that extra state should be pickleable to ensure working serialization</span>
<span class="sd">    of the state_dict. We only provide provide backwards compatibility guarantees</span>
<span class="sd">    for serializing Tensors; other objects may break backwards compatibility if</span>
<span class="sd">    their serialized pickled form changes.</span>
<span class="sd">    Returns:</span>
<span class="sd">        object: Any extra state to store in the module&#39;s state_dict</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
        <span class="s2">&quot;Reached a code path in Module.get_extra_state() that should never be called. &quot;</span>
        <span class="s2">&quot;Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md &quot;</span>
        <span class="s2">&quot;to report this bug.&quot;</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.get_parameter">
    <p>def <span class="ident">get_parameter</span>(</p><p>self, target: str)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the parameter given by <code>target</code> if it exists,
otherwise throws an error.</p>
<p>See the docstring for <code>get_submodule</code> for a more detailed
explanation of this method's functionality as well as how to
correctly specify <code>target</code>.</p>
<p>Args:
    target: The fully-qualified string name of the Parameter
        to look for. (See <code>get_submodule</code> for how to specify a
        fully-qualified string.)</p>
<p>Returns:
    torch.nn.Parameter: The Parameter referenced by <code>target</code></p>
<p>Raises:
    AttributeError: If the target string references an invalid
        path or resolves to something that is not an
        <code>nn.Parameter</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.get_parameter', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.get_parameter" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_parameter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Parameter&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the parameter given by ``target`` if it exists,</span>
<span class="sd">    otherwise throws an error.</span>
<span class="sd">    See the docstring for ``get_submodule`` for a more detailed</span>
<span class="sd">    explanation of this method&#39;s functionality as well as how to</span>
<span class="sd">    correctly specify ``target``.</span>
<span class="sd">    Args:</span>
<span class="sd">        target: The fully-qualified string name of the Parameter</span>
<span class="sd">            to look for. (See ``get_submodule`` for how to specify a</span>
<span class="sd">            fully-qualified string.)</span>
<span class="sd">    Returns:</span>
<span class="sd">        torch.nn.Parameter: The Parameter referenced by ``target``</span>
<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: If the target string references an invalid</span>
<span class="sd">            path or resolves to something that is not an</span>
<span class="sd">            ``nn.Parameter``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">module_path</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">param_name</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">rpartition</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
    <span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_submodule</span><span class="p">(</span><span class="n">module_path</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">param_name</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">_get_name</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot; has no attribute `&quot;</span>
                             <span class="o">+</span> <span class="n">param_name</span> <span class="o">+</span> <span class="s2">&quot;`&quot;</span><span class="p">)</span>
    <span class="n">param</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">param_name</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s2">&quot;`&quot;</span> <span class="o">+</span> <span class="n">param_name</span> <span class="o">+</span> <span class="s2">&quot;` is not an &quot;</span>
                             <span class="s2">&quot;nn.Parameter&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">param</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.get_submodule">
    <p>def <span class="ident">get_submodule</span>(</p><p>self, target: str)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the submodule given by <code>target</code> if it exists,
otherwise throws an error.</p>
<p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that
looks like this:</p>
<p>.. code-block::text</p>
<pre><code>A(
    (net_b): Module(
        (net_c): Module(
            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))
        )
        (linear): Linear(in_features=100, out_features=200, bias=True)
    )
)
</code></pre>
<p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested
submodule <code>net_b</code>, which itself has two submodules <code>net_c</code>
and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p>
<p>To check whether or not we have the <code>linear</code> submodule, we
would call <code>get_submodule("net_b.linear")</code>. To check whether
we have the <code>conv</code> submodule, we would call
<code>get_submodule("net_b.net_c.conv")</code>.</p>
<p>The runtime of <code>get_submodule</code> is bounded by the degree
of module nesting in <code>target</code>. A query against
<code>named_modules</code> achieves the same result, but it is O(N) in
the number of transitive modules. So, for a simple check to see
if some submodule exists, <code>get_submodule</code> should always be
used.</p>
<p>Args:
    target: The fully-qualified string name of the submodule
        to look for. (See above example for how to specify a
        fully-qualified string.)</p>
<p>Returns:
    torch.nn.Module: The submodule referenced by <code>target</code></p>
<p>Raises:
    AttributeError: If the target string references an invalid
        path or resolves to something that is not an
        <code>nn.Module</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.get_submodule', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.get_submodule" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_submodule</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Module&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the submodule given by ``target`` if it exists,</span>
<span class="sd">    otherwise throws an error.</span>
<span class="sd">    For example, let&#39;s say you have an ``nn.Module`` ``A`` that</span>
<span class="sd">    looks like this:</span>
<span class="sd">    .. code-block::text</span>
<span class="sd">        A(</span>
<span class="sd">            (net_b): Module(</span>
<span class="sd">                (net_c): Module(</span>
<span class="sd">                    (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))</span>
<span class="sd">                )</span>
<span class="sd">                (linear): Linear(in_features=100, out_features=200, bias=True)</span>
<span class="sd">            )</span>
<span class="sd">        )</span>
<span class="sd">    (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested</span>
<span class="sd">    submodule ``net_b``, which itself has two submodules ``net_c``</span>
<span class="sd">    and ``linear``. ``net_c`` then has a submodule ``conv``.)</span>
<span class="sd">    To check whether or not we have the ``linear`` submodule, we</span>
<span class="sd">    would call ``get_submodule(&quot;net_b.linear&quot;)``. To check whether</span>
<span class="sd">    we have the ``conv`` submodule, we would call</span>
<span class="sd">    ``get_submodule(&quot;net_b.net_c.conv&quot;)``.</span>
<span class="sd">    The runtime of ``get_submodule`` is bounded by the degree</span>
<span class="sd">    of module nesting in ``target``. A query against</span>
<span class="sd">    ``named_modules`` achieves the same result, but it is O(N) in</span>
<span class="sd">    the number of transitive modules. So, for a simple check to see</span>
<span class="sd">    if some submodule exists, ``get_submodule`` should always be</span>
<span class="sd">    used.</span>
<span class="sd">    Args:</span>
<span class="sd">        target: The fully-qualified string name of the submodule</span>
<span class="sd">            to look for. (See above example for how to specify a</span>
<span class="sd">            fully-qualified string.)</span>
<span class="sd">    Returns:</span>
<span class="sd">        torch.nn.Module: The submodule referenced by ``target``</span>
<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: If the target string references an invalid</span>
<span class="sd">            path or resolves to something that is not an</span>
<span class="sd">            ``nn.Module``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">target</span> <span class="o">==</span> <span class="s2">&quot;&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span>
    <span class="n">atoms</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
    <span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="bp">self</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">atoms</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">_get_name</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot; has no &quot;</span>
                                 <span class="s2">&quot;attribute `&quot;</span> <span class="o">+</span> <span class="n">item</span> <span class="o">+</span> <span class="s2">&quot;`&quot;</span><span class="p">)</span>
        <span class="n">mod</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">item</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s2">&quot;`&quot;</span> <span class="o">+</span> <span class="n">item</span> <span class="o">+</span> <span class="s2">&quot;` is not &quot;</span>
                                 <span class="s2">&quot;an nn.Module&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mod</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.half">
    <p>def <span class="ident">half</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.half', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.half" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">half</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all floating point parameters and buffers to ``half`` datatype.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">half</span><span class="p">()</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="k">else</span> <span class="n">t</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.load_state_dict">
    <p>def <span class="ident">load_state_dict</span>(</p><p>self, state_dict: &#39;OrderedDict[str, Tensor]&#39;, strict: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Copies parameters and buffers from :attr:<code>state_dict</code> into
this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then
the keys of :attr:<code>state_dict</code> must exactly match the keys returned
by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.</p>
<p>Args:
    state_dict (dict): a dict containing parameters and
        persistent buffers.
    strict (bool, optional): whether to strictly enforce that the keys
        in :attr:<code>state_dict</code> match the keys returned by this module's
        :meth:<code>~torch.nn.Module.state_dict</code> function. Default: <code>True</code></p>
<p>Returns:
    <code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:
        * <strong>missing_keys</strong> is a list of str containing the missing keys
        * <strong>unexpected_keys</strong> is a list of str containing the unexpected keys</p>
<p>Note:
    If a parameter or buffer is registered as <code>None</code> and its corresponding key
    exists in :attr:<code>state_dict</code>, :meth:<code>load_state_dict</code> will raise a
    <code>RuntimeError</code>.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.load_state_dict', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.load_state_dict" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="s1">&#39;OrderedDict[str, Tensor]&#39;</span><span class="p">,</span>
                    <span class="n">strict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Copies parameters and buffers from :attr:`state_dict` into</span>
<span class="sd">    this module and its descendants. If :attr:`strict` is ``True``, then</span>
<span class="sd">    the keys of :attr:`state_dict` must exactly match the keys returned</span>
<span class="sd">    by this module&#39;s :meth:`~torch.nn.Module.state_dict` function.</span>
<span class="sd">    Args:</span>
<span class="sd">        state_dict (dict): a dict containing parameters and</span>
<span class="sd">            persistent buffers.</span>
<span class="sd">        strict (bool, optional): whether to strictly enforce that the keys</span>
<span class="sd">            in :attr:`state_dict` match the keys returned by this module&#39;s</span>
<span class="sd">            :meth:`~torch.nn.Module.state_dict` function. Default: ``True``</span>
<span class="sd">    Returns:</span>
<span class="sd">        ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:</span>
<span class="sd">            * **missing_keys** is a list of str containing the missing keys</span>
<span class="sd">            * **unexpected_keys** is a list of str containing the unexpected keys</span>
<span class="sd">    Note:</span>
<span class="sd">        If a parameter or buffer is registered as ``None`` and its corresponding key</span>
<span class="sd">        exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a</span>
<span class="sd">        ``RuntimeError``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">missing_keys</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">unexpected_keys</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">error_msgs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># copy state_dict so _load_from_state_dict can modify it</span>
    <span class="n">metadata</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="s1">&#39;_metadata&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># mypy isn&#39;t aware that &quot;_metadata&quot; exists in state_dict</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">_metadata</span> <span class="o">=</span> <span class="n">metadata</span>  <span class="c1"># type: ignore[attr-defined]</span>
    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
        <span class="n">local_metadata</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="n">metadata</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">metadata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">prefix</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">{})</span>
        <span class="n">module</span><span class="o">.</span><span class="n">_load_from_state_dict</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">local_metadata</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">missing_keys</span><span class="p">,</span> <span class="n">unexpected_keys</span><span class="p">,</span> <span class="n">error_msgs</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">child</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">load</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">prefix</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>
    <span class="n">load</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">load</span>
    <span class="k">if</span> <span class="n">strict</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">unexpected_keys</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">error_msgs</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span>
                <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Unexpected key(s) in state_dict: </span><span class="si">{}</span><span class="s1">. &#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;&quot;</span><span class="si">{}</span><span class="s1">&quot;&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">unexpected_keys</span><span class="p">)))</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">missing_keys</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">error_msgs</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span>
                <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Missing key(s) in state_dict: </span><span class="si">{}</span><span class="s1">. &#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;&quot;</span><span class="si">{}</span><span class="s1">&quot;&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">missing_keys</span><span class="p">)))</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">error_msgs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Error(s) in loading state_dict for </span><span class="si">{}</span><span class="s1">:</span><span class="se">\n\t</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                           <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n\t</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">error_msgs</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">_IncompatibleKeys</span><span class="p">(</span><span class="n">missing_keys</span><span class="p">,</span> <span class="n">unexpected_keys</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.modules">
    <p>def <span class="ident">modules</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over all modules in the network.</p>
<p>Yields:
    Module: a module in the network</p>
<p>Note:
    Duplicate modules are returned only once. In the following
    example, <code>l</code> will be returned only once.</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; l = nn.Linear(2, 2)
&gt;&gt;&gt; net = nn.Sequential(l, l)
&gt;&gt;&gt; for idx, m in enumerate(net.modules()):
        print(idx, '-&gt;', m)

0 -&gt; Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
1 -&gt; Linear(in_features=2, out_features=2, bias=True)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.modules', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.modules" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">modules</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="s1">&#39;Module&#39;</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over all modules in the network.</span>
<span class="sd">    Yields:</span>
<span class="sd">        Module: a module in the network</span>
<span class="sd">    Note:</span>
<span class="sd">        Duplicate modules are returned only once. In the following</span>
<span class="sd">        example, ``l`` will be returned only once.</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; l = nn.Linear(2, 2)</span>
<span class="sd">        &gt;&gt;&gt; net = nn.Sequential(l, l)</span>
<span class="sd">        &gt;&gt;&gt; for idx, m in enumerate(net.modules()):</span>
<span class="sd">                print(idx, &#39;-&gt;&#39;, m)</span>
<span class="sd">        0 -&gt; Sequential(</span>
<span class="sd">          (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">          (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        )</span>
<span class="sd">        1 -&gt; Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
        <span class="k">yield</span> <span class="n">module</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.named_buffers">
    <p>def <span class="ident">named_buffers</span>(</p><p>self, prefix: str = &#39;&#39;, recurse: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over module buffers, yielding both the
name of the buffer as well as the buffer itself.</p>
<p>Args:
    prefix (str): prefix to prepend to all buffer names.
    recurse (bool): if True, then yields buffers of this module
        and all submodules. Otherwise, yields only buffers that
        are direct members of this module.</p>
<p>Yields:
    (string, torch.Tensor): Tuple containing the name and buffer</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for name, buf in self.named_buffers():
&gt;&gt;&gt;    if name in ['running_var']:
&gt;&gt;&gt;        print(buf.size())
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.named_buffers', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.named_buffers" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">named_buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over module buffers, yielding both the</span>
<span class="sd">    name of the buffer as well as the buffer itself.</span>
<span class="sd">    Args:</span>
<span class="sd">        prefix (str): prefix to prepend to all buffer names.</span>
<span class="sd">        recurse (bool): if True, then yields buffers of this module</span>
<span class="sd">            and all submodules. Otherwise, yields only buffers that</span>
<span class="sd">            are direct members of this module.</span>
<span class="sd">    Yields:</span>
<span class="sd">        (string, torch.Tensor): Tuple containing the name and buffer</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for name, buf in self.named_buffers():</span>
<span class="sd">        &gt;&gt;&gt;    if name in [&#39;running_var&#39;]:</span>
<span class="sd">        &gt;&gt;&gt;        print(buf.size())</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">gen</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_named_members</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">module</span><span class="p">:</span> <span class="n">module</span><span class="o">.</span><span class="n">_buffers</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span>
        <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span> <span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">gen</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">elem</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.named_children">
    <p>def <span class="ident">named_children</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over immediate children modules, yielding both
the name of the module as well as the module itself.</p>
<p>Yields:
    (string, Module): Tuple containing a name and child module</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for name, module in model.named_children():
&gt;&gt;&gt;     if name in ['conv4', 'conv5']:
&gt;&gt;&gt;         print(module)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.named_children', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.named_children" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">named_children</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s1">&#39;Module&#39;</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over immediate children modules, yielding both</span>
<span class="sd">    the name of the module as well as the module itself.</span>
<span class="sd">    Yields:</span>
<span class="sd">        (string, Module): Tuple containing a name and child module</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for name, module in model.named_children():</span>
<span class="sd">        &gt;&gt;&gt;     if name in [&#39;conv4&#39;, &#39;conv5&#39;]:</span>
<span class="sd">        &gt;&gt;&gt;         print(module)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">memo</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">module</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">memo</span><span class="p">:</span>
            <span class="n">memo</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
            <span class="k">yield</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.named_modules">
    <p>def <span class="ident">named_modules</span>(</p><p>self, memo: Union[Set[ForwardRef(&#39;Module&#39;)], NoneType] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over all modules in the network, yielding
both the name of the module as well as the module itself.</p>
<p>Args:
    memo: a memo to store the set of modules already added to the result
    prefix: a prefix that will be added to the name of the module
    remove_duplicate: whether to remove the duplicated module instances in the result
    or not</p>
<p>Yields:
    (string, Module): Tuple of name and module</p>
<p>Note:
    Duplicate modules are returned only once. In the following
    example, <code>l</code> will be returned only once.</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; l = nn.Linear(2, 2)
&gt;&gt;&gt; net = nn.Sequential(l, l)
&gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):
        print(idx, '-&gt;', m)

0 -&gt; ('', Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
))
1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.named_modules', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.named_modules" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">named_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memo</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Set</span><span class="p">[</span><span class="s1">&#39;Module&#39;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">remove_duplicate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over all modules in the network, yielding</span>
<span class="sd">    both the name of the module as well as the module itself.</span>
<span class="sd">    Args:</span>
<span class="sd">        memo: a memo to store the set of modules already added to the result</span>
<span class="sd">        prefix: a prefix that will be added to the name of the module</span>
<span class="sd">        remove_duplicate: whether to remove the duplicated module instances in the result</span>
<span class="sd">        or not</span>
<span class="sd">    Yields:</span>
<span class="sd">        (string, Module): Tuple of name and module</span>
<span class="sd">    Note:</span>
<span class="sd">        Duplicate modules are returned only once. In the following</span>
<span class="sd">        example, ``l`` will be returned only once.</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; l = nn.Linear(2, 2)</span>
<span class="sd">        &gt;&gt;&gt; net = nn.Sequential(l, l)</span>
<span class="sd">        &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):</span>
<span class="sd">                print(idx, &#39;-&gt;&#39;, m)</span>
<span class="sd">        0 -&gt; (&#39;&#39;, Sequential(</span>
<span class="sd">          (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">          (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        ))</span>
<span class="sd">        1 -&gt; (&#39;0&#39;, Linear(in_features=2, out_features=2, bias=True))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">memo</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">memo</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">if</span> <span class="bp">self</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">memo</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">remove_duplicate</span><span class="p">:</span>
            <span class="n">memo</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">prefix</span><span class="p">,</span> <span class="bp">self</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">submodule_prefix</span> <span class="o">=</span> <span class="n">prefix</span> <span class="o">+</span> <span class="p">(</span><span class="s1">&#39;.&#39;</span> <span class="k">if</span> <span class="n">prefix</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span><span class="p">)</span> <span class="o">+</span> <span class="n">name</span>
            <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_modules</span><span class="p">(</span><span class="n">memo</span><span class="p">,</span> <span class="n">submodule_prefix</span><span class="p">,</span> <span class="n">remove_duplicate</span><span class="p">):</span>
                <span class="k">yield</span> <span class="n">m</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.named_parameters">
    <p>def <span class="ident">named_parameters</span>(</p><p>self, prefix: str = &#39;&#39;, recurse: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over module parameters, yielding both the
name of the parameter as well as the parameter itself.</p>
<p>Args:
    prefix (str): prefix to prepend to all parameter names.
    recurse (bool): if True, then yields parameters of this module
        and all submodules. Otherwise, yields only parameters that
        are direct members of this module.</p>
<p>Yields:
    (string, Parameter): Tuple containing the name and parameter</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for name, param in self.named_parameters():
&gt;&gt;&gt;    if name in ['bias']:
&gt;&gt;&gt;        print(param.size())
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.named_parameters', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.named_parameters" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">named_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over module parameters, yielding both the</span>
<span class="sd">    name of the parameter as well as the parameter itself.</span>
<span class="sd">    Args:</span>
<span class="sd">        prefix (str): prefix to prepend to all parameter names.</span>
<span class="sd">        recurse (bool): if True, then yields parameters of this module</span>
<span class="sd">            and all submodules. Otherwise, yields only parameters that</span>
<span class="sd">            are direct members of this module.</span>
<span class="sd">    Yields:</span>
<span class="sd">        (string, Parameter): Tuple containing the name and parameter</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for name, param in self.named_parameters():</span>
<span class="sd">        &gt;&gt;&gt;    if name in [&#39;bias&#39;]:</span>
<span class="sd">        &gt;&gt;&gt;        print(param.size())</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">gen</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_named_members</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">module</span><span class="p">:</span> <span class="n">module</span><span class="o">.</span><span class="n">_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span>
        <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span> <span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">gen</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">elem</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.parameters">
    <p>def <span class="ident">parameters</span>(</p><p>self, recurse: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over module parameters.</p>
<p>This is typically passed to an optimizer.</p>
<p>Args:
    recurse (bool): if True, then yields parameters of this module
        and all submodules. Otherwise, yields only parameters that
        are direct members of this module.</p>
<p>Yields:
    Parameter: module parameter</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for param in model.parameters():
&gt;&gt;&gt;     print(type(param), param.size())
&lt;class 'torch.Tensor'&gt; (20L,)
&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.parameters', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.parameters" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Parameter</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over module parameters.</span>
<span class="sd">    This is typically passed to an optimizer.</span>
<span class="sd">    Args:</span>
<span class="sd">        recurse (bool): if True, then yields parameters of this module</span>
<span class="sd">            and all submodules. Otherwise, yields only parameters that</span>
<span class="sd">            are direct members of this module.</span>
<span class="sd">    Yields:</span>
<span class="sd">        Parameter: module parameter</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for param in model.parameters():</span>
<span class="sd">        &gt;&gt;&gt;     print(type(param), param.size())</span>
<span class="sd">        &lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="sd">        &lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">param</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.register_backward_hook">
    <p>def <span class="ident">register_backward_hook</span>(</p><p>self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]])</p>
    </div>
    

    
  
    <div class="desc"><p>Registers a backward hook on the module.</p>
<p>This function is deprecated in favor of :meth:<code>~torch.nn.Module.register_full_backward_hook</code> and
the behavior of this function will change in future versions.</p>
<p>Returns:
    :class:<code>torch.utils.hooks.RemovableHandle</code>:
        a handle that can be used to remove the added hook by calling
        <code>handle.remove()</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.register_backward_hook', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.register_backward_hook" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_backward_hook</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="s1">&#39;Module&#39;</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">],</span> <span class="n">Union</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a backward hook on the module.</span>
<span class="sd">    This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and</span>
<span class="sd">    the behavior of this function will change in future versions.</span>
<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_full_backward_hook</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Cannot use both regular backward hooks and full backward hooks on a &quot;</span>
                           <span class="s2">&quot;single Module. Please use only one of them.&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_is_full_backward_hook</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.register_buffer">
    <p>def <span class="ident">register_buffer</span>(</p><p>self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Adds a buffer to the module.</p>
<p>This is typically used to register a buffer that should not to be
considered a model parameter. For example, BatchNorm's <code>running_mean</code>
is not a parameter, but is part of the module's state. Buffers, by
default, are persistent and will be saved alongside parameters. This
behavior can be changed by setting :attr:<code>persistent</code> to <code>False</code>. The
only difference between a persistent buffer and a non-persistent buffer
is that the latter will not be a part of this module's
:attr:<code>state_dict</code>.</p>
<p>Buffers can be accessed as attributes using given names.</p>
<p>Args:
    name (string): name of the buffer. The buffer can be accessed
        from this module using the given name
    tensor (Tensor or None): buffer to be registered. If <code>None</code>, then operations
        that run on buffers, such as :attr:<code>cuda</code>, are ignored. If <code>None</code>,
        the buffer is <strong>not</strong> included in the module's :attr:<code>state_dict</code>.
    persistent (bool): whether the buffer is part of this module's
        :attr:<code>state_dict</code>.</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.register_buffer', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.register_buffer" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_buffer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">persistent</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Adds a buffer to the module.</span>
<span class="sd">    This is typically used to register a buffer that should not to be</span>
<span class="sd">    considered a model parameter. For example, BatchNorm&#39;s ``running_mean``</span>
<span class="sd">    is not a parameter, but is part of the module&#39;s state. Buffers, by</span>
<span class="sd">    default, are persistent and will be saved alongside parameters. This</span>
<span class="sd">    behavior can be changed by setting :attr:`persistent` to ``False``. The</span>
<span class="sd">    only difference between a persistent buffer and a non-persistent buffer</span>
<span class="sd">    is that the latter will not be a part of this module&#39;s</span>
<span class="sd">    :attr:`state_dict`.</span>
<span class="sd">    Buffers can be accessed as attributes using given names.</span>
<span class="sd">    Args:</span>
<span class="sd">        name (string): name of the buffer. The buffer can be accessed</span>
<span class="sd">            from this module using the given name</span>
<span class="sd">        tensor (Tensor or None): buffer to be registered. If ``None``, then operations</span>
<span class="sd">            that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,</span>
<span class="sd">            the buffer is **not** included in the module&#39;s :attr:`state_dict`.</span>
<span class="sd">        persistent (bool): whether the buffer is part of this module&#39;s</span>
<span class="sd">            :attr:`state_dict`.</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; self.register_buffer(&#39;running_mean&#39;, torch.zeros(num_features))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">persistent</span> <span class="ow">is</span> <span class="kc">False</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ScriptModule</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;ScriptModule does not support non-persistent buffers&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="s1">&#39;_buffers&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
            <span class="s2">&quot;cannot assign buffer before Module.__init__() call&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_six</span><span class="o">.</span><span class="n">string_classes</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;buffer name should be a string. &quot;</span>
                        <span class="s2">&quot;Got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">name</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="s1">&#39;.&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;buffer name can&#39;t contain </span><span class="se">\&quot;</span><span class="s2">.</span><span class="se">\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;buffer name can&#39;t be empty string </span><span class="se">\&quot;\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span> <span class="ow">and</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffers</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;attribute &#39;</span><span class="si">{}</span><span class="s2">&#39; already exists&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;cannot assign &#39;</span><span class="si">{}</span><span class="s2">&#39; object to buffer &#39;</span><span class="si">{}</span><span class="s2">&#39; &quot;</span>
                        <span class="s2">&quot;(torch Tensor or None required)&quot;</span>
                        <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">tensor</span><span class="p">),</span> <span class="n">name</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_buffers</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">tensor</span>
        <span class="k">if</span> <span class="n">persistent</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_non_persistent_buffers_set</span><span class="o">.</span><span class="n">discard</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_non_persistent_buffers_set</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.register_forward_hook">
    <p>def <span class="ident">register_forward_hook</span>(</p><p>self, hook: Callable[..., NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Registers a forward hook on the module.</p>
<p>The hook will be called every time after :func:<code>forward</code> has computed an output.
It should have the following signature::</p>
<pre><code>hook(module, input, output) -&gt; None or modified output
</code></pre>
<p>The input contains only the positional arguments given to the module.
Keyword arguments won't be passed to the hooks and only to the <code>forward</code>.
The hook can modify the output. It can modify the input inplace but
it will not have effect on forward since this is called after
:func:<code>forward</code> is called.</p>
<p>Returns:
    :class:<code>torch.utils.hooks.RemovableHandle</code>:
        a handle that can be used to remove the added hook by calling
        <code>handle.remove()</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.register_forward_hook', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.register_forward_hook" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_forward_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a forward hook on the module.</span>
<span class="sd">    The hook will be called every time after :func:`forward` has computed an output.</span>
<span class="sd">    It should have the following signature::</span>
<span class="sd">        hook(module, input, output) -&gt; None or modified output</span>
<span class="sd">    The input contains only the positional arguments given to the module.</span>
<span class="sd">    Keyword arguments won&#39;t be passed to the hooks and only to the ``forward``.</span>
<span class="sd">    The hook can modify the output. It can modify the input inplace but</span>
<span class="sd">    it will not have effect on forward since this is called after</span>
<span class="sd">    :func:`forward` is called.</span>
<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.register_forward_pre_hook">
    <p>def <span class="ident">register_forward_pre_hook</span>(</p><p>self, hook: Callable[..., NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Registers a forward pre-hook on the module.</p>
<p>The hook will be called every time before :func:<code>forward</code> is invoked.
It should have the following signature::</p>
<pre><code>hook(module, input) -&gt; None or modified input
</code></pre>
<p>The input contains only the positional arguments given to the module.
Keyword arguments won't be passed to the hooks and only to the <code>forward</code>.
The hook can modify the input. User can either return a tuple or a
single modified value in the hook. We will wrap the value into a tuple
if a single value is returned(unless that value is already a tuple).</p>
<p>Returns:
    :class:<code>torch.utils.hooks.RemovableHandle</code>:
        a handle that can be used to remove the added hook by calling
        <code>handle.remove()</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.register_forward_pre_hook', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.register_forward_pre_hook" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_forward_pre_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a forward pre-hook on the module.</span>
<span class="sd">    The hook will be called every time before :func:`forward` is invoked.</span>
<span class="sd">    It should have the following signature::</span>
<span class="sd">        hook(module, input) -&gt; None or modified input</span>
<span class="sd">    The input contains only the positional arguments given to the module.</span>
<span class="sd">    Keyword arguments won&#39;t be passed to the hooks and only to the ``forward``.</span>
<span class="sd">    The hook can modify the input. User can either return a tuple or a</span>
<span class="sd">    single modified value in the hook. We will wrap the value into a tuple</span>
<span class="sd">    if a single value is returned(unless that value is already a tuple).</span>
<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.register_full_backward_hook">
    <p>def <span class="ident">register_full_backward_hook</span>(</p><p>self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]])</p>
    </div>
    

    
  
    <div class="desc"><p>Registers a backward hook on the module.</p>
<p>The hook will be called every time the gradients with respect to module
inputs are computed. The hook should have the following signature::</p>
<pre><code>hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None
</code></pre>
<p>The :attr:<code>grad_input</code> and :attr:<code>grad_output</code> are tuples that contain the gradients
with respect to the inputs and outputs respectively. The hook should
not modify its arguments, but it can optionally return a new gradient with
respect to the input that will be used in place of :attr:<code>grad_input</code> in
subsequent computations. :attr:<code>grad_input</code> will only correspond to the inputs given
as positional arguments and all kwarg arguments are ignored. Entries
in :attr:<code>grad_input</code> and :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor
arguments.</p>
<p>For technical reasons, when this hook is applied to a Module, its forward function will
receive a view of each Tensor passed to the Module. Similarly the caller will receive a view
of each Tensor returned by the Module's forward function.</p>
<p>.. warning ::
    Modifying inputs or outputs inplace is not allowed when using backward hooks and
    will raise an error.</p>
<p>Returns:
    :class:<code>torch.utils.hooks.RemovableHandle</code>:
        a handle that can be used to remove the added hook by calling
        <code>handle.remove()</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.register_full_backward_hook', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.register_full_backward_hook" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_full_backward_hook</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="s1">&#39;Module&#39;</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">],</span> <span class="n">Union</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a backward hook on the module.</span>
<span class="sd">    The hook will be called every time the gradients with respect to module</span>
<span class="sd">    inputs are computed. The hook should have the following signature::</span>
<span class="sd">        hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None</span>
<span class="sd">    The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients</span>
<span class="sd">    with respect to the inputs and outputs respectively. The hook should</span>
<span class="sd">    not modify its arguments, but it can optionally return a new gradient with</span>
<span class="sd">    respect to the input that will be used in place of :attr:`grad_input` in</span>
<span class="sd">    subsequent computations. :attr:`grad_input` will only correspond to the inputs given</span>
<span class="sd">    as positional arguments and all kwarg arguments are ignored. Entries</span>
<span class="sd">    in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor</span>
<span class="sd">    arguments.</span>
<span class="sd">    For technical reasons, when this hook is applied to a Module, its forward function will</span>
<span class="sd">    receive a view of each Tensor passed to the Module. Similarly the caller will receive a view</span>
<span class="sd">    of each Tensor returned by the Module&#39;s forward function.</span>
<span class="sd">    .. warning ::</span>
<span class="sd">        Modifying inputs or outputs inplace is not allowed when using backward hooks and</span>
<span class="sd">        will raise an error.</span>
<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_full_backward_hook</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Cannot use both regular backward hooks and full backward hooks on a &quot;</span>
                           <span class="s2">&quot;single Module. Please use only one of them.&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_is_full_backward_hook</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.register_parameter">
    <p>def <span class="ident">register_parameter</span>(</p><p>self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Adds a parameter to the module.</p>
<p>The parameter can be accessed as an attribute using given name.</p>
<p>Args:
    name (string): name of the parameter. The parameter can be accessed
        from this module using the given name
    param (Parameter or None): parameter to be added to the module. If
        <code>None</code>, then operations that run on parameters, such as :attr:<code>cuda</code>,
        are ignored. If <code>None</code>, the parameter is <strong>not</strong> included in the
        module's :attr:<code>state_dict</code>.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.register_parameter', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.register_parameter" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_parameter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">param</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Parameter</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Adds a parameter to the module.</span>
<span class="sd">    The parameter can be accessed as an attribute using given name.</span>
<span class="sd">    Args:</span>
<span class="sd">        name (string): name of the parameter. The parameter can be accessed</span>
<span class="sd">            from this module using the given name</span>
<span class="sd">        param (Parameter or None): parameter to be added to the module. If</span>
<span class="sd">            ``None``, then operations that run on parameters, such as :attr:`cuda`,</span>
<span class="sd">            are ignored. If ``None``, the parameter is **not** included in the</span>
<span class="sd">            module&#39;s :attr:`state_dict`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="s1">&#39;_parameters&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
            <span class="s2">&quot;cannot assign parameter before Module.__init__() call&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_six</span><span class="o">.</span><span class="n">string_classes</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;parameter name should be a string. &quot;</span>
                        <span class="s2">&quot;Got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">name</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="s1">&#39;.&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;parameter name can&#39;t contain </span><span class="se">\&quot;</span><span class="s2">.</span><span class="se">\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;parameter name can&#39;t be empty string </span><span class="se">\&quot;\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span> <span class="ow">and</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parameters</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;attribute &#39;</span><span class="si">{}</span><span class="s2">&#39; already exists&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">param</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_parameters</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;cannot assign &#39;</span><span class="si">{}</span><span class="s2">&#39; object to parameter &#39;</span><span class="si">{}</span><span class="s2">&#39; &quot;</span>
                        <span class="s2">&quot;(torch.nn.Parameter or None required)&quot;</span>
                        <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">param</span><span class="p">),</span> <span class="n">name</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">param</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Cannot assign non-leaf Tensor to parameter &#39;</span><span class="si">{0}</span><span class="s2">&#39;. Model &quot;</span>
            <span class="s2">&quot;parameters must be created explicitly. To express &#39;</span><span class="si">{0}</span><span class="s2">&#39; &quot;</span>
            <span class="s2">&quot;as a function of another Tensor, compute the value in &quot;</span>
            <span class="s2">&quot;the forward() method.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_parameters</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">param</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.requires_grad_">
    <p>def <span class="ident">requires_grad_</span>(</p><p>self: ~T, requires_grad: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Change if autograd should record operations on parameters in this
module.</p>
<p>This method sets the parameters' :attr:<code>requires_grad</code> attributes
in-place.</p>
<p>This method is helpful for freezing part of the module for finetuning
or training parts of a model individually (e.g., GAN training).</p>
<p>See :ref:<code>locally-disable-grad-doc</code> for a comparison between
<code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.</p>
<p>Args:
    requires_grad (bool): whether autograd should record operations on
                          parameters in this module. Default: <code>True</code>.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.requires_grad_', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.requires_grad_" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Change if autograd should record operations on parameters in this</span>
<span class="sd">    module.</span>
<span class="sd">    This method sets the parameters&#39; :attr:`requires_grad` attributes</span>
<span class="sd">    in-place.</span>
<span class="sd">    This method is helpful for freezing part of the module for finetuning</span>
<span class="sd">    or training parts of a model individually (e.g., GAN training).</span>
<span class="sd">    See :ref:`locally-disable-grad-doc` for a comparison between</span>
<span class="sd">    `.requires_grad_()` and several similar mechanisms that may be confused with it.</span>
<span class="sd">    Args:</span>
<span class="sd">        requires_grad (bool): whether autograd should record operations on</span>
<span class="sd">                              parameters in this module. Default: ``True``.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="n">p</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="n">requires_grad</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.set_extra_state">
    <p>def <span class="ident">set_extra_state</span>(</p><p>self, state: Any)</p>
    </div>
    

    
  
    <div class="desc"><p>This function is called from :func:<code>load_state_dict</code> to handle any extra state
found within the <code>state_dict</code>. Implement this function and a corresponding
:func:<code>get_extra_state</code> for your module if you need to store extra state within its
<code>state_dict</code>.</p>
<p>Args:
    state (dict): Extra state from the <code>state_dict</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.set_extra_state', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.set_extra_state" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">set_extra_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function is called from :func:`load_state_dict` to handle any extra state</span>
<span class="sd">    found within the `state_dict`. Implement this function and a corresponding</span>
<span class="sd">    :func:`get_extra_state` for your module if you need to store extra state within its</span>
<span class="sd">    `state_dict`.</span>
<span class="sd">    Args:</span>
<span class="sd">        state (dict): Extra state from the `state_dict`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
        <span class="s2">&quot;Reached a code path in Module.set_extra_state() that should never be called. &quot;</span>
        <span class="s2">&quot;Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md &quot;</span>
        <span class="s2">&quot;to report this bug.&quot;</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.share_memory">
    <p>def <span class="ident">share_memory</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>See :meth:<code>torch.Tensor.share_memory_</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.share_memory', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.share_memory" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">share_memory</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;See :meth:`torch.Tensor.share_memory_`&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">share_memory_</span><span class="p">())</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.state_dict">
    <p>def <span class="ident">state_dict</span>(</p><p>self, destination=None, prefix=&#39;&#39;, keep_vars=False)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns a dictionary containing a whole state of the module.</p>
<p>Both parameters and persistent buffers (e.g. running averages) are
included. Keys are corresponding parameter and buffer names.
Parameters and buffers set to <code>None</code> are not included.</p>
<p>Returns:
    dict:
        a dictionary containing a whole state of the module</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; module.state_dict().keys()
['bias', 'weight']
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.state_dict', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.state_dict" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">destination</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">keep_vars</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns a dictionary containing a whole state of the module.</span>
<span class="sd">    Both parameters and persistent buffers (e.g. running averages) are</span>
<span class="sd">    included. Keys are corresponding parameter and buffer names.</span>
<span class="sd">    Parameters and buffers set to ``None`` are not included.</span>
<span class="sd">    Returns:</span>
<span class="sd">        dict:</span>
<span class="sd">            a dictionary containing a whole state of the module</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; module.state_dict().keys()</span>
<span class="sd">        [&#39;bias&#39;, &#39;weight&#39;]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">destination</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">destination</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="n">destination</span><span class="o">.</span><span class="n">_metadata</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
    <span class="n">destination</span><span class="o">.</span><span class="n">_metadata</span><span class="p">[</span><span class="n">prefix</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">local_metadata</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">version</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_version</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_save_to_state_dict</span><span class="p">(</span><span class="n">destination</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">keep_vars</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(</span><span class="n">destination</span><span class="p">,</span> <span class="n">prefix</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">keep_vars</span><span class="o">=</span><span class="n">keep_vars</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">hook</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_dict_hooks</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
        <span class="n">hook_result</span> <span class="o">=</span> <span class="n">hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">destination</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">local_metadata</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">hook_result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">destination</span> <span class="o">=</span> <span class="n">hook_result</span>
    <span class="k">return</span> <span class="n">destination</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.to">
    <p>def <span class="ident">to</span>(</p><p>self, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>Moves and/or casts the parameters and buffers.</p>
<p>This can be called as</p>
<p>.. function:: to(device=None, dtype=None, non_blocking=False)
   :noindex:</p>
<p>.. function:: to(dtype, non_blocking=False)
   :noindex:</p>
<p>.. function:: to(tensor, non_blocking=False)
   :noindex:</p>
<p>.. function:: to(memory_format=torch.channels_last)
   :noindex:</p>
<p>Its signature is similar to :meth:<code>torch.Tensor.to</code>, but only accepts
floating point or complex :attr:<code>dtype</code>\ s. In addition, this method will
only cast the floating point or complex parameters and buffers to :attr:<code>dtype</code>
(if given). The integral parameters and buffers will be moved
:attr:<code>device</code>, if that is given, but with dtypes unchanged. When
:attr:<code>non_blocking</code> is set, it tries to convert/move asynchronously
with respect to the host if possible, e.g., moving CPU Tensors with
pinned memory to CUDA devices.</p>
<p>See below for examples.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Args:
    device (:class:<code>torch.device</code>): the desired device of the parameters
        and buffers in this module
    dtype (:class:<code>torch.dtype</code>): the desired floating point or complex dtype of
        the parameters and buffers in this module
    tensor (torch.Tensor): Tensor whose dtype and device are the desired
        dtype and device for all parameters and buffers in this module
    memory_format (:class:<code>torch.memory_format</code>): the desired memory
        format for 4D parameters and buffers in this module (keyword
        only argument)</p>
<p>Returns:
    Module: self</p>
<p>Examples::</p>
<pre><code>&gt;&gt;&gt; linear = nn.Linear(2, 2)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1913, -0.3420],
        [-0.5113, -0.2325]])
&gt;&gt;&gt; linear.to(torch.double)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1913, -0.3420],
        [-0.5113, -0.2325]], dtype=torch.float64)
&gt;&gt;&gt; gpu1 = torch.device("cuda:1")
&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1914, -0.3420],
        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')
&gt;&gt;&gt; cpu = torch.device("cpu")
&gt;&gt;&gt; linear.to(cpu)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1914, -0.3420],
        [-0.5112, -0.2324]], dtype=torch.float16)

&gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.3741+0.j,  0.2382+0.j],
        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)
&gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))
tensor([[0.6122+0.j, 0.1150+0.j],
        [0.6122+0.j, 0.1150+0.j],
        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.to', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.to" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves and/or casts the parameters and buffers.</span>
<span class="sd">    This can be called as</span>
<span class="sd">    .. function:: to(device=None, dtype=None, non_blocking=False)</span>
<span class="sd">       :noindex:</span>
<span class="sd">    .. function:: to(dtype, non_blocking=False)</span>
<span class="sd">       :noindex:</span>
<span class="sd">    .. function:: to(tensor, non_blocking=False)</span>
<span class="sd">       :noindex:</span>
<span class="sd">    .. function:: to(memory_format=torch.channels_last)</span>
<span class="sd">       :noindex:</span>
<span class="sd">    Its signature is similar to :meth:`torch.Tensor.to`, but only accepts</span>
<span class="sd">    floating point or complex :attr:`dtype`\ s. In addition, this method will</span>
<span class="sd">    only cast the floating point or complex parameters and buffers to :attr:`dtype`</span>
<span class="sd">    (if given). The integral parameters and buffers will be moved</span>
<span class="sd">    :attr:`device`, if that is given, but with dtypes unchanged. When</span>
<span class="sd">    :attr:`non_blocking` is set, it tries to convert/move asynchronously</span>
<span class="sd">    with respect to the host if possible, e.g., moving CPU Tensors with</span>
<span class="sd">    pinned memory to CUDA devices.</span>
<span class="sd">    See below for examples.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Args:</span>
<span class="sd">        device (:class:`torch.device`): the desired device of the parameters</span>
<span class="sd">            and buffers in this module</span>
<span class="sd">        dtype (:class:`torch.dtype`): the desired floating point or complex dtype of</span>
<span class="sd">            the parameters and buffers in this module</span>
<span class="sd">        tensor (torch.Tensor): Tensor whose dtype and device are the desired</span>
<span class="sd">            dtype and device for all parameters and buffers in this module</span>
<span class="sd">        memory_format (:class:`torch.memory_format`): the desired memory</span>
<span class="sd">            format for 4D parameters and buffers in this module (keyword</span>
<span class="sd">            only argument)</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    Examples::</span>
<span class="sd">        &gt;&gt;&gt; linear = nn.Linear(2, 2)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.1913, -0.3420],</span>
<span class="sd">                [-0.5113, -0.2325]])</span>
<span class="sd">        &gt;&gt;&gt; linear.to(torch.double)</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.1913, -0.3420],</span>
<span class="sd">                [-0.5113, -0.2325]], dtype=torch.float64)</span>
<span class="sd">        &gt;&gt;&gt; gpu1 = torch.device(&quot;cuda:1&quot;)</span>
<span class="sd">        &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.1914, -0.3420],</span>
<span class="sd">                [-0.5112, -0.2324]], dtype=torch.float16, device=&#39;cuda:1&#39;)</span>
<span class="sd">        &gt;&gt;&gt; cpu = torch.device(&quot;cpu&quot;)</span>
<span class="sd">        &gt;&gt;&gt; linear.to(cpu)</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.1914, -0.3420],</span>
<span class="sd">                [-0.5112, -0.2324]], dtype=torch.float16)</span>
<span class="sd">        &gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.3741+0.j,  0.2382+0.j],</span>
<span class="sd">                [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)</span>
<span class="sd">        &gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))</span>
<span class="sd">        tensor([[0.6122+0.j, 0.1150+0.j],</span>
<span class="sd">                [0.6122+0.j, 0.1150+0.j],</span>
<span class="sd">                [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">,</span> <span class="n">convert_to_format</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">_parse_to</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_floating_point</span> <span class="ow">or</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;nn.Module.to only accepts floating point or complex &#39;</span>
                            <span class="s1">&#39;dtypes, but got desired dtype=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dtype</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Complex modules are a new feature under active development whose design may change, &quot;</span>
                <span class="s2">&quot;and some modules might not work as expected when using complex tensors as parameters or buffers. &quot;</span>
                <span class="s2">&quot;Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md &quot;</span>
                <span class="s2">&quot;if a complex module does not work as expected.&quot;</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">convert</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">convert_to_format</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">t</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">t</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="ow">or</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
                        <span class="n">non_blocking</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">convert_to_format</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">t</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="ow">or</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="n">convert</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.to_empty">
    <p>def <span class="ident">to_empty</span>(</p><p>self: ~T, *, device: Union[str, torch.device])</p>
    </div>
    

    
  
    <div class="desc"><p>Moves the parameters and buffers to the specified device without copying storage.</p>
<p>Args:
    device (:class:<code>torch.device</code>): The desired device of the parameters
        and buffers in this module.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.to_empty', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.to_empty" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">to_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves the parameters and buffers to the specified device without copying storage.</span>
<span class="sd">    Args:</span>
<span class="sd">        device (:class:`torch.device`): The desired device of the parameters</span>
<span class="sd">            and buffers in this module.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">))</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.train">
    <p>def <span class="ident">train</span>(</p><p>self: ~T, mode: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Sets the module in training mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>,
etc.</p>
<p>Args:
    mode (bool): whether to set training mode (<code>True</code>) or evaluation
                 mode (<code>False</code>). Default: <code>True</code>.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.train', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.train" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets the module in training mode.</span>
<span class="sd">    This has any effect only on certain modules. See documentations of</span>
<span class="sd">    particular modules for details of their behaviors in training/evaluation</span>
<span class="sd">    mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,</span>
<span class="sd">    etc.</span>
<span class="sd">    Args:</span>
<span class="sd">        mode (bool): whether to set training mode (``True``) or evaluation</span>
<span class="sd">                     mode (``False``). Default: ``True``.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;training mode is expected to be boolean&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="o">=</span> <span class="n">mode</span>
    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">():</span>
        <span class="n">module</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.type">
    <p>def <span class="ident">type</span>(</p><p>self: ~T, dst_type: Union[torch.dtype, str])</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Args:
    dst_type (type or string): the desired type</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.type', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.type" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">type</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">dst_type</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all parameters and buffers to :attr:`dst_type`.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Args:</span>
<span class="sd">        dst_type (type or string): the desired type</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">dst_type</span><span class="p">))</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.xpu">
    <p>def <span class="ident">xpu</span>(</p><p>self: ~T, device: Union[int, torch.device, NoneType] = None)</p>
    </div>
    

    
  
    <div class="desc"><p>Moves all model parameters and buffers to the XPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on XPU while being optimized.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Arguments:
    device (int, optional): if specified, all parameters will be
        copied to that device</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.xpu', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.xpu" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">xpu</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves all model parameters and buffers to the XPU.</span>
<span class="sd">    This also makes associated parameters and buffers different objects. So</span>
<span class="sd">    it should be called before constructing optimizer if the module will</span>
<span class="sd">    live on XPU while being optimized.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Arguments:</span>
<span class="sd">        device (int, optional): if specified, all parameters will be</span>
<span class="sd">            copied to that device</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">xpu</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Encoder.zero_grad">
    <p>def <span class="ident">zero_grad</span>(</p><p>self, set_to_none: bool = False)</p>
    </div>
    

    
  
    <div class="desc"><p>Sets gradients of all model parameters to zero. See similar function
under :class:<code>torch.optim.Optimizer</code> for more context.</p>
<p>Args:
    set_to_none (bool): instead of setting to zero, set the grads to None.
        See :meth:<code>torch.optim.Optimizer.zero_grad</code> for details.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Encoder.zero_grad', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Encoder.zero_grad" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">set_to_none</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets gradients of all model parameters to zero. See similar function</span>
<span class="sd">    under :class:`torch.optim.Optimizer` for more context.</span>
<span class="sd">    Args:</span>
<span class="sd">        set_to_none (bool): instead of setting to zero, set the grads to None.</span>
<span class="sd">            See :meth:`torch.optim.Optimizer.zero_grad` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_is_replica&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;Calling .zero_grad() from a module created with nn.DataParallel() has no effect. &quot;</span>
            <span class="s2">&quot;The parameters are copied (in a differentiable manner) from the original module. &quot;</span>
            <span class="s2">&quot;This means they are not leaf nodes in autograd and so don&#39;t accumulate gradients. &quot;</span>
            <span class="s2">&quot;If you need gradients in your forward method, consider using autograd.grad instead.&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">set_to_none</span><span class="p">:</span>
                <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">grad_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">detach_</span><span class="p">()</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
                <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</pre></div>

  </div>
</div>

  </div>
  
      </div>
      </div>
      
      <div class="item">
      <p id="seq2seq.EncoderLayer" class="name">class <span class="ident">EncoderLayer</span></p>
      
  
    <div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p></div>
  <div class="source_cont">
</div>


      <div class="class">
          <h3>Ancestors (in MRO)</h3>
          <ul class="class_list">
          <li><a href="/torch.nn.modules.module.Module.ext">torch.nn.modules.module.Module</a></li>
          </ul>
          <h3>Class variables</h3>
            <div class="item">
            <p id="seq2seq.EncoderLayer.T_destination" class="name">var <span class="ident">T_destination</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="seq2seq.EncoderLayer.dump_patches" class="name">var <span class="ident">dump_patches</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
          <h3>Methods</h3>
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.add_module">
    <p>def <span class="ident">add_module</span>(</p><p>self, name: str, module: Union[ForwardRef(&#39;Module&#39;), NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Adds a child module to the current module.</p>
<p>The module can be accessed as an attribute using the given name.</p>
<p>Args:
    name (string): name of the child module. The child module can be
        accessed from this module using the given name
    module (Module): child module to be added to the module.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.add_module', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.add_module" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">add_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s1">&#39;Module&#39;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Adds a child module to the current module.</span>
<span class="sd">    The module can be accessed as an attribute using the given name.</span>
<span class="sd">    Args:</span>
<span class="sd">        name (string): name of the child module. The child module can be</span>
<span class="sd">            accessed from this module using the given name</span>
<span class="sd">        module (Module): child module to be added to the module.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">Module</span><span class="p">)</span> <span class="ow">and</span> <span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> is not a Module subclass&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">module</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_six</span><span class="o">.</span><span class="n">string_classes</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;module name should be a string. Got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">name</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span> <span class="ow">and</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;attribute &#39;</span><span class="si">{}</span><span class="s2">&#39; already exists&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">elif</span> <span class="s1">&#39;.&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;module name can&#39;t contain </span><span class="se">\&quot;</span><span class="s2">.</span><span class="se">\&quot;</span><span class="s2">, got: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;module name can&#39;t be empty string </span><span class="se">\&quot;\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.apply">
    <p>def <span class="ident">apply</span>(</p><p>self: ~T, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>)
as well as self. Typical use includes initializing the parameters of a model
(see also :ref:<code>nn-init-doc</code>).</p>
<p>Args:
    fn (:class:<code>Module</code> -&gt; None): function to be applied to each submodule</p>
<p>Returns:
    Module: self</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; @torch.no_grad()
&gt;&gt;&gt; def init_weights(m):
&gt;&gt;&gt;     print(m)
&gt;&gt;&gt;     if type(m) == nn.Linear:
&gt;&gt;&gt;         m.weight.fill_(1.0)
&gt;&gt;&gt;         print(m.weight)
&gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))
&gt;&gt;&gt; net.apply(init_weights)
Linear(in_features=2, out_features=2, bias=True)
Parameter containing:
tensor([[ 1.,  1.],
        [ 1.,  1.]])
Linear(in_features=2, out_features=2, bias=True)
Parameter containing:
tensor([[ 1.,  1.],
        [ 1.,  1.]])
Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.apply', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.apply" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="s1">&#39;Module&#39;</span><span class="p">],</span> <span class="kc">None</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies ``fn`` recursively to every submodule (as returned by ``.children()``)</span>
<span class="sd">    as well as self. Typical use includes initializing the parameters of a model</span>
<span class="sd">    (see also :ref:`nn-init-doc`).</span>
<span class="sd">    Args:</span>
<span class="sd">        fn (:class:`Module` -&gt; None): function to be applied to each submodule</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; @torch.no_grad()</span>
<span class="sd">        &gt;&gt;&gt; def init_weights(m):</span>
<span class="sd">        &gt;&gt;&gt;     print(m)</span>
<span class="sd">        &gt;&gt;&gt;     if type(m) == nn.Linear:</span>
<span class="sd">        &gt;&gt;&gt;         m.weight.fill_(1.0)</span>
<span class="sd">        &gt;&gt;&gt;         print(m.weight)</span>
<span class="sd">        &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))</span>
<span class="sd">        &gt;&gt;&gt; net.apply(init_weights)</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 1.,  1.],</span>
<span class="sd">                [ 1.,  1.]])</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 1.,  1.],</span>
<span class="sd">                [ 1.,  1.]])</span>
<span class="sd">        Sequential(</span>
<span class="sd">          (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">          (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        )</span>
<span class="sd">        Sequential(</span>
<span class="sd">          (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">          (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        )</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">():</span>
        <span class="n">module</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
    <span class="n">fn</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.bfloat16">
    <p>def <span class="ident">bfloat16</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.bfloat16', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.bfloat16" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">bfloat16</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all floating point parameters and buffers to ``bfloat16`` datatype.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">()</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="k">else</span> <span class="n">t</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.buffers">
    <p>def <span class="ident">buffers</span>(</p><p>self, recurse: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over module buffers.</p>
<p>Args:
    recurse (bool): if True, then yields buffers of this module
        and all submodules. Otherwise, yields only buffers that
        are direct members of this module.</p>
<p>Yields:
    torch.Tensor: module buffer</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for buf in model.buffers():
&gt;&gt;&gt;     print(type(buf), buf.size())
&lt;class 'torch.Tensor'&gt; (20L,)
&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.buffers', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.buffers" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over module buffers.</span>
<span class="sd">    Args:</span>
<span class="sd">        recurse (bool): if True, then yields buffers of this module</span>
<span class="sd">            and all submodules. Otherwise, yields only buffers that</span>
<span class="sd">            are direct members of this module.</span>
<span class="sd">    Yields:</span>
<span class="sd">        torch.Tensor: module buffer</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for buf in model.buffers():</span>
<span class="sd">        &gt;&gt;&gt;     print(type(buf), buf.size())</span>
<span class="sd">        &lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="sd">        &lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">buf</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">buf</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.children">
    <p>def <span class="ident">children</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over immediate children modules.</p>
<p>Yields:
    Module: a child module</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.children', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.children" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">children</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="s1">&#39;Module&#39;</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over immediate children modules.</span>
<span class="sd">    Yields:</span>
<span class="sd">        Module: a child module</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
        <span class="k">yield</span> <span class="n">module</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.cpu">
    <p>def <span class="ident">cpu</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Moves all model parameters and buffers to the CPU.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.cpu', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.cpu" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves all model parameters and buffers to the CPU.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.cuda">
    <p>def <span class="ident">cuda</span>(</p><p>self: ~T, device: Union[int, torch.device, NoneType] = None)</p>
    </div>
    

    
  
    <div class="desc"><p>Moves all model parameters and buffers to the GPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on GPU while being optimized.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Args:
    device (int, optional): if specified, all parameters will be
        copied to that device</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.cuda', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.cuda" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">cuda</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves all model parameters and buffers to the GPU.</span>
<span class="sd">    This also makes associated parameters and buffers different objects. So</span>
<span class="sd">    it should be called before constructing optimizer if the module will</span>
<span class="sd">    live on GPU while being optimized.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Args:</span>
<span class="sd">        device (int, optional): if specified, all parameters will be</span>
<span class="sd">            copied to that device</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.double">
    <p>def <span class="ident">double</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.double', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.double" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">double</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all floating point parameters and buffers to ``double`` datatype.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">double</span><span class="p">()</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="k">else</span> <span class="n">t</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.eval">
    <p>def <span class="ident">eval</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Sets the module in evaluation mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>,
etc.</p>
<p>This is equivalent with :meth:<code>self.train(False) &lt;torch.nn.Module.train&gt;</code>.</p>
<p>See :ref:<code>locally-disable-grad-doc</code> for a comparison between
<code>.eval()</code> and several similar mechanisms that may be confused with it.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.eval', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.eval" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets the module in evaluation mode.</span>
<span class="sd">    This has any effect only on certain modules. See documentations of</span>
<span class="sd">    particular modules for details of their behaviors in training/evaluation</span>
<span class="sd">    mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,</span>
<span class="sd">    etc.</span>
<span class="sd">    This is equivalent with :meth:`self.train(False) &lt;torch.nn.Module.train&gt;`.</span>
<span class="sd">    See :ref:`locally-disable-grad-doc` for a comparison between</span>
<span class="sd">    `.eval()` and several similar mechanisms that may be confused with it.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.extra_repr">
    <p>def <span class="ident">extra_repr</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.extra_repr', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.extra_repr" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Set the extra representation of the module</span>
<span class="sd">    To print customized extra information, you should re-implement</span>
<span class="sd">    this method in your own modules. Both single-line and multi-line</span>
<span class="sd">    strings are acceptable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="s1">&#39;&#39;</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.float">
    <p>def <span class="ident">float</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.float', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.float" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">float</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all floating point parameters and buffers to ``float`` datatype.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="k">else</span> <span class="n">t</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.forward">
    <p>def <span class="ident">forward</span>(</p><p>self, src, src_mask)</p>
    </div>
    

    
  
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.forward', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.forward" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">):</span>
    
    <span class="c1">#src = [batch size, src len, hid dim]</span>
    <span class="c1">#src_mask = [batch size, 1, 1, src len] </span>
            
    <span class="c1">#self attention</span>
    <span class="n">_src</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attention</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
    
    <span class="c1">#dropout, residual connection and layer norm</span>
    <span class="n">src</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_layer_norm</span><span class="p">(</span><span class="n">src</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">_src</span><span class="p">))</span>
    
    <span class="c1">#src = [batch size, src len, hid dim]</span>
    
    <span class="c1">#positionwise feedforward</span>
    <span class="n">_src</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">positionwise_feedforward</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
    
    <span class="c1">#dropout, residual and layer norm</span>
    <span class="n">src</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff_layer_norm</span><span class="p">(</span><span class="n">src</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">_src</span><span class="p">))</span>
    
    <span class="c1">#src = [batch size, src len, hid dim]</span>
    
    <span class="k">return</span> <span class="n">src</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.get_buffer">
    <p>def <span class="ident">get_buffer</span>(</p><p>self, target: str)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the buffer given by <code>target</code> if it exists,
otherwise throws an error.</p>
<p>See the docstring for <code>get_submodule</code> for a more detailed
explanation of this method's functionality as well as how to
correctly specify <code>target</code>.</p>
<p>Args:
    target: The fully-qualified string name of the buffer
        to look for. (See <code>get_submodule</code> for how to specify a
        fully-qualified string.)</p>
<p>Returns:
    torch.Tensor: The buffer referenced by <code>target</code></p>
<p>Raises:
    AttributeError: If the target string references an invalid
        path or resolves to something that is not a
        buffer</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.get_buffer', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.get_buffer" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_buffer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the buffer given by ``target`` if it exists,</span>
<span class="sd">    otherwise throws an error.</span>
<span class="sd">    See the docstring for ``get_submodule`` for a more detailed</span>
<span class="sd">    explanation of this method&#39;s functionality as well as how to</span>
<span class="sd">    correctly specify ``target``.</span>
<span class="sd">    Args:</span>
<span class="sd">        target: The fully-qualified string name of the buffer</span>
<span class="sd">            to look for. (See ``get_submodule`` for how to specify a</span>
<span class="sd">            fully-qualified string.)</span>
<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: The buffer referenced by ``target``</span>
<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: If the target string references an invalid</span>
<span class="sd">            path or resolves to something that is not a</span>
<span class="sd">            buffer</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">module_path</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">buffer_name</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">rpartition</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
    <span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_submodule</span><span class="p">(</span><span class="n">module_path</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">buffer_name</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">_get_name</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot; has no attribute `&quot;</span>
                             <span class="o">+</span> <span class="n">buffer_name</span> <span class="o">+</span> <span class="s2">&quot;`&quot;</span><span class="p">)</span>
    <span class="n">buffer</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">buffer_name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">buffer_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">mod</span><span class="o">.</span><span class="n">_buffers</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s2">&quot;`&quot;</span> <span class="o">+</span> <span class="n">buffer_name</span> <span class="o">+</span> <span class="s2">&quot;` is not a buffer&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">buffer</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.get_extra_state">
    <p>def <span class="ident">get_extra_state</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns any extra state to include in the module's state_dict.
Implement this and a corresponding :func:<code>set_extra_state</code> for your module
if you need to store extra state. This function is called when building the
module's <code>state_dict()</code>.</p>
<p>Note that extra state should be pickleable to ensure working serialization
of the state_dict. We only provide provide backwards compatibility guarantees
for serializing Tensors; other objects may break backwards compatibility if
their serialized pickled form changes.</p>
<p>Returns:
    object: Any extra state to store in the module's state_dict</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.get_extra_state', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.get_extra_state" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_extra_state</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns any extra state to include in the module&#39;s state_dict.</span>
<span class="sd">    Implement this and a corresponding :func:`set_extra_state` for your module</span>
<span class="sd">    if you need to store extra state. This function is called when building the</span>
<span class="sd">    module&#39;s `state_dict()`.</span>
<span class="sd">    Note that extra state should be pickleable to ensure working serialization</span>
<span class="sd">    of the state_dict. We only provide provide backwards compatibility guarantees</span>
<span class="sd">    for serializing Tensors; other objects may break backwards compatibility if</span>
<span class="sd">    their serialized pickled form changes.</span>
<span class="sd">    Returns:</span>
<span class="sd">        object: Any extra state to store in the module&#39;s state_dict</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
        <span class="s2">&quot;Reached a code path in Module.get_extra_state() that should never be called. &quot;</span>
        <span class="s2">&quot;Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md &quot;</span>
        <span class="s2">&quot;to report this bug.&quot;</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.get_parameter">
    <p>def <span class="ident">get_parameter</span>(</p><p>self, target: str)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the parameter given by <code>target</code> if it exists,
otherwise throws an error.</p>
<p>See the docstring for <code>get_submodule</code> for a more detailed
explanation of this method's functionality as well as how to
correctly specify <code>target</code>.</p>
<p>Args:
    target: The fully-qualified string name of the Parameter
        to look for. (See <code>get_submodule</code> for how to specify a
        fully-qualified string.)</p>
<p>Returns:
    torch.nn.Parameter: The Parameter referenced by <code>target</code></p>
<p>Raises:
    AttributeError: If the target string references an invalid
        path or resolves to something that is not an
        <code>nn.Parameter</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.get_parameter', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.get_parameter" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_parameter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Parameter&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the parameter given by ``target`` if it exists,</span>
<span class="sd">    otherwise throws an error.</span>
<span class="sd">    See the docstring for ``get_submodule`` for a more detailed</span>
<span class="sd">    explanation of this method&#39;s functionality as well as how to</span>
<span class="sd">    correctly specify ``target``.</span>
<span class="sd">    Args:</span>
<span class="sd">        target: The fully-qualified string name of the Parameter</span>
<span class="sd">            to look for. (See ``get_submodule`` for how to specify a</span>
<span class="sd">            fully-qualified string.)</span>
<span class="sd">    Returns:</span>
<span class="sd">        torch.nn.Parameter: The Parameter referenced by ``target``</span>
<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: If the target string references an invalid</span>
<span class="sd">            path or resolves to something that is not an</span>
<span class="sd">            ``nn.Parameter``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">module_path</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">param_name</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">rpartition</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
    <span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_submodule</span><span class="p">(</span><span class="n">module_path</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">param_name</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">_get_name</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot; has no attribute `&quot;</span>
                             <span class="o">+</span> <span class="n">param_name</span> <span class="o">+</span> <span class="s2">&quot;`&quot;</span><span class="p">)</span>
    <span class="n">param</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">param_name</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s2">&quot;`&quot;</span> <span class="o">+</span> <span class="n">param_name</span> <span class="o">+</span> <span class="s2">&quot;` is not an &quot;</span>
                             <span class="s2">&quot;nn.Parameter&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">param</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.get_submodule">
    <p>def <span class="ident">get_submodule</span>(</p><p>self, target: str)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the submodule given by <code>target</code> if it exists,
otherwise throws an error.</p>
<p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that
looks like this:</p>
<p>.. code-block::text</p>
<pre><code>A(
    (net_b): Module(
        (net_c): Module(
            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))
        )
        (linear): Linear(in_features=100, out_features=200, bias=True)
    )
)
</code></pre>
<p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested
submodule <code>net_b</code>, which itself has two submodules <code>net_c</code>
and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p>
<p>To check whether or not we have the <code>linear</code> submodule, we
would call <code>get_submodule("net_b.linear")</code>. To check whether
we have the <code>conv</code> submodule, we would call
<code>get_submodule("net_b.net_c.conv")</code>.</p>
<p>The runtime of <code>get_submodule</code> is bounded by the degree
of module nesting in <code>target</code>. A query against
<code>named_modules</code> achieves the same result, but it is O(N) in
the number of transitive modules. So, for a simple check to see
if some submodule exists, <code>get_submodule</code> should always be
used.</p>
<p>Args:
    target: The fully-qualified string name of the submodule
        to look for. (See above example for how to specify a
        fully-qualified string.)</p>
<p>Returns:
    torch.nn.Module: The submodule referenced by <code>target</code></p>
<p>Raises:
    AttributeError: If the target string references an invalid
        path or resolves to something that is not an
        <code>nn.Module</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.get_submodule', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.get_submodule" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_submodule</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Module&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the submodule given by ``target`` if it exists,</span>
<span class="sd">    otherwise throws an error.</span>
<span class="sd">    For example, let&#39;s say you have an ``nn.Module`` ``A`` that</span>
<span class="sd">    looks like this:</span>
<span class="sd">    .. code-block::text</span>
<span class="sd">        A(</span>
<span class="sd">            (net_b): Module(</span>
<span class="sd">                (net_c): Module(</span>
<span class="sd">                    (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))</span>
<span class="sd">                )</span>
<span class="sd">                (linear): Linear(in_features=100, out_features=200, bias=True)</span>
<span class="sd">            )</span>
<span class="sd">        )</span>
<span class="sd">    (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested</span>
<span class="sd">    submodule ``net_b``, which itself has two submodules ``net_c``</span>
<span class="sd">    and ``linear``. ``net_c`` then has a submodule ``conv``.)</span>
<span class="sd">    To check whether or not we have the ``linear`` submodule, we</span>
<span class="sd">    would call ``get_submodule(&quot;net_b.linear&quot;)``. To check whether</span>
<span class="sd">    we have the ``conv`` submodule, we would call</span>
<span class="sd">    ``get_submodule(&quot;net_b.net_c.conv&quot;)``.</span>
<span class="sd">    The runtime of ``get_submodule`` is bounded by the degree</span>
<span class="sd">    of module nesting in ``target``. A query against</span>
<span class="sd">    ``named_modules`` achieves the same result, but it is O(N) in</span>
<span class="sd">    the number of transitive modules. So, for a simple check to see</span>
<span class="sd">    if some submodule exists, ``get_submodule`` should always be</span>
<span class="sd">    used.</span>
<span class="sd">    Args:</span>
<span class="sd">        target: The fully-qualified string name of the submodule</span>
<span class="sd">            to look for. (See above example for how to specify a</span>
<span class="sd">            fully-qualified string.)</span>
<span class="sd">    Returns:</span>
<span class="sd">        torch.nn.Module: The submodule referenced by ``target``</span>
<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: If the target string references an invalid</span>
<span class="sd">            path or resolves to something that is not an</span>
<span class="sd">            ``nn.Module``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">target</span> <span class="o">==</span> <span class="s2">&quot;&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span>
    <span class="n">atoms</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
    <span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="bp">self</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">atoms</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">_get_name</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot; has no &quot;</span>
                                 <span class="s2">&quot;attribute `&quot;</span> <span class="o">+</span> <span class="n">item</span> <span class="o">+</span> <span class="s2">&quot;`&quot;</span><span class="p">)</span>
        <span class="n">mod</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">item</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s2">&quot;`&quot;</span> <span class="o">+</span> <span class="n">item</span> <span class="o">+</span> <span class="s2">&quot;` is not &quot;</span>
                                 <span class="s2">&quot;an nn.Module&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mod</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.half">
    <p>def <span class="ident">half</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.half', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.half" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">half</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all floating point parameters and buffers to ``half`` datatype.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">half</span><span class="p">()</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="k">else</span> <span class="n">t</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.load_state_dict">
    <p>def <span class="ident">load_state_dict</span>(</p><p>self, state_dict: &#39;OrderedDict[str, Tensor]&#39;, strict: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Copies parameters and buffers from :attr:<code>state_dict</code> into
this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then
the keys of :attr:<code>state_dict</code> must exactly match the keys returned
by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.</p>
<p>Args:
    state_dict (dict): a dict containing parameters and
        persistent buffers.
    strict (bool, optional): whether to strictly enforce that the keys
        in :attr:<code>state_dict</code> match the keys returned by this module's
        :meth:<code>~torch.nn.Module.state_dict</code> function. Default: <code>True</code></p>
<p>Returns:
    <code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:
        * <strong>missing_keys</strong> is a list of str containing the missing keys
        * <strong>unexpected_keys</strong> is a list of str containing the unexpected keys</p>
<p>Note:
    If a parameter or buffer is registered as <code>None</code> and its corresponding key
    exists in :attr:<code>state_dict</code>, :meth:<code>load_state_dict</code> will raise a
    <code>RuntimeError</code>.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.load_state_dict', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.load_state_dict" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="s1">&#39;OrderedDict[str, Tensor]&#39;</span><span class="p">,</span>
                    <span class="n">strict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Copies parameters and buffers from :attr:`state_dict` into</span>
<span class="sd">    this module and its descendants. If :attr:`strict` is ``True``, then</span>
<span class="sd">    the keys of :attr:`state_dict` must exactly match the keys returned</span>
<span class="sd">    by this module&#39;s :meth:`~torch.nn.Module.state_dict` function.</span>
<span class="sd">    Args:</span>
<span class="sd">        state_dict (dict): a dict containing parameters and</span>
<span class="sd">            persistent buffers.</span>
<span class="sd">        strict (bool, optional): whether to strictly enforce that the keys</span>
<span class="sd">            in :attr:`state_dict` match the keys returned by this module&#39;s</span>
<span class="sd">            :meth:`~torch.nn.Module.state_dict` function. Default: ``True``</span>
<span class="sd">    Returns:</span>
<span class="sd">        ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:</span>
<span class="sd">            * **missing_keys** is a list of str containing the missing keys</span>
<span class="sd">            * **unexpected_keys** is a list of str containing the unexpected keys</span>
<span class="sd">    Note:</span>
<span class="sd">        If a parameter or buffer is registered as ``None`` and its corresponding key</span>
<span class="sd">        exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a</span>
<span class="sd">        ``RuntimeError``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">missing_keys</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">unexpected_keys</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">error_msgs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># copy state_dict so _load_from_state_dict can modify it</span>
    <span class="n">metadata</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="s1">&#39;_metadata&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># mypy isn&#39;t aware that &quot;_metadata&quot; exists in state_dict</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">_metadata</span> <span class="o">=</span> <span class="n">metadata</span>  <span class="c1"># type: ignore[attr-defined]</span>
    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
        <span class="n">local_metadata</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="n">metadata</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">metadata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">prefix</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">{})</span>
        <span class="n">module</span><span class="o">.</span><span class="n">_load_from_state_dict</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">local_metadata</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">missing_keys</span><span class="p">,</span> <span class="n">unexpected_keys</span><span class="p">,</span> <span class="n">error_msgs</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">child</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">load</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">prefix</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>
    <span class="n">load</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">load</span>
    <span class="k">if</span> <span class="n">strict</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">unexpected_keys</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">error_msgs</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span>
                <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Unexpected key(s) in state_dict: </span><span class="si">{}</span><span class="s1">. &#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;&quot;</span><span class="si">{}</span><span class="s1">&quot;&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">unexpected_keys</span><span class="p">)))</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">missing_keys</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">error_msgs</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span>
                <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Missing key(s) in state_dict: </span><span class="si">{}</span><span class="s1">. &#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;&quot;</span><span class="si">{}</span><span class="s1">&quot;&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">missing_keys</span><span class="p">)))</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">error_msgs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Error(s) in loading state_dict for </span><span class="si">{}</span><span class="s1">:</span><span class="se">\n\t</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                           <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n\t</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">error_msgs</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">_IncompatibleKeys</span><span class="p">(</span><span class="n">missing_keys</span><span class="p">,</span> <span class="n">unexpected_keys</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.modules">
    <p>def <span class="ident">modules</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over all modules in the network.</p>
<p>Yields:
    Module: a module in the network</p>
<p>Note:
    Duplicate modules are returned only once. In the following
    example, <code>l</code> will be returned only once.</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; l = nn.Linear(2, 2)
&gt;&gt;&gt; net = nn.Sequential(l, l)
&gt;&gt;&gt; for idx, m in enumerate(net.modules()):
        print(idx, '-&gt;', m)

0 -&gt; Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
1 -&gt; Linear(in_features=2, out_features=2, bias=True)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.modules', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.modules" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">modules</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="s1">&#39;Module&#39;</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over all modules in the network.</span>
<span class="sd">    Yields:</span>
<span class="sd">        Module: a module in the network</span>
<span class="sd">    Note:</span>
<span class="sd">        Duplicate modules are returned only once. In the following</span>
<span class="sd">        example, ``l`` will be returned only once.</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; l = nn.Linear(2, 2)</span>
<span class="sd">        &gt;&gt;&gt; net = nn.Sequential(l, l)</span>
<span class="sd">        &gt;&gt;&gt; for idx, m in enumerate(net.modules()):</span>
<span class="sd">                print(idx, &#39;-&gt;&#39;, m)</span>
<span class="sd">        0 -&gt; Sequential(</span>
<span class="sd">          (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">          (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        )</span>
<span class="sd">        1 -&gt; Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
        <span class="k">yield</span> <span class="n">module</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.named_buffers">
    <p>def <span class="ident">named_buffers</span>(</p><p>self, prefix: str = &#39;&#39;, recurse: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over module buffers, yielding both the
name of the buffer as well as the buffer itself.</p>
<p>Args:
    prefix (str): prefix to prepend to all buffer names.
    recurse (bool): if True, then yields buffers of this module
        and all submodules. Otherwise, yields only buffers that
        are direct members of this module.</p>
<p>Yields:
    (string, torch.Tensor): Tuple containing the name and buffer</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for name, buf in self.named_buffers():
&gt;&gt;&gt;    if name in ['running_var']:
&gt;&gt;&gt;        print(buf.size())
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.named_buffers', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.named_buffers" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">named_buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over module buffers, yielding both the</span>
<span class="sd">    name of the buffer as well as the buffer itself.</span>
<span class="sd">    Args:</span>
<span class="sd">        prefix (str): prefix to prepend to all buffer names.</span>
<span class="sd">        recurse (bool): if True, then yields buffers of this module</span>
<span class="sd">            and all submodules. Otherwise, yields only buffers that</span>
<span class="sd">            are direct members of this module.</span>
<span class="sd">    Yields:</span>
<span class="sd">        (string, torch.Tensor): Tuple containing the name and buffer</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for name, buf in self.named_buffers():</span>
<span class="sd">        &gt;&gt;&gt;    if name in [&#39;running_var&#39;]:</span>
<span class="sd">        &gt;&gt;&gt;        print(buf.size())</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">gen</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_named_members</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">module</span><span class="p">:</span> <span class="n">module</span><span class="o">.</span><span class="n">_buffers</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span>
        <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span> <span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">gen</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">elem</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.named_children">
    <p>def <span class="ident">named_children</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over immediate children modules, yielding both
the name of the module as well as the module itself.</p>
<p>Yields:
    (string, Module): Tuple containing a name and child module</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for name, module in model.named_children():
&gt;&gt;&gt;     if name in ['conv4', 'conv5']:
&gt;&gt;&gt;         print(module)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.named_children', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.named_children" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">named_children</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s1">&#39;Module&#39;</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over immediate children modules, yielding both</span>
<span class="sd">    the name of the module as well as the module itself.</span>
<span class="sd">    Yields:</span>
<span class="sd">        (string, Module): Tuple containing a name and child module</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for name, module in model.named_children():</span>
<span class="sd">        &gt;&gt;&gt;     if name in [&#39;conv4&#39;, &#39;conv5&#39;]:</span>
<span class="sd">        &gt;&gt;&gt;         print(module)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">memo</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">module</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">memo</span><span class="p">:</span>
            <span class="n">memo</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
            <span class="k">yield</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.named_modules">
    <p>def <span class="ident">named_modules</span>(</p><p>self, memo: Union[Set[ForwardRef(&#39;Module&#39;)], NoneType] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over all modules in the network, yielding
both the name of the module as well as the module itself.</p>
<p>Args:
    memo: a memo to store the set of modules already added to the result
    prefix: a prefix that will be added to the name of the module
    remove_duplicate: whether to remove the duplicated module instances in the result
    or not</p>
<p>Yields:
    (string, Module): Tuple of name and module</p>
<p>Note:
    Duplicate modules are returned only once. In the following
    example, <code>l</code> will be returned only once.</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; l = nn.Linear(2, 2)
&gt;&gt;&gt; net = nn.Sequential(l, l)
&gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):
        print(idx, '-&gt;', m)

0 -&gt; ('', Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
))
1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.named_modules', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.named_modules" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">named_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memo</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Set</span><span class="p">[</span><span class="s1">&#39;Module&#39;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">remove_duplicate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over all modules in the network, yielding</span>
<span class="sd">    both the name of the module as well as the module itself.</span>
<span class="sd">    Args:</span>
<span class="sd">        memo: a memo to store the set of modules already added to the result</span>
<span class="sd">        prefix: a prefix that will be added to the name of the module</span>
<span class="sd">        remove_duplicate: whether to remove the duplicated module instances in the result</span>
<span class="sd">        or not</span>
<span class="sd">    Yields:</span>
<span class="sd">        (string, Module): Tuple of name and module</span>
<span class="sd">    Note:</span>
<span class="sd">        Duplicate modules are returned only once. In the following</span>
<span class="sd">        example, ``l`` will be returned only once.</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; l = nn.Linear(2, 2)</span>
<span class="sd">        &gt;&gt;&gt; net = nn.Sequential(l, l)</span>
<span class="sd">        &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):</span>
<span class="sd">                print(idx, &#39;-&gt;&#39;, m)</span>
<span class="sd">        0 -&gt; (&#39;&#39;, Sequential(</span>
<span class="sd">          (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">          (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        ))</span>
<span class="sd">        1 -&gt; (&#39;0&#39;, Linear(in_features=2, out_features=2, bias=True))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">memo</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">memo</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">if</span> <span class="bp">self</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">memo</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">remove_duplicate</span><span class="p">:</span>
            <span class="n">memo</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">prefix</span><span class="p">,</span> <span class="bp">self</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">submodule_prefix</span> <span class="o">=</span> <span class="n">prefix</span> <span class="o">+</span> <span class="p">(</span><span class="s1">&#39;.&#39;</span> <span class="k">if</span> <span class="n">prefix</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span><span class="p">)</span> <span class="o">+</span> <span class="n">name</span>
            <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_modules</span><span class="p">(</span><span class="n">memo</span><span class="p">,</span> <span class="n">submodule_prefix</span><span class="p">,</span> <span class="n">remove_duplicate</span><span class="p">):</span>
                <span class="k">yield</span> <span class="n">m</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.named_parameters">
    <p>def <span class="ident">named_parameters</span>(</p><p>self, prefix: str = &#39;&#39;, recurse: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over module parameters, yielding both the
name of the parameter as well as the parameter itself.</p>
<p>Args:
    prefix (str): prefix to prepend to all parameter names.
    recurse (bool): if True, then yields parameters of this module
        and all submodules. Otherwise, yields only parameters that
        are direct members of this module.</p>
<p>Yields:
    (string, Parameter): Tuple containing the name and parameter</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for name, param in self.named_parameters():
&gt;&gt;&gt;    if name in ['bias']:
&gt;&gt;&gt;        print(param.size())
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.named_parameters', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.named_parameters" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">named_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over module parameters, yielding both the</span>
<span class="sd">    name of the parameter as well as the parameter itself.</span>
<span class="sd">    Args:</span>
<span class="sd">        prefix (str): prefix to prepend to all parameter names.</span>
<span class="sd">        recurse (bool): if True, then yields parameters of this module</span>
<span class="sd">            and all submodules. Otherwise, yields only parameters that</span>
<span class="sd">            are direct members of this module.</span>
<span class="sd">    Yields:</span>
<span class="sd">        (string, Parameter): Tuple containing the name and parameter</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for name, param in self.named_parameters():</span>
<span class="sd">        &gt;&gt;&gt;    if name in [&#39;bias&#39;]:</span>
<span class="sd">        &gt;&gt;&gt;        print(param.size())</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">gen</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_named_members</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">module</span><span class="p">:</span> <span class="n">module</span><span class="o">.</span><span class="n">_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span>
        <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span> <span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">gen</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">elem</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.parameters">
    <p>def <span class="ident">parameters</span>(</p><p>self, recurse: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over module parameters.</p>
<p>This is typically passed to an optimizer.</p>
<p>Args:
    recurse (bool): if True, then yields parameters of this module
        and all submodules. Otherwise, yields only parameters that
        are direct members of this module.</p>
<p>Yields:
    Parameter: module parameter</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for param in model.parameters():
&gt;&gt;&gt;     print(type(param), param.size())
&lt;class 'torch.Tensor'&gt; (20L,)
&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.parameters', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.parameters" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Parameter</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over module parameters.</span>
<span class="sd">    This is typically passed to an optimizer.</span>
<span class="sd">    Args:</span>
<span class="sd">        recurse (bool): if True, then yields parameters of this module</span>
<span class="sd">            and all submodules. Otherwise, yields only parameters that</span>
<span class="sd">            are direct members of this module.</span>
<span class="sd">    Yields:</span>
<span class="sd">        Parameter: module parameter</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for param in model.parameters():</span>
<span class="sd">        &gt;&gt;&gt;     print(type(param), param.size())</span>
<span class="sd">        &lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="sd">        &lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">param</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.register_backward_hook">
    <p>def <span class="ident">register_backward_hook</span>(</p><p>self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]])</p>
    </div>
    

    
  
    <div class="desc"><p>Registers a backward hook on the module.</p>
<p>This function is deprecated in favor of :meth:<code>~torch.nn.Module.register_full_backward_hook</code> and
the behavior of this function will change in future versions.</p>
<p>Returns:
    :class:<code>torch.utils.hooks.RemovableHandle</code>:
        a handle that can be used to remove the added hook by calling
        <code>handle.remove()</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.register_backward_hook', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.register_backward_hook" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_backward_hook</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="s1">&#39;Module&#39;</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">],</span> <span class="n">Union</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a backward hook on the module.</span>
<span class="sd">    This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and</span>
<span class="sd">    the behavior of this function will change in future versions.</span>
<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_full_backward_hook</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Cannot use both regular backward hooks and full backward hooks on a &quot;</span>
                           <span class="s2">&quot;single Module. Please use only one of them.&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_is_full_backward_hook</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.register_buffer">
    <p>def <span class="ident">register_buffer</span>(</p><p>self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Adds a buffer to the module.</p>
<p>This is typically used to register a buffer that should not to be
considered a model parameter. For example, BatchNorm's <code>running_mean</code>
is not a parameter, but is part of the module's state. Buffers, by
default, are persistent and will be saved alongside parameters. This
behavior can be changed by setting :attr:<code>persistent</code> to <code>False</code>. The
only difference between a persistent buffer and a non-persistent buffer
is that the latter will not be a part of this module's
:attr:<code>state_dict</code>.</p>
<p>Buffers can be accessed as attributes using given names.</p>
<p>Args:
    name (string): name of the buffer. The buffer can be accessed
        from this module using the given name
    tensor (Tensor or None): buffer to be registered. If <code>None</code>, then operations
        that run on buffers, such as :attr:<code>cuda</code>, are ignored. If <code>None</code>,
        the buffer is <strong>not</strong> included in the module's :attr:<code>state_dict</code>.
    persistent (bool): whether the buffer is part of this module's
        :attr:<code>state_dict</code>.</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.register_buffer', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.register_buffer" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_buffer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">persistent</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Adds a buffer to the module.</span>
<span class="sd">    This is typically used to register a buffer that should not to be</span>
<span class="sd">    considered a model parameter. For example, BatchNorm&#39;s ``running_mean``</span>
<span class="sd">    is not a parameter, but is part of the module&#39;s state. Buffers, by</span>
<span class="sd">    default, are persistent and will be saved alongside parameters. This</span>
<span class="sd">    behavior can be changed by setting :attr:`persistent` to ``False``. The</span>
<span class="sd">    only difference between a persistent buffer and a non-persistent buffer</span>
<span class="sd">    is that the latter will not be a part of this module&#39;s</span>
<span class="sd">    :attr:`state_dict`.</span>
<span class="sd">    Buffers can be accessed as attributes using given names.</span>
<span class="sd">    Args:</span>
<span class="sd">        name (string): name of the buffer. The buffer can be accessed</span>
<span class="sd">            from this module using the given name</span>
<span class="sd">        tensor (Tensor or None): buffer to be registered. If ``None``, then operations</span>
<span class="sd">            that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,</span>
<span class="sd">            the buffer is **not** included in the module&#39;s :attr:`state_dict`.</span>
<span class="sd">        persistent (bool): whether the buffer is part of this module&#39;s</span>
<span class="sd">            :attr:`state_dict`.</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; self.register_buffer(&#39;running_mean&#39;, torch.zeros(num_features))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">persistent</span> <span class="ow">is</span> <span class="kc">False</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ScriptModule</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;ScriptModule does not support non-persistent buffers&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="s1">&#39;_buffers&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
            <span class="s2">&quot;cannot assign buffer before Module.__init__() call&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_six</span><span class="o">.</span><span class="n">string_classes</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;buffer name should be a string. &quot;</span>
                        <span class="s2">&quot;Got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">name</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="s1">&#39;.&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;buffer name can&#39;t contain </span><span class="se">\&quot;</span><span class="s2">.</span><span class="se">\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;buffer name can&#39;t be empty string </span><span class="se">\&quot;\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span> <span class="ow">and</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffers</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;attribute &#39;</span><span class="si">{}</span><span class="s2">&#39; already exists&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;cannot assign &#39;</span><span class="si">{}</span><span class="s2">&#39; object to buffer &#39;</span><span class="si">{}</span><span class="s2">&#39; &quot;</span>
                        <span class="s2">&quot;(torch Tensor or None required)&quot;</span>
                        <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">tensor</span><span class="p">),</span> <span class="n">name</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_buffers</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">tensor</span>
        <span class="k">if</span> <span class="n">persistent</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_non_persistent_buffers_set</span><span class="o">.</span><span class="n">discard</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_non_persistent_buffers_set</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.register_forward_hook">
    <p>def <span class="ident">register_forward_hook</span>(</p><p>self, hook: Callable[..., NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Registers a forward hook on the module.</p>
<p>The hook will be called every time after :func:<code>forward</code> has computed an output.
It should have the following signature::</p>
<pre><code>hook(module, input, output) -&gt; None or modified output
</code></pre>
<p>The input contains only the positional arguments given to the module.
Keyword arguments won't be passed to the hooks and only to the <code>forward</code>.
The hook can modify the output. It can modify the input inplace but
it will not have effect on forward since this is called after
:func:<code>forward</code> is called.</p>
<p>Returns:
    :class:<code>torch.utils.hooks.RemovableHandle</code>:
        a handle that can be used to remove the added hook by calling
        <code>handle.remove()</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.register_forward_hook', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.register_forward_hook" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_forward_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a forward hook on the module.</span>
<span class="sd">    The hook will be called every time after :func:`forward` has computed an output.</span>
<span class="sd">    It should have the following signature::</span>
<span class="sd">        hook(module, input, output) -&gt; None or modified output</span>
<span class="sd">    The input contains only the positional arguments given to the module.</span>
<span class="sd">    Keyword arguments won&#39;t be passed to the hooks and only to the ``forward``.</span>
<span class="sd">    The hook can modify the output. It can modify the input inplace but</span>
<span class="sd">    it will not have effect on forward since this is called after</span>
<span class="sd">    :func:`forward` is called.</span>
<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.register_forward_pre_hook">
    <p>def <span class="ident">register_forward_pre_hook</span>(</p><p>self, hook: Callable[..., NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Registers a forward pre-hook on the module.</p>
<p>The hook will be called every time before :func:<code>forward</code> is invoked.
It should have the following signature::</p>
<pre><code>hook(module, input) -&gt; None or modified input
</code></pre>
<p>The input contains only the positional arguments given to the module.
Keyword arguments won't be passed to the hooks and only to the <code>forward</code>.
The hook can modify the input. User can either return a tuple or a
single modified value in the hook. We will wrap the value into a tuple
if a single value is returned(unless that value is already a tuple).</p>
<p>Returns:
    :class:<code>torch.utils.hooks.RemovableHandle</code>:
        a handle that can be used to remove the added hook by calling
        <code>handle.remove()</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.register_forward_pre_hook', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.register_forward_pre_hook" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_forward_pre_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a forward pre-hook on the module.</span>
<span class="sd">    The hook will be called every time before :func:`forward` is invoked.</span>
<span class="sd">    It should have the following signature::</span>
<span class="sd">        hook(module, input) -&gt; None or modified input</span>
<span class="sd">    The input contains only the positional arguments given to the module.</span>
<span class="sd">    Keyword arguments won&#39;t be passed to the hooks and only to the ``forward``.</span>
<span class="sd">    The hook can modify the input. User can either return a tuple or a</span>
<span class="sd">    single modified value in the hook. We will wrap the value into a tuple</span>
<span class="sd">    if a single value is returned(unless that value is already a tuple).</span>
<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.register_full_backward_hook">
    <p>def <span class="ident">register_full_backward_hook</span>(</p><p>self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]])</p>
    </div>
    

    
  
    <div class="desc"><p>Registers a backward hook on the module.</p>
<p>The hook will be called every time the gradients with respect to module
inputs are computed. The hook should have the following signature::</p>
<pre><code>hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None
</code></pre>
<p>The :attr:<code>grad_input</code> and :attr:<code>grad_output</code> are tuples that contain the gradients
with respect to the inputs and outputs respectively. The hook should
not modify its arguments, but it can optionally return a new gradient with
respect to the input that will be used in place of :attr:<code>grad_input</code> in
subsequent computations. :attr:<code>grad_input</code> will only correspond to the inputs given
as positional arguments and all kwarg arguments are ignored. Entries
in :attr:<code>grad_input</code> and :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor
arguments.</p>
<p>For technical reasons, when this hook is applied to a Module, its forward function will
receive a view of each Tensor passed to the Module. Similarly the caller will receive a view
of each Tensor returned by the Module's forward function.</p>
<p>.. warning ::
    Modifying inputs or outputs inplace is not allowed when using backward hooks and
    will raise an error.</p>
<p>Returns:
    :class:<code>torch.utils.hooks.RemovableHandle</code>:
        a handle that can be used to remove the added hook by calling
        <code>handle.remove()</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.register_full_backward_hook', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.register_full_backward_hook" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_full_backward_hook</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="s1">&#39;Module&#39;</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">],</span> <span class="n">Union</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a backward hook on the module.</span>
<span class="sd">    The hook will be called every time the gradients with respect to module</span>
<span class="sd">    inputs are computed. The hook should have the following signature::</span>
<span class="sd">        hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None</span>
<span class="sd">    The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients</span>
<span class="sd">    with respect to the inputs and outputs respectively. The hook should</span>
<span class="sd">    not modify its arguments, but it can optionally return a new gradient with</span>
<span class="sd">    respect to the input that will be used in place of :attr:`grad_input` in</span>
<span class="sd">    subsequent computations. :attr:`grad_input` will only correspond to the inputs given</span>
<span class="sd">    as positional arguments and all kwarg arguments are ignored. Entries</span>
<span class="sd">    in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor</span>
<span class="sd">    arguments.</span>
<span class="sd">    For technical reasons, when this hook is applied to a Module, its forward function will</span>
<span class="sd">    receive a view of each Tensor passed to the Module. Similarly the caller will receive a view</span>
<span class="sd">    of each Tensor returned by the Module&#39;s forward function.</span>
<span class="sd">    .. warning ::</span>
<span class="sd">        Modifying inputs or outputs inplace is not allowed when using backward hooks and</span>
<span class="sd">        will raise an error.</span>
<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_full_backward_hook</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Cannot use both regular backward hooks and full backward hooks on a &quot;</span>
                           <span class="s2">&quot;single Module. Please use only one of them.&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_is_full_backward_hook</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.register_parameter">
    <p>def <span class="ident">register_parameter</span>(</p><p>self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Adds a parameter to the module.</p>
<p>The parameter can be accessed as an attribute using given name.</p>
<p>Args:
    name (string): name of the parameter. The parameter can be accessed
        from this module using the given name
    param (Parameter or None): parameter to be added to the module. If
        <code>None</code>, then operations that run on parameters, such as :attr:<code>cuda</code>,
        are ignored. If <code>None</code>, the parameter is <strong>not</strong> included in the
        module's :attr:<code>state_dict</code>.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.register_parameter', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.register_parameter" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_parameter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">param</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Parameter</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Adds a parameter to the module.</span>
<span class="sd">    The parameter can be accessed as an attribute using given name.</span>
<span class="sd">    Args:</span>
<span class="sd">        name (string): name of the parameter. The parameter can be accessed</span>
<span class="sd">            from this module using the given name</span>
<span class="sd">        param (Parameter or None): parameter to be added to the module. If</span>
<span class="sd">            ``None``, then operations that run on parameters, such as :attr:`cuda`,</span>
<span class="sd">            are ignored. If ``None``, the parameter is **not** included in the</span>
<span class="sd">            module&#39;s :attr:`state_dict`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="s1">&#39;_parameters&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
            <span class="s2">&quot;cannot assign parameter before Module.__init__() call&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_six</span><span class="o">.</span><span class="n">string_classes</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;parameter name should be a string. &quot;</span>
                        <span class="s2">&quot;Got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">name</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="s1">&#39;.&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;parameter name can&#39;t contain </span><span class="se">\&quot;</span><span class="s2">.</span><span class="se">\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;parameter name can&#39;t be empty string </span><span class="se">\&quot;\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span> <span class="ow">and</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parameters</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;attribute &#39;</span><span class="si">{}</span><span class="s2">&#39; already exists&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">param</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_parameters</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;cannot assign &#39;</span><span class="si">{}</span><span class="s2">&#39; object to parameter &#39;</span><span class="si">{}</span><span class="s2">&#39; &quot;</span>
                        <span class="s2">&quot;(torch.nn.Parameter or None required)&quot;</span>
                        <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">param</span><span class="p">),</span> <span class="n">name</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">param</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Cannot assign non-leaf Tensor to parameter &#39;</span><span class="si">{0}</span><span class="s2">&#39;. Model &quot;</span>
            <span class="s2">&quot;parameters must be created explicitly. To express &#39;</span><span class="si">{0}</span><span class="s2">&#39; &quot;</span>
            <span class="s2">&quot;as a function of another Tensor, compute the value in &quot;</span>
            <span class="s2">&quot;the forward() method.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_parameters</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">param</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.requires_grad_">
    <p>def <span class="ident">requires_grad_</span>(</p><p>self: ~T, requires_grad: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Change if autograd should record operations on parameters in this
module.</p>
<p>This method sets the parameters' :attr:<code>requires_grad</code> attributes
in-place.</p>
<p>This method is helpful for freezing part of the module for finetuning
or training parts of a model individually (e.g., GAN training).</p>
<p>See :ref:<code>locally-disable-grad-doc</code> for a comparison between
<code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.</p>
<p>Args:
    requires_grad (bool): whether autograd should record operations on
                          parameters in this module. Default: <code>True</code>.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.requires_grad_', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.requires_grad_" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Change if autograd should record operations on parameters in this</span>
<span class="sd">    module.</span>
<span class="sd">    This method sets the parameters&#39; :attr:`requires_grad` attributes</span>
<span class="sd">    in-place.</span>
<span class="sd">    This method is helpful for freezing part of the module for finetuning</span>
<span class="sd">    or training parts of a model individually (e.g., GAN training).</span>
<span class="sd">    See :ref:`locally-disable-grad-doc` for a comparison between</span>
<span class="sd">    `.requires_grad_()` and several similar mechanisms that may be confused with it.</span>
<span class="sd">    Args:</span>
<span class="sd">        requires_grad (bool): whether autograd should record operations on</span>
<span class="sd">                              parameters in this module. Default: ``True``.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="n">p</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="n">requires_grad</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.set_extra_state">
    <p>def <span class="ident">set_extra_state</span>(</p><p>self, state: Any)</p>
    </div>
    

    
  
    <div class="desc"><p>This function is called from :func:<code>load_state_dict</code> to handle any extra state
found within the <code>state_dict</code>. Implement this function and a corresponding
:func:<code>get_extra_state</code> for your module if you need to store extra state within its
<code>state_dict</code>.</p>
<p>Args:
    state (dict): Extra state from the <code>state_dict</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.set_extra_state', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.set_extra_state" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">set_extra_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function is called from :func:`load_state_dict` to handle any extra state</span>
<span class="sd">    found within the `state_dict`. Implement this function and a corresponding</span>
<span class="sd">    :func:`get_extra_state` for your module if you need to store extra state within its</span>
<span class="sd">    `state_dict`.</span>
<span class="sd">    Args:</span>
<span class="sd">        state (dict): Extra state from the `state_dict`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
        <span class="s2">&quot;Reached a code path in Module.set_extra_state() that should never be called. &quot;</span>
        <span class="s2">&quot;Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md &quot;</span>
        <span class="s2">&quot;to report this bug.&quot;</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.share_memory">
    <p>def <span class="ident">share_memory</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>See :meth:<code>torch.Tensor.share_memory_</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.share_memory', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.share_memory" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">share_memory</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;See :meth:`torch.Tensor.share_memory_`&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">share_memory_</span><span class="p">())</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.state_dict">
    <p>def <span class="ident">state_dict</span>(</p><p>self, destination=None, prefix=&#39;&#39;, keep_vars=False)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns a dictionary containing a whole state of the module.</p>
<p>Both parameters and persistent buffers (e.g. running averages) are
included. Keys are corresponding parameter and buffer names.
Parameters and buffers set to <code>None</code> are not included.</p>
<p>Returns:
    dict:
        a dictionary containing a whole state of the module</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; module.state_dict().keys()
['bias', 'weight']
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.state_dict', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.state_dict" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">destination</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">keep_vars</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns a dictionary containing a whole state of the module.</span>
<span class="sd">    Both parameters and persistent buffers (e.g. running averages) are</span>
<span class="sd">    included. Keys are corresponding parameter and buffer names.</span>
<span class="sd">    Parameters and buffers set to ``None`` are not included.</span>
<span class="sd">    Returns:</span>
<span class="sd">        dict:</span>
<span class="sd">            a dictionary containing a whole state of the module</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; module.state_dict().keys()</span>
<span class="sd">        [&#39;bias&#39;, &#39;weight&#39;]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">destination</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">destination</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="n">destination</span><span class="o">.</span><span class="n">_metadata</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
    <span class="n">destination</span><span class="o">.</span><span class="n">_metadata</span><span class="p">[</span><span class="n">prefix</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">local_metadata</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">version</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_version</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_save_to_state_dict</span><span class="p">(</span><span class="n">destination</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">keep_vars</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(</span><span class="n">destination</span><span class="p">,</span> <span class="n">prefix</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">keep_vars</span><span class="o">=</span><span class="n">keep_vars</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">hook</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_dict_hooks</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
        <span class="n">hook_result</span> <span class="o">=</span> <span class="n">hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">destination</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">local_metadata</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">hook_result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">destination</span> <span class="o">=</span> <span class="n">hook_result</span>
    <span class="k">return</span> <span class="n">destination</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.to">
    <p>def <span class="ident">to</span>(</p><p>self, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>Moves and/or casts the parameters and buffers.</p>
<p>This can be called as</p>
<p>.. function:: to(device=None, dtype=None, non_blocking=False)
   :noindex:</p>
<p>.. function:: to(dtype, non_blocking=False)
   :noindex:</p>
<p>.. function:: to(tensor, non_blocking=False)
   :noindex:</p>
<p>.. function:: to(memory_format=torch.channels_last)
   :noindex:</p>
<p>Its signature is similar to :meth:<code>torch.Tensor.to</code>, but only accepts
floating point or complex :attr:<code>dtype</code>\ s. In addition, this method will
only cast the floating point or complex parameters and buffers to :attr:<code>dtype</code>
(if given). The integral parameters and buffers will be moved
:attr:<code>device</code>, if that is given, but with dtypes unchanged. When
:attr:<code>non_blocking</code> is set, it tries to convert/move asynchronously
with respect to the host if possible, e.g., moving CPU Tensors with
pinned memory to CUDA devices.</p>
<p>See below for examples.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Args:
    device (:class:<code>torch.device</code>): the desired device of the parameters
        and buffers in this module
    dtype (:class:<code>torch.dtype</code>): the desired floating point or complex dtype of
        the parameters and buffers in this module
    tensor (torch.Tensor): Tensor whose dtype and device are the desired
        dtype and device for all parameters and buffers in this module
    memory_format (:class:<code>torch.memory_format</code>): the desired memory
        format for 4D parameters and buffers in this module (keyword
        only argument)</p>
<p>Returns:
    Module: self</p>
<p>Examples::</p>
<pre><code>&gt;&gt;&gt; linear = nn.Linear(2, 2)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1913, -0.3420],
        [-0.5113, -0.2325]])
&gt;&gt;&gt; linear.to(torch.double)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1913, -0.3420],
        [-0.5113, -0.2325]], dtype=torch.float64)
&gt;&gt;&gt; gpu1 = torch.device("cuda:1")
&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1914, -0.3420],
        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')
&gt;&gt;&gt; cpu = torch.device("cpu")
&gt;&gt;&gt; linear.to(cpu)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1914, -0.3420],
        [-0.5112, -0.2324]], dtype=torch.float16)

&gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.3741+0.j,  0.2382+0.j],
        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)
&gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))
tensor([[0.6122+0.j, 0.1150+0.j],
        [0.6122+0.j, 0.1150+0.j],
        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.to', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.to" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves and/or casts the parameters and buffers.</span>
<span class="sd">    This can be called as</span>
<span class="sd">    .. function:: to(device=None, dtype=None, non_blocking=False)</span>
<span class="sd">       :noindex:</span>
<span class="sd">    .. function:: to(dtype, non_blocking=False)</span>
<span class="sd">       :noindex:</span>
<span class="sd">    .. function:: to(tensor, non_blocking=False)</span>
<span class="sd">       :noindex:</span>
<span class="sd">    .. function:: to(memory_format=torch.channels_last)</span>
<span class="sd">       :noindex:</span>
<span class="sd">    Its signature is similar to :meth:`torch.Tensor.to`, but only accepts</span>
<span class="sd">    floating point or complex :attr:`dtype`\ s. In addition, this method will</span>
<span class="sd">    only cast the floating point or complex parameters and buffers to :attr:`dtype`</span>
<span class="sd">    (if given). The integral parameters and buffers will be moved</span>
<span class="sd">    :attr:`device`, if that is given, but with dtypes unchanged. When</span>
<span class="sd">    :attr:`non_blocking` is set, it tries to convert/move asynchronously</span>
<span class="sd">    with respect to the host if possible, e.g., moving CPU Tensors with</span>
<span class="sd">    pinned memory to CUDA devices.</span>
<span class="sd">    See below for examples.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Args:</span>
<span class="sd">        device (:class:`torch.device`): the desired device of the parameters</span>
<span class="sd">            and buffers in this module</span>
<span class="sd">        dtype (:class:`torch.dtype`): the desired floating point or complex dtype of</span>
<span class="sd">            the parameters and buffers in this module</span>
<span class="sd">        tensor (torch.Tensor): Tensor whose dtype and device are the desired</span>
<span class="sd">            dtype and device for all parameters and buffers in this module</span>
<span class="sd">        memory_format (:class:`torch.memory_format`): the desired memory</span>
<span class="sd">            format for 4D parameters and buffers in this module (keyword</span>
<span class="sd">            only argument)</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    Examples::</span>
<span class="sd">        &gt;&gt;&gt; linear = nn.Linear(2, 2)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.1913, -0.3420],</span>
<span class="sd">                [-0.5113, -0.2325]])</span>
<span class="sd">        &gt;&gt;&gt; linear.to(torch.double)</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.1913, -0.3420],</span>
<span class="sd">                [-0.5113, -0.2325]], dtype=torch.float64)</span>
<span class="sd">        &gt;&gt;&gt; gpu1 = torch.device(&quot;cuda:1&quot;)</span>
<span class="sd">        &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.1914, -0.3420],</span>
<span class="sd">                [-0.5112, -0.2324]], dtype=torch.float16, device=&#39;cuda:1&#39;)</span>
<span class="sd">        &gt;&gt;&gt; cpu = torch.device(&quot;cpu&quot;)</span>
<span class="sd">        &gt;&gt;&gt; linear.to(cpu)</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.1914, -0.3420],</span>
<span class="sd">                [-0.5112, -0.2324]], dtype=torch.float16)</span>
<span class="sd">        &gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.3741+0.j,  0.2382+0.j],</span>
<span class="sd">                [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)</span>
<span class="sd">        &gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))</span>
<span class="sd">        tensor([[0.6122+0.j, 0.1150+0.j],</span>
<span class="sd">                [0.6122+0.j, 0.1150+0.j],</span>
<span class="sd">                [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">,</span> <span class="n">convert_to_format</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">_parse_to</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_floating_point</span> <span class="ow">or</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;nn.Module.to only accepts floating point or complex &#39;</span>
                            <span class="s1">&#39;dtypes, but got desired dtype=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dtype</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Complex modules are a new feature under active development whose design may change, &quot;</span>
                <span class="s2">&quot;and some modules might not work as expected when using complex tensors as parameters or buffers. &quot;</span>
                <span class="s2">&quot;Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md &quot;</span>
                <span class="s2">&quot;if a complex module does not work as expected.&quot;</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">convert</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">convert_to_format</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">t</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">t</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="ow">or</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
                        <span class="n">non_blocking</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">convert_to_format</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">t</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="ow">or</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="n">convert</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.to_empty">
    <p>def <span class="ident">to_empty</span>(</p><p>self: ~T, *, device: Union[str, torch.device])</p>
    </div>
    

    
  
    <div class="desc"><p>Moves the parameters and buffers to the specified device without copying storage.</p>
<p>Args:
    device (:class:<code>torch.device</code>): The desired device of the parameters
        and buffers in this module.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.to_empty', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.to_empty" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">to_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves the parameters and buffers to the specified device without copying storage.</span>
<span class="sd">    Args:</span>
<span class="sd">        device (:class:`torch.device`): The desired device of the parameters</span>
<span class="sd">            and buffers in this module.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">))</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.train">
    <p>def <span class="ident">train</span>(</p><p>self: ~T, mode: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Sets the module in training mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>,
etc.</p>
<p>Args:
    mode (bool): whether to set training mode (<code>True</code>) or evaluation
                 mode (<code>False</code>). Default: <code>True</code>.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.train', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.train" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets the module in training mode.</span>
<span class="sd">    This has any effect only on certain modules. See documentations of</span>
<span class="sd">    particular modules for details of their behaviors in training/evaluation</span>
<span class="sd">    mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,</span>
<span class="sd">    etc.</span>
<span class="sd">    Args:</span>
<span class="sd">        mode (bool): whether to set training mode (``True``) or evaluation</span>
<span class="sd">                     mode (``False``). Default: ``True``.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;training mode is expected to be boolean&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="o">=</span> <span class="n">mode</span>
    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">():</span>
        <span class="n">module</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.type">
    <p>def <span class="ident">type</span>(</p><p>self: ~T, dst_type: Union[torch.dtype, str])</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Args:
    dst_type (type or string): the desired type</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.type', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.type" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">type</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">dst_type</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all parameters and buffers to :attr:`dst_type`.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Args:</span>
<span class="sd">        dst_type (type or string): the desired type</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">dst_type</span><span class="p">))</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.xpu">
    <p>def <span class="ident">xpu</span>(</p><p>self: ~T, device: Union[int, torch.device, NoneType] = None)</p>
    </div>
    

    
  
    <div class="desc"><p>Moves all model parameters and buffers to the XPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on XPU while being optimized.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Arguments:
    device (int, optional): if specified, all parameters will be
        copied to that device</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.xpu', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.xpu" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">xpu</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves all model parameters and buffers to the XPU.</span>
<span class="sd">    This also makes associated parameters and buffers different objects. So</span>
<span class="sd">    it should be called before constructing optimizer if the module will</span>
<span class="sd">    live on XPU while being optimized.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Arguments:</span>
<span class="sd">        device (int, optional): if specified, all parameters will be</span>
<span class="sd">            copied to that device</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">xpu</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.EncoderLayer.zero_grad">
    <p>def <span class="ident">zero_grad</span>(</p><p>self, set_to_none: bool = False)</p>
    </div>
    

    
  
    <div class="desc"><p>Sets gradients of all model parameters to zero. See similar function
under :class:<code>torch.optim.Optimizer</code> for more context.</p>
<p>Args:
    set_to_none (bool): instead of setting to zero, set the grads to None.
        See :meth:<code>torch.optim.Optimizer.zero_grad</code> for details.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.EncoderLayer.zero_grad', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.EncoderLayer.zero_grad" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">set_to_none</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets gradients of all model parameters to zero. See similar function</span>
<span class="sd">    under :class:`torch.optim.Optimizer` for more context.</span>
<span class="sd">    Args:</span>
<span class="sd">        set_to_none (bool): instead of setting to zero, set the grads to None.</span>
<span class="sd">            See :meth:`torch.optim.Optimizer.zero_grad` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_is_replica&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;Calling .zero_grad() from a module created with nn.DataParallel() has no effect. &quot;</span>
            <span class="s2">&quot;The parameters are copied (in a differentiable manner) from the original module. &quot;</span>
            <span class="s2">&quot;This means they are not leaf nodes in autograd and so don&#39;t accumulate gradients. &quot;</span>
            <span class="s2">&quot;If you need gradients in your forward method, consider using autograd.grad instead.&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">set_to_none</span><span class="p">:</span>
                <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">grad_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">detach_</span><span class="p">()</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
                <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</pre></div>

  </div>
</div>

  </div>
  
      </div>
      </div>
      
      <div class="item">
      <p id="seq2seq.MultiHeadAttentionLayer" class="name">class <span class="ident">MultiHeadAttentionLayer</span></p>
      
  
    <div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p></div>
  <div class="source_cont">
</div>


      <div class="class">
          <h3>Ancestors (in MRO)</h3>
          <ul class="class_list">
          <li><a href="/torch.nn.modules.module.Module.ext">torch.nn.modules.module.Module</a></li>
          </ul>
          <h3>Class variables</h3>
            <div class="item">
            <p id="seq2seq.MultiHeadAttentionLayer.T_destination" class="name">var <span class="ident">T_destination</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="seq2seq.MultiHeadAttentionLayer.dump_patches" class="name">var <span class="ident">dump_patches</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
          <h3>Methods</h3>
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.add_module">
    <p>def <span class="ident">add_module</span>(</p><p>self, name: str, module: Union[ForwardRef(&#39;Module&#39;), NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Adds a child module to the current module.</p>
<p>The module can be accessed as an attribute using the given name.</p>
<p>Args:
    name (string): name of the child module. The child module can be
        accessed from this module using the given name
    module (Module): child module to be added to the module.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.add_module', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.add_module" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">add_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s1">&#39;Module&#39;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Adds a child module to the current module.</span>
<span class="sd">    The module can be accessed as an attribute using the given name.</span>
<span class="sd">    Args:</span>
<span class="sd">        name (string): name of the child module. The child module can be</span>
<span class="sd">            accessed from this module using the given name</span>
<span class="sd">        module (Module): child module to be added to the module.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">Module</span><span class="p">)</span> <span class="ow">and</span> <span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> is not a Module subclass&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">module</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_six</span><span class="o">.</span><span class="n">string_classes</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;module name should be a string. Got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">name</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span> <span class="ow">and</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;attribute &#39;</span><span class="si">{}</span><span class="s2">&#39; already exists&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">elif</span> <span class="s1">&#39;.&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;module name can&#39;t contain </span><span class="se">\&quot;</span><span class="s2">.</span><span class="se">\&quot;</span><span class="s2">, got: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;module name can&#39;t be empty string </span><span class="se">\&quot;\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.apply">
    <p>def <span class="ident">apply</span>(</p><p>self: ~T, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>)
as well as self. Typical use includes initializing the parameters of a model
(see also :ref:<code>nn-init-doc</code>).</p>
<p>Args:
    fn (:class:<code>Module</code> -&gt; None): function to be applied to each submodule</p>
<p>Returns:
    Module: self</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; @torch.no_grad()
&gt;&gt;&gt; def init_weights(m):
&gt;&gt;&gt;     print(m)
&gt;&gt;&gt;     if type(m) == nn.Linear:
&gt;&gt;&gt;         m.weight.fill_(1.0)
&gt;&gt;&gt;         print(m.weight)
&gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))
&gt;&gt;&gt; net.apply(init_weights)
Linear(in_features=2, out_features=2, bias=True)
Parameter containing:
tensor([[ 1.,  1.],
        [ 1.,  1.]])
Linear(in_features=2, out_features=2, bias=True)
Parameter containing:
tensor([[ 1.,  1.],
        [ 1.,  1.]])
Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.apply', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.apply" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="s1">&#39;Module&#39;</span><span class="p">],</span> <span class="kc">None</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies ``fn`` recursively to every submodule (as returned by ``.children()``)</span>
<span class="sd">    as well as self. Typical use includes initializing the parameters of a model</span>
<span class="sd">    (see also :ref:`nn-init-doc`).</span>
<span class="sd">    Args:</span>
<span class="sd">        fn (:class:`Module` -&gt; None): function to be applied to each submodule</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; @torch.no_grad()</span>
<span class="sd">        &gt;&gt;&gt; def init_weights(m):</span>
<span class="sd">        &gt;&gt;&gt;     print(m)</span>
<span class="sd">        &gt;&gt;&gt;     if type(m) == nn.Linear:</span>
<span class="sd">        &gt;&gt;&gt;         m.weight.fill_(1.0)</span>
<span class="sd">        &gt;&gt;&gt;         print(m.weight)</span>
<span class="sd">        &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))</span>
<span class="sd">        &gt;&gt;&gt; net.apply(init_weights)</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 1.,  1.],</span>
<span class="sd">                [ 1.,  1.]])</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 1.,  1.],</span>
<span class="sd">                [ 1.,  1.]])</span>
<span class="sd">        Sequential(</span>
<span class="sd">          (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">          (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        )</span>
<span class="sd">        Sequential(</span>
<span class="sd">          (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">          (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        )</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">():</span>
        <span class="n">module</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
    <span class="n">fn</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.bfloat16">
    <p>def <span class="ident">bfloat16</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.bfloat16', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.bfloat16" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">bfloat16</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all floating point parameters and buffers to ``bfloat16`` datatype.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">()</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="k">else</span> <span class="n">t</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.buffers">
    <p>def <span class="ident">buffers</span>(</p><p>self, recurse: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over module buffers.</p>
<p>Args:
    recurse (bool): if True, then yields buffers of this module
        and all submodules. Otherwise, yields only buffers that
        are direct members of this module.</p>
<p>Yields:
    torch.Tensor: module buffer</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for buf in model.buffers():
&gt;&gt;&gt;     print(type(buf), buf.size())
&lt;class 'torch.Tensor'&gt; (20L,)
&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.buffers', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.buffers" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over module buffers.</span>
<span class="sd">    Args:</span>
<span class="sd">        recurse (bool): if True, then yields buffers of this module</span>
<span class="sd">            and all submodules. Otherwise, yields only buffers that</span>
<span class="sd">            are direct members of this module.</span>
<span class="sd">    Yields:</span>
<span class="sd">        torch.Tensor: module buffer</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for buf in model.buffers():</span>
<span class="sd">        &gt;&gt;&gt;     print(type(buf), buf.size())</span>
<span class="sd">        &lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="sd">        &lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">buf</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">buf</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.children">
    <p>def <span class="ident">children</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over immediate children modules.</p>
<p>Yields:
    Module: a child module</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.children', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.children" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">children</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="s1">&#39;Module&#39;</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over immediate children modules.</span>
<span class="sd">    Yields:</span>
<span class="sd">        Module: a child module</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
        <span class="k">yield</span> <span class="n">module</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.cpu">
    <p>def <span class="ident">cpu</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Moves all model parameters and buffers to the CPU.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.cpu', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.cpu" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves all model parameters and buffers to the CPU.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.cuda">
    <p>def <span class="ident">cuda</span>(</p><p>self: ~T, device: Union[int, torch.device, NoneType] = None)</p>
    </div>
    

    
  
    <div class="desc"><p>Moves all model parameters and buffers to the GPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on GPU while being optimized.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Args:
    device (int, optional): if specified, all parameters will be
        copied to that device</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.cuda', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.cuda" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">cuda</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves all model parameters and buffers to the GPU.</span>
<span class="sd">    This also makes associated parameters and buffers different objects. So</span>
<span class="sd">    it should be called before constructing optimizer if the module will</span>
<span class="sd">    live on GPU while being optimized.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Args:</span>
<span class="sd">        device (int, optional): if specified, all parameters will be</span>
<span class="sd">            copied to that device</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.double">
    <p>def <span class="ident">double</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.double', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.double" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">double</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all floating point parameters and buffers to ``double`` datatype.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">double</span><span class="p">()</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="k">else</span> <span class="n">t</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.eval">
    <p>def <span class="ident">eval</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Sets the module in evaluation mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>,
etc.</p>
<p>This is equivalent with :meth:<code>self.train(False) &lt;torch.nn.Module.train&gt;</code>.</p>
<p>See :ref:<code>locally-disable-grad-doc</code> for a comparison between
<code>.eval()</code> and several similar mechanisms that may be confused with it.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.eval', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.eval" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets the module in evaluation mode.</span>
<span class="sd">    This has any effect only on certain modules. See documentations of</span>
<span class="sd">    particular modules for details of their behaviors in training/evaluation</span>
<span class="sd">    mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,</span>
<span class="sd">    etc.</span>
<span class="sd">    This is equivalent with :meth:`self.train(False) &lt;torch.nn.Module.train&gt;`.</span>
<span class="sd">    See :ref:`locally-disable-grad-doc` for a comparison between</span>
<span class="sd">    `.eval()` and several similar mechanisms that may be confused with it.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.extra_repr">
    <p>def <span class="ident">extra_repr</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.extra_repr', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.extra_repr" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Set the extra representation of the module</span>
<span class="sd">    To print customized extra information, you should re-implement</span>
<span class="sd">    this method in your own modules. Both single-line and multi-line</span>
<span class="sd">    strings are acceptable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="s1">&#39;&#39;</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.float">
    <p>def <span class="ident">float</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.float', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.float" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">float</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all floating point parameters and buffers to ``float`` datatype.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="k">else</span> <span class="n">t</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.forward">
    <p>def <span class="ident">forward</span>(</p><p>self, query, key, value, mask=None)</p>
    </div>
    

    
  
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.forward', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.forward" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="c1">#query = [batch size, query len, hid dim]</span>
    <span class="c1">#key = [batch size, key len, hid dim]</span>
    <span class="c1">#value = [batch size, value len, hid dim]</span>
            
    <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_q</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_k</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
    <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_v</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
    
    <span class="c1">#Q = [batch size, query len, hid dim]</span>
    <span class="c1">#K = [batch size, key len, hid dim]</span>
    <span class="c1">#V = [batch size, value len, hid dim]</span>
            
    <span class="n">Q</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    
    <span class="c1">#Q = [batch size, n heads, query len, head dim]</span>
    <span class="c1">#K = [batch size, n heads, key len, head dim]</span>
    <span class="c1">#V = [batch size, n heads, value len, head dim]</span>
            
    <span class="n">energy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>
    
    <span class="c1">#energy = [batch size, n heads, query len, key len]</span>
    
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">energy</span> <span class="o">=</span> <span class="n">energy</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e10</span><span class="p">)</span>
    
    <span class="n">attention</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">energy</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            
    <span class="c1">#attention = [batch size, n heads, query len, key len]</span>
            
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attention</span><span class="p">),</span> <span class="n">V</span><span class="p">)</span>
    
    <span class="c1">#x = [batch size, n heads, query len, head dim]</span>
    
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
    
    <span class="c1">#x = [batch size, query len, n heads, head dim]</span>
    
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hid_dim</span><span class="p">)</span>
    
    <span class="c1">#x = [batch size, query len, hid dim]</span>
    
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_o</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
    <span class="c1">#x = [batch size, query len, hid dim]</span>
    
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.get_buffer">
    <p>def <span class="ident">get_buffer</span>(</p><p>self, target: str)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the buffer given by <code>target</code> if it exists,
otherwise throws an error.</p>
<p>See the docstring for <code>get_submodule</code> for a more detailed
explanation of this method's functionality as well as how to
correctly specify <code>target</code>.</p>
<p>Args:
    target: The fully-qualified string name of the buffer
        to look for. (See <code>get_submodule</code> for how to specify a
        fully-qualified string.)</p>
<p>Returns:
    torch.Tensor: The buffer referenced by <code>target</code></p>
<p>Raises:
    AttributeError: If the target string references an invalid
        path or resolves to something that is not a
        buffer</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.get_buffer', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.get_buffer" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_buffer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the buffer given by ``target`` if it exists,</span>
<span class="sd">    otherwise throws an error.</span>
<span class="sd">    See the docstring for ``get_submodule`` for a more detailed</span>
<span class="sd">    explanation of this method&#39;s functionality as well as how to</span>
<span class="sd">    correctly specify ``target``.</span>
<span class="sd">    Args:</span>
<span class="sd">        target: The fully-qualified string name of the buffer</span>
<span class="sd">            to look for. (See ``get_submodule`` for how to specify a</span>
<span class="sd">            fully-qualified string.)</span>
<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: The buffer referenced by ``target``</span>
<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: If the target string references an invalid</span>
<span class="sd">            path or resolves to something that is not a</span>
<span class="sd">            buffer</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">module_path</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">buffer_name</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">rpartition</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
    <span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_submodule</span><span class="p">(</span><span class="n">module_path</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">buffer_name</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">_get_name</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot; has no attribute `&quot;</span>
                             <span class="o">+</span> <span class="n">buffer_name</span> <span class="o">+</span> <span class="s2">&quot;`&quot;</span><span class="p">)</span>
    <span class="n">buffer</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">buffer_name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">buffer_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">mod</span><span class="o">.</span><span class="n">_buffers</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s2">&quot;`&quot;</span> <span class="o">+</span> <span class="n">buffer_name</span> <span class="o">+</span> <span class="s2">&quot;` is not a buffer&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">buffer</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.get_extra_state">
    <p>def <span class="ident">get_extra_state</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns any extra state to include in the module's state_dict.
Implement this and a corresponding :func:<code>set_extra_state</code> for your module
if you need to store extra state. This function is called when building the
module's <code>state_dict()</code>.</p>
<p>Note that extra state should be pickleable to ensure working serialization
of the state_dict. We only provide provide backwards compatibility guarantees
for serializing Tensors; other objects may break backwards compatibility if
their serialized pickled form changes.</p>
<p>Returns:
    object: Any extra state to store in the module's state_dict</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.get_extra_state', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.get_extra_state" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_extra_state</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns any extra state to include in the module&#39;s state_dict.</span>
<span class="sd">    Implement this and a corresponding :func:`set_extra_state` for your module</span>
<span class="sd">    if you need to store extra state. This function is called when building the</span>
<span class="sd">    module&#39;s `state_dict()`.</span>
<span class="sd">    Note that extra state should be pickleable to ensure working serialization</span>
<span class="sd">    of the state_dict. We only provide provide backwards compatibility guarantees</span>
<span class="sd">    for serializing Tensors; other objects may break backwards compatibility if</span>
<span class="sd">    their serialized pickled form changes.</span>
<span class="sd">    Returns:</span>
<span class="sd">        object: Any extra state to store in the module&#39;s state_dict</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
        <span class="s2">&quot;Reached a code path in Module.get_extra_state() that should never be called. &quot;</span>
        <span class="s2">&quot;Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md &quot;</span>
        <span class="s2">&quot;to report this bug.&quot;</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.get_parameter">
    <p>def <span class="ident">get_parameter</span>(</p><p>self, target: str)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the parameter given by <code>target</code> if it exists,
otherwise throws an error.</p>
<p>See the docstring for <code>get_submodule</code> for a more detailed
explanation of this method's functionality as well as how to
correctly specify <code>target</code>.</p>
<p>Args:
    target: The fully-qualified string name of the Parameter
        to look for. (See <code>get_submodule</code> for how to specify a
        fully-qualified string.)</p>
<p>Returns:
    torch.nn.Parameter: The Parameter referenced by <code>target</code></p>
<p>Raises:
    AttributeError: If the target string references an invalid
        path or resolves to something that is not an
        <code>nn.Parameter</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.get_parameter', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.get_parameter" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_parameter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Parameter&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the parameter given by ``target`` if it exists,</span>
<span class="sd">    otherwise throws an error.</span>
<span class="sd">    See the docstring for ``get_submodule`` for a more detailed</span>
<span class="sd">    explanation of this method&#39;s functionality as well as how to</span>
<span class="sd">    correctly specify ``target``.</span>
<span class="sd">    Args:</span>
<span class="sd">        target: The fully-qualified string name of the Parameter</span>
<span class="sd">            to look for. (See ``get_submodule`` for how to specify a</span>
<span class="sd">            fully-qualified string.)</span>
<span class="sd">    Returns:</span>
<span class="sd">        torch.nn.Parameter: The Parameter referenced by ``target``</span>
<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: If the target string references an invalid</span>
<span class="sd">            path or resolves to something that is not an</span>
<span class="sd">            ``nn.Parameter``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">module_path</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">param_name</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">rpartition</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
    <span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_submodule</span><span class="p">(</span><span class="n">module_path</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">param_name</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">_get_name</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot; has no attribute `&quot;</span>
                             <span class="o">+</span> <span class="n">param_name</span> <span class="o">+</span> <span class="s2">&quot;`&quot;</span><span class="p">)</span>
    <span class="n">param</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">param_name</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s2">&quot;`&quot;</span> <span class="o">+</span> <span class="n">param_name</span> <span class="o">+</span> <span class="s2">&quot;` is not an &quot;</span>
                             <span class="s2">&quot;nn.Parameter&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">param</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.get_submodule">
    <p>def <span class="ident">get_submodule</span>(</p><p>self, target: str)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the submodule given by <code>target</code> if it exists,
otherwise throws an error.</p>
<p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that
looks like this:</p>
<p>.. code-block::text</p>
<pre><code>A(
    (net_b): Module(
        (net_c): Module(
            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))
        )
        (linear): Linear(in_features=100, out_features=200, bias=True)
    )
)
</code></pre>
<p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested
submodule <code>net_b</code>, which itself has two submodules <code>net_c</code>
and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p>
<p>To check whether or not we have the <code>linear</code> submodule, we
would call <code>get_submodule("net_b.linear")</code>. To check whether
we have the <code>conv</code> submodule, we would call
<code>get_submodule("net_b.net_c.conv")</code>.</p>
<p>The runtime of <code>get_submodule</code> is bounded by the degree
of module nesting in <code>target</code>. A query against
<code>named_modules</code> achieves the same result, but it is O(N) in
the number of transitive modules. So, for a simple check to see
if some submodule exists, <code>get_submodule</code> should always be
used.</p>
<p>Args:
    target: The fully-qualified string name of the submodule
        to look for. (See above example for how to specify a
        fully-qualified string.)</p>
<p>Returns:
    torch.nn.Module: The submodule referenced by <code>target</code></p>
<p>Raises:
    AttributeError: If the target string references an invalid
        path or resolves to something that is not an
        <code>nn.Module</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.get_submodule', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.get_submodule" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_submodule</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Module&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the submodule given by ``target`` if it exists,</span>
<span class="sd">    otherwise throws an error.</span>
<span class="sd">    For example, let&#39;s say you have an ``nn.Module`` ``A`` that</span>
<span class="sd">    looks like this:</span>
<span class="sd">    .. code-block::text</span>
<span class="sd">        A(</span>
<span class="sd">            (net_b): Module(</span>
<span class="sd">                (net_c): Module(</span>
<span class="sd">                    (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))</span>
<span class="sd">                )</span>
<span class="sd">                (linear): Linear(in_features=100, out_features=200, bias=True)</span>
<span class="sd">            )</span>
<span class="sd">        )</span>
<span class="sd">    (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested</span>
<span class="sd">    submodule ``net_b``, which itself has two submodules ``net_c``</span>
<span class="sd">    and ``linear``. ``net_c`` then has a submodule ``conv``.)</span>
<span class="sd">    To check whether or not we have the ``linear`` submodule, we</span>
<span class="sd">    would call ``get_submodule(&quot;net_b.linear&quot;)``. To check whether</span>
<span class="sd">    we have the ``conv`` submodule, we would call</span>
<span class="sd">    ``get_submodule(&quot;net_b.net_c.conv&quot;)``.</span>
<span class="sd">    The runtime of ``get_submodule`` is bounded by the degree</span>
<span class="sd">    of module nesting in ``target``. A query against</span>
<span class="sd">    ``named_modules`` achieves the same result, but it is O(N) in</span>
<span class="sd">    the number of transitive modules. So, for a simple check to see</span>
<span class="sd">    if some submodule exists, ``get_submodule`` should always be</span>
<span class="sd">    used.</span>
<span class="sd">    Args:</span>
<span class="sd">        target: The fully-qualified string name of the submodule</span>
<span class="sd">            to look for. (See above example for how to specify a</span>
<span class="sd">            fully-qualified string.)</span>
<span class="sd">    Returns:</span>
<span class="sd">        torch.nn.Module: The submodule referenced by ``target``</span>
<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: If the target string references an invalid</span>
<span class="sd">            path or resolves to something that is not an</span>
<span class="sd">            ``nn.Module``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">target</span> <span class="o">==</span> <span class="s2">&quot;&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span>
    <span class="n">atoms</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
    <span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="bp">self</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">atoms</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">_get_name</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot; has no &quot;</span>
                                 <span class="s2">&quot;attribute `&quot;</span> <span class="o">+</span> <span class="n">item</span> <span class="o">+</span> <span class="s2">&quot;`&quot;</span><span class="p">)</span>
        <span class="n">mod</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">item</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s2">&quot;`&quot;</span> <span class="o">+</span> <span class="n">item</span> <span class="o">+</span> <span class="s2">&quot;` is not &quot;</span>
                                 <span class="s2">&quot;an nn.Module&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mod</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.half">
    <p>def <span class="ident">half</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.half', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.half" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">half</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all floating point parameters and buffers to ``half`` datatype.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">half</span><span class="p">()</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="k">else</span> <span class="n">t</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.load_state_dict">
    <p>def <span class="ident">load_state_dict</span>(</p><p>self, state_dict: &#39;OrderedDict[str, Tensor]&#39;, strict: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Copies parameters and buffers from :attr:<code>state_dict</code> into
this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then
the keys of :attr:<code>state_dict</code> must exactly match the keys returned
by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.</p>
<p>Args:
    state_dict (dict): a dict containing parameters and
        persistent buffers.
    strict (bool, optional): whether to strictly enforce that the keys
        in :attr:<code>state_dict</code> match the keys returned by this module's
        :meth:<code>~torch.nn.Module.state_dict</code> function. Default: <code>True</code></p>
<p>Returns:
    <code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:
        * <strong>missing_keys</strong> is a list of str containing the missing keys
        * <strong>unexpected_keys</strong> is a list of str containing the unexpected keys</p>
<p>Note:
    If a parameter or buffer is registered as <code>None</code> and its corresponding key
    exists in :attr:<code>state_dict</code>, :meth:<code>load_state_dict</code> will raise a
    <code>RuntimeError</code>.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.load_state_dict', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.load_state_dict" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="s1">&#39;OrderedDict[str, Tensor]&#39;</span><span class="p">,</span>
                    <span class="n">strict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Copies parameters and buffers from :attr:`state_dict` into</span>
<span class="sd">    this module and its descendants. If :attr:`strict` is ``True``, then</span>
<span class="sd">    the keys of :attr:`state_dict` must exactly match the keys returned</span>
<span class="sd">    by this module&#39;s :meth:`~torch.nn.Module.state_dict` function.</span>
<span class="sd">    Args:</span>
<span class="sd">        state_dict (dict): a dict containing parameters and</span>
<span class="sd">            persistent buffers.</span>
<span class="sd">        strict (bool, optional): whether to strictly enforce that the keys</span>
<span class="sd">            in :attr:`state_dict` match the keys returned by this module&#39;s</span>
<span class="sd">            :meth:`~torch.nn.Module.state_dict` function. Default: ``True``</span>
<span class="sd">    Returns:</span>
<span class="sd">        ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:</span>
<span class="sd">            * **missing_keys** is a list of str containing the missing keys</span>
<span class="sd">            * **unexpected_keys** is a list of str containing the unexpected keys</span>
<span class="sd">    Note:</span>
<span class="sd">        If a parameter or buffer is registered as ``None`` and its corresponding key</span>
<span class="sd">        exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a</span>
<span class="sd">        ``RuntimeError``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">missing_keys</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">unexpected_keys</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">error_msgs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># copy state_dict so _load_from_state_dict can modify it</span>
    <span class="n">metadata</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="s1">&#39;_metadata&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># mypy isn&#39;t aware that &quot;_metadata&quot; exists in state_dict</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">_metadata</span> <span class="o">=</span> <span class="n">metadata</span>  <span class="c1"># type: ignore[attr-defined]</span>
    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
        <span class="n">local_metadata</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="n">metadata</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">metadata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">prefix</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">{})</span>
        <span class="n">module</span><span class="o">.</span><span class="n">_load_from_state_dict</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">local_metadata</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">missing_keys</span><span class="p">,</span> <span class="n">unexpected_keys</span><span class="p">,</span> <span class="n">error_msgs</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">child</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">load</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">prefix</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>
    <span class="n">load</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">load</span>
    <span class="k">if</span> <span class="n">strict</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">unexpected_keys</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">error_msgs</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span>
                <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Unexpected key(s) in state_dict: </span><span class="si">{}</span><span class="s1">. &#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;&quot;</span><span class="si">{}</span><span class="s1">&quot;&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">unexpected_keys</span><span class="p">)))</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">missing_keys</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">error_msgs</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span>
                <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Missing key(s) in state_dict: </span><span class="si">{}</span><span class="s1">. &#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;&quot;</span><span class="si">{}</span><span class="s1">&quot;&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">missing_keys</span><span class="p">)))</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">error_msgs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Error(s) in loading state_dict for </span><span class="si">{}</span><span class="s1">:</span><span class="se">\n\t</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                           <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n\t</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">error_msgs</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">_IncompatibleKeys</span><span class="p">(</span><span class="n">missing_keys</span><span class="p">,</span> <span class="n">unexpected_keys</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.modules">
    <p>def <span class="ident">modules</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over all modules in the network.</p>
<p>Yields:
    Module: a module in the network</p>
<p>Note:
    Duplicate modules are returned only once. In the following
    example, <code>l</code> will be returned only once.</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; l = nn.Linear(2, 2)
&gt;&gt;&gt; net = nn.Sequential(l, l)
&gt;&gt;&gt; for idx, m in enumerate(net.modules()):
        print(idx, '-&gt;', m)

0 -&gt; Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
1 -&gt; Linear(in_features=2, out_features=2, bias=True)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.modules', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.modules" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">modules</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="s1">&#39;Module&#39;</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over all modules in the network.</span>
<span class="sd">    Yields:</span>
<span class="sd">        Module: a module in the network</span>
<span class="sd">    Note:</span>
<span class="sd">        Duplicate modules are returned only once. In the following</span>
<span class="sd">        example, ``l`` will be returned only once.</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; l = nn.Linear(2, 2)</span>
<span class="sd">        &gt;&gt;&gt; net = nn.Sequential(l, l)</span>
<span class="sd">        &gt;&gt;&gt; for idx, m in enumerate(net.modules()):</span>
<span class="sd">                print(idx, &#39;-&gt;&#39;, m)</span>
<span class="sd">        0 -&gt; Sequential(</span>
<span class="sd">          (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">          (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        )</span>
<span class="sd">        1 -&gt; Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
        <span class="k">yield</span> <span class="n">module</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.named_buffers">
    <p>def <span class="ident">named_buffers</span>(</p><p>self, prefix: str = &#39;&#39;, recurse: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over module buffers, yielding both the
name of the buffer as well as the buffer itself.</p>
<p>Args:
    prefix (str): prefix to prepend to all buffer names.
    recurse (bool): if True, then yields buffers of this module
        and all submodules. Otherwise, yields only buffers that
        are direct members of this module.</p>
<p>Yields:
    (string, torch.Tensor): Tuple containing the name and buffer</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for name, buf in self.named_buffers():
&gt;&gt;&gt;    if name in ['running_var']:
&gt;&gt;&gt;        print(buf.size())
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.named_buffers', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.named_buffers" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">named_buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over module buffers, yielding both the</span>
<span class="sd">    name of the buffer as well as the buffer itself.</span>
<span class="sd">    Args:</span>
<span class="sd">        prefix (str): prefix to prepend to all buffer names.</span>
<span class="sd">        recurse (bool): if True, then yields buffers of this module</span>
<span class="sd">            and all submodules. Otherwise, yields only buffers that</span>
<span class="sd">            are direct members of this module.</span>
<span class="sd">    Yields:</span>
<span class="sd">        (string, torch.Tensor): Tuple containing the name and buffer</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for name, buf in self.named_buffers():</span>
<span class="sd">        &gt;&gt;&gt;    if name in [&#39;running_var&#39;]:</span>
<span class="sd">        &gt;&gt;&gt;        print(buf.size())</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">gen</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_named_members</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">module</span><span class="p">:</span> <span class="n">module</span><span class="o">.</span><span class="n">_buffers</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span>
        <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span> <span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">gen</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">elem</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.named_children">
    <p>def <span class="ident">named_children</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over immediate children modules, yielding both
the name of the module as well as the module itself.</p>
<p>Yields:
    (string, Module): Tuple containing a name and child module</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for name, module in model.named_children():
&gt;&gt;&gt;     if name in ['conv4', 'conv5']:
&gt;&gt;&gt;         print(module)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.named_children', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.named_children" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">named_children</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s1">&#39;Module&#39;</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over immediate children modules, yielding both</span>
<span class="sd">    the name of the module as well as the module itself.</span>
<span class="sd">    Yields:</span>
<span class="sd">        (string, Module): Tuple containing a name and child module</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for name, module in model.named_children():</span>
<span class="sd">        &gt;&gt;&gt;     if name in [&#39;conv4&#39;, &#39;conv5&#39;]:</span>
<span class="sd">        &gt;&gt;&gt;         print(module)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">memo</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">module</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">memo</span><span class="p">:</span>
            <span class="n">memo</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
            <span class="k">yield</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.named_modules">
    <p>def <span class="ident">named_modules</span>(</p><p>self, memo: Union[Set[ForwardRef(&#39;Module&#39;)], NoneType] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over all modules in the network, yielding
both the name of the module as well as the module itself.</p>
<p>Args:
    memo: a memo to store the set of modules already added to the result
    prefix: a prefix that will be added to the name of the module
    remove_duplicate: whether to remove the duplicated module instances in the result
    or not</p>
<p>Yields:
    (string, Module): Tuple of name and module</p>
<p>Note:
    Duplicate modules are returned only once. In the following
    example, <code>l</code> will be returned only once.</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; l = nn.Linear(2, 2)
&gt;&gt;&gt; net = nn.Sequential(l, l)
&gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):
        print(idx, '-&gt;', m)

0 -&gt; ('', Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
))
1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.named_modules', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.named_modules" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">named_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memo</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Set</span><span class="p">[</span><span class="s1">&#39;Module&#39;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">remove_duplicate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over all modules in the network, yielding</span>
<span class="sd">    both the name of the module as well as the module itself.</span>
<span class="sd">    Args:</span>
<span class="sd">        memo: a memo to store the set of modules already added to the result</span>
<span class="sd">        prefix: a prefix that will be added to the name of the module</span>
<span class="sd">        remove_duplicate: whether to remove the duplicated module instances in the result</span>
<span class="sd">        or not</span>
<span class="sd">    Yields:</span>
<span class="sd">        (string, Module): Tuple of name and module</span>
<span class="sd">    Note:</span>
<span class="sd">        Duplicate modules are returned only once. In the following</span>
<span class="sd">        example, ``l`` will be returned only once.</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; l = nn.Linear(2, 2)</span>
<span class="sd">        &gt;&gt;&gt; net = nn.Sequential(l, l)</span>
<span class="sd">        &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):</span>
<span class="sd">                print(idx, &#39;-&gt;&#39;, m)</span>
<span class="sd">        0 -&gt; (&#39;&#39;, Sequential(</span>
<span class="sd">          (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">          (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        ))</span>
<span class="sd">        1 -&gt; (&#39;0&#39;, Linear(in_features=2, out_features=2, bias=True))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">memo</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">memo</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">if</span> <span class="bp">self</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">memo</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">remove_duplicate</span><span class="p">:</span>
            <span class="n">memo</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">prefix</span><span class="p">,</span> <span class="bp">self</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">submodule_prefix</span> <span class="o">=</span> <span class="n">prefix</span> <span class="o">+</span> <span class="p">(</span><span class="s1">&#39;.&#39;</span> <span class="k">if</span> <span class="n">prefix</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span><span class="p">)</span> <span class="o">+</span> <span class="n">name</span>
            <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_modules</span><span class="p">(</span><span class="n">memo</span><span class="p">,</span> <span class="n">submodule_prefix</span><span class="p">,</span> <span class="n">remove_duplicate</span><span class="p">):</span>
                <span class="k">yield</span> <span class="n">m</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.named_parameters">
    <p>def <span class="ident">named_parameters</span>(</p><p>self, prefix: str = &#39;&#39;, recurse: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over module parameters, yielding both the
name of the parameter as well as the parameter itself.</p>
<p>Args:
    prefix (str): prefix to prepend to all parameter names.
    recurse (bool): if True, then yields parameters of this module
        and all submodules. Otherwise, yields only parameters that
        are direct members of this module.</p>
<p>Yields:
    (string, Parameter): Tuple containing the name and parameter</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for name, param in self.named_parameters():
&gt;&gt;&gt;    if name in ['bias']:
&gt;&gt;&gt;        print(param.size())
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.named_parameters', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.named_parameters" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">named_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over module parameters, yielding both the</span>
<span class="sd">    name of the parameter as well as the parameter itself.</span>
<span class="sd">    Args:</span>
<span class="sd">        prefix (str): prefix to prepend to all parameter names.</span>
<span class="sd">        recurse (bool): if True, then yields parameters of this module</span>
<span class="sd">            and all submodules. Otherwise, yields only parameters that</span>
<span class="sd">            are direct members of this module.</span>
<span class="sd">    Yields:</span>
<span class="sd">        (string, Parameter): Tuple containing the name and parameter</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for name, param in self.named_parameters():</span>
<span class="sd">        &gt;&gt;&gt;    if name in [&#39;bias&#39;]:</span>
<span class="sd">        &gt;&gt;&gt;        print(param.size())</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">gen</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_named_members</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">module</span><span class="p">:</span> <span class="n">module</span><span class="o">.</span><span class="n">_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span>
        <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span> <span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">gen</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">elem</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.parameters">
    <p>def <span class="ident">parameters</span>(</p><p>self, recurse: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over module parameters.</p>
<p>This is typically passed to an optimizer.</p>
<p>Args:
    recurse (bool): if True, then yields parameters of this module
        and all submodules. Otherwise, yields only parameters that
        are direct members of this module.</p>
<p>Yields:
    Parameter: module parameter</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for param in model.parameters():
&gt;&gt;&gt;     print(type(param), param.size())
&lt;class 'torch.Tensor'&gt; (20L,)
&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.parameters', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.parameters" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Parameter</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over module parameters.</span>
<span class="sd">    This is typically passed to an optimizer.</span>
<span class="sd">    Args:</span>
<span class="sd">        recurse (bool): if True, then yields parameters of this module</span>
<span class="sd">            and all submodules. Otherwise, yields only parameters that</span>
<span class="sd">            are direct members of this module.</span>
<span class="sd">    Yields:</span>
<span class="sd">        Parameter: module parameter</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for param in model.parameters():</span>
<span class="sd">        &gt;&gt;&gt;     print(type(param), param.size())</span>
<span class="sd">        &lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="sd">        &lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">param</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.register_backward_hook">
    <p>def <span class="ident">register_backward_hook</span>(</p><p>self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]])</p>
    </div>
    

    
  
    <div class="desc"><p>Registers a backward hook on the module.</p>
<p>This function is deprecated in favor of :meth:<code>~torch.nn.Module.register_full_backward_hook</code> and
the behavior of this function will change in future versions.</p>
<p>Returns:
    :class:<code>torch.utils.hooks.RemovableHandle</code>:
        a handle that can be used to remove the added hook by calling
        <code>handle.remove()</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.register_backward_hook', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.register_backward_hook" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_backward_hook</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="s1">&#39;Module&#39;</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">],</span> <span class="n">Union</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a backward hook on the module.</span>
<span class="sd">    This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and</span>
<span class="sd">    the behavior of this function will change in future versions.</span>
<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_full_backward_hook</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Cannot use both regular backward hooks and full backward hooks on a &quot;</span>
                           <span class="s2">&quot;single Module. Please use only one of them.&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_is_full_backward_hook</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.register_buffer">
    <p>def <span class="ident">register_buffer</span>(</p><p>self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Adds a buffer to the module.</p>
<p>This is typically used to register a buffer that should not to be
considered a model parameter. For example, BatchNorm's <code>running_mean</code>
is not a parameter, but is part of the module's state. Buffers, by
default, are persistent and will be saved alongside parameters. This
behavior can be changed by setting :attr:<code>persistent</code> to <code>False</code>. The
only difference between a persistent buffer and a non-persistent buffer
is that the latter will not be a part of this module's
:attr:<code>state_dict</code>.</p>
<p>Buffers can be accessed as attributes using given names.</p>
<p>Args:
    name (string): name of the buffer. The buffer can be accessed
        from this module using the given name
    tensor (Tensor or None): buffer to be registered. If <code>None</code>, then operations
        that run on buffers, such as :attr:<code>cuda</code>, are ignored. If <code>None</code>,
        the buffer is <strong>not</strong> included in the module's :attr:<code>state_dict</code>.
    persistent (bool): whether the buffer is part of this module's
        :attr:<code>state_dict</code>.</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.register_buffer', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.register_buffer" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_buffer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">persistent</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Adds a buffer to the module.</span>
<span class="sd">    This is typically used to register a buffer that should not to be</span>
<span class="sd">    considered a model parameter. For example, BatchNorm&#39;s ``running_mean``</span>
<span class="sd">    is not a parameter, but is part of the module&#39;s state. Buffers, by</span>
<span class="sd">    default, are persistent and will be saved alongside parameters. This</span>
<span class="sd">    behavior can be changed by setting :attr:`persistent` to ``False``. The</span>
<span class="sd">    only difference between a persistent buffer and a non-persistent buffer</span>
<span class="sd">    is that the latter will not be a part of this module&#39;s</span>
<span class="sd">    :attr:`state_dict`.</span>
<span class="sd">    Buffers can be accessed as attributes using given names.</span>
<span class="sd">    Args:</span>
<span class="sd">        name (string): name of the buffer. The buffer can be accessed</span>
<span class="sd">            from this module using the given name</span>
<span class="sd">        tensor (Tensor or None): buffer to be registered. If ``None``, then operations</span>
<span class="sd">            that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,</span>
<span class="sd">            the buffer is **not** included in the module&#39;s :attr:`state_dict`.</span>
<span class="sd">        persistent (bool): whether the buffer is part of this module&#39;s</span>
<span class="sd">            :attr:`state_dict`.</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; self.register_buffer(&#39;running_mean&#39;, torch.zeros(num_features))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">persistent</span> <span class="ow">is</span> <span class="kc">False</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ScriptModule</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;ScriptModule does not support non-persistent buffers&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="s1">&#39;_buffers&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
            <span class="s2">&quot;cannot assign buffer before Module.__init__() call&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_six</span><span class="o">.</span><span class="n">string_classes</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;buffer name should be a string. &quot;</span>
                        <span class="s2">&quot;Got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">name</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="s1">&#39;.&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;buffer name can&#39;t contain </span><span class="se">\&quot;</span><span class="s2">.</span><span class="se">\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;buffer name can&#39;t be empty string </span><span class="se">\&quot;\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span> <span class="ow">and</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffers</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;attribute &#39;</span><span class="si">{}</span><span class="s2">&#39; already exists&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;cannot assign &#39;</span><span class="si">{}</span><span class="s2">&#39; object to buffer &#39;</span><span class="si">{}</span><span class="s2">&#39; &quot;</span>
                        <span class="s2">&quot;(torch Tensor or None required)&quot;</span>
                        <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">tensor</span><span class="p">),</span> <span class="n">name</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_buffers</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">tensor</span>
        <span class="k">if</span> <span class="n">persistent</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_non_persistent_buffers_set</span><span class="o">.</span><span class="n">discard</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_non_persistent_buffers_set</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.register_forward_hook">
    <p>def <span class="ident">register_forward_hook</span>(</p><p>self, hook: Callable[..., NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Registers a forward hook on the module.</p>
<p>The hook will be called every time after :func:<code>forward</code> has computed an output.
It should have the following signature::</p>
<pre><code>hook(module, input, output) -&gt; None or modified output
</code></pre>
<p>The input contains only the positional arguments given to the module.
Keyword arguments won't be passed to the hooks and only to the <code>forward</code>.
The hook can modify the output. It can modify the input inplace but
it will not have effect on forward since this is called after
:func:<code>forward</code> is called.</p>
<p>Returns:
    :class:<code>torch.utils.hooks.RemovableHandle</code>:
        a handle that can be used to remove the added hook by calling
        <code>handle.remove()</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.register_forward_hook', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.register_forward_hook" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_forward_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a forward hook on the module.</span>
<span class="sd">    The hook will be called every time after :func:`forward` has computed an output.</span>
<span class="sd">    It should have the following signature::</span>
<span class="sd">        hook(module, input, output) -&gt; None or modified output</span>
<span class="sd">    The input contains only the positional arguments given to the module.</span>
<span class="sd">    Keyword arguments won&#39;t be passed to the hooks and only to the ``forward``.</span>
<span class="sd">    The hook can modify the output. It can modify the input inplace but</span>
<span class="sd">    it will not have effect on forward since this is called after</span>
<span class="sd">    :func:`forward` is called.</span>
<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.register_forward_pre_hook">
    <p>def <span class="ident">register_forward_pre_hook</span>(</p><p>self, hook: Callable[..., NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Registers a forward pre-hook on the module.</p>
<p>The hook will be called every time before :func:<code>forward</code> is invoked.
It should have the following signature::</p>
<pre><code>hook(module, input) -&gt; None or modified input
</code></pre>
<p>The input contains only the positional arguments given to the module.
Keyword arguments won't be passed to the hooks and only to the <code>forward</code>.
The hook can modify the input. User can either return a tuple or a
single modified value in the hook. We will wrap the value into a tuple
if a single value is returned(unless that value is already a tuple).</p>
<p>Returns:
    :class:<code>torch.utils.hooks.RemovableHandle</code>:
        a handle that can be used to remove the added hook by calling
        <code>handle.remove()</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.register_forward_pre_hook', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.register_forward_pre_hook" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_forward_pre_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a forward pre-hook on the module.</span>
<span class="sd">    The hook will be called every time before :func:`forward` is invoked.</span>
<span class="sd">    It should have the following signature::</span>
<span class="sd">        hook(module, input) -&gt; None or modified input</span>
<span class="sd">    The input contains only the positional arguments given to the module.</span>
<span class="sd">    Keyword arguments won&#39;t be passed to the hooks and only to the ``forward``.</span>
<span class="sd">    The hook can modify the input. User can either return a tuple or a</span>
<span class="sd">    single modified value in the hook. We will wrap the value into a tuple</span>
<span class="sd">    if a single value is returned(unless that value is already a tuple).</span>
<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.register_full_backward_hook">
    <p>def <span class="ident">register_full_backward_hook</span>(</p><p>self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]])</p>
    </div>
    

    
  
    <div class="desc"><p>Registers a backward hook on the module.</p>
<p>The hook will be called every time the gradients with respect to module
inputs are computed. The hook should have the following signature::</p>
<pre><code>hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None
</code></pre>
<p>The :attr:<code>grad_input</code> and :attr:<code>grad_output</code> are tuples that contain the gradients
with respect to the inputs and outputs respectively. The hook should
not modify its arguments, but it can optionally return a new gradient with
respect to the input that will be used in place of :attr:<code>grad_input</code> in
subsequent computations. :attr:<code>grad_input</code> will only correspond to the inputs given
as positional arguments and all kwarg arguments are ignored. Entries
in :attr:<code>grad_input</code> and :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor
arguments.</p>
<p>For technical reasons, when this hook is applied to a Module, its forward function will
receive a view of each Tensor passed to the Module. Similarly the caller will receive a view
of each Tensor returned by the Module's forward function.</p>
<p>.. warning ::
    Modifying inputs or outputs inplace is not allowed when using backward hooks and
    will raise an error.</p>
<p>Returns:
    :class:<code>torch.utils.hooks.RemovableHandle</code>:
        a handle that can be used to remove the added hook by calling
        <code>handle.remove()</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.register_full_backward_hook', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.register_full_backward_hook" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_full_backward_hook</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="s1">&#39;Module&#39;</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">],</span> <span class="n">Union</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a backward hook on the module.</span>
<span class="sd">    The hook will be called every time the gradients with respect to module</span>
<span class="sd">    inputs are computed. The hook should have the following signature::</span>
<span class="sd">        hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None</span>
<span class="sd">    The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients</span>
<span class="sd">    with respect to the inputs and outputs respectively. The hook should</span>
<span class="sd">    not modify its arguments, but it can optionally return a new gradient with</span>
<span class="sd">    respect to the input that will be used in place of :attr:`grad_input` in</span>
<span class="sd">    subsequent computations. :attr:`grad_input` will only correspond to the inputs given</span>
<span class="sd">    as positional arguments and all kwarg arguments are ignored. Entries</span>
<span class="sd">    in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor</span>
<span class="sd">    arguments.</span>
<span class="sd">    For technical reasons, when this hook is applied to a Module, its forward function will</span>
<span class="sd">    receive a view of each Tensor passed to the Module. Similarly the caller will receive a view</span>
<span class="sd">    of each Tensor returned by the Module&#39;s forward function.</span>
<span class="sd">    .. warning ::</span>
<span class="sd">        Modifying inputs or outputs inplace is not allowed when using backward hooks and</span>
<span class="sd">        will raise an error.</span>
<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_full_backward_hook</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Cannot use both regular backward hooks and full backward hooks on a &quot;</span>
                           <span class="s2">&quot;single Module. Please use only one of them.&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_is_full_backward_hook</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.register_parameter">
    <p>def <span class="ident">register_parameter</span>(</p><p>self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Adds a parameter to the module.</p>
<p>The parameter can be accessed as an attribute using given name.</p>
<p>Args:
    name (string): name of the parameter. The parameter can be accessed
        from this module using the given name
    param (Parameter or None): parameter to be added to the module. If
        <code>None</code>, then operations that run on parameters, such as :attr:<code>cuda</code>,
        are ignored. If <code>None</code>, the parameter is <strong>not</strong> included in the
        module's :attr:<code>state_dict</code>.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.register_parameter', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.register_parameter" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_parameter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">param</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Parameter</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Adds a parameter to the module.</span>
<span class="sd">    The parameter can be accessed as an attribute using given name.</span>
<span class="sd">    Args:</span>
<span class="sd">        name (string): name of the parameter. The parameter can be accessed</span>
<span class="sd">            from this module using the given name</span>
<span class="sd">        param (Parameter or None): parameter to be added to the module. If</span>
<span class="sd">            ``None``, then operations that run on parameters, such as :attr:`cuda`,</span>
<span class="sd">            are ignored. If ``None``, the parameter is **not** included in the</span>
<span class="sd">            module&#39;s :attr:`state_dict`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="s1">&#39;_parameters&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
            <span class="s2">&quot;cannot assign parameter before Module.__init__() call&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_six</span><span class="o">.</span><span class="n">string_classes</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;parameter name should be a string. &quot;</span>
                        <span class="s2">&quot;Got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">name</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="s1">&#39;.&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;parameter name can&#39;t contain </span><span class="se">\&quot;</span><span class="s2">.</span><span class="se">\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;parameter name can&#39;t be empty string </span><span class="se">\&quot;\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span> <span class="ow">and</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parameters</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;attribute &#39;</span><span class="si">{}</span><span class="s2">&#39; already exists&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">param</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_parameters</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;cannot assign &#39;</span><span class="si">{}</span><span class="s2">&#39; object to parameter &#39;</span><span class="si">{}</span><span class="s2">&#39; &quot;</span>
                        <span class="s2">&quot;(torch.nn.Parameter or None required)&quot;</span>
                        <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">param</span><span class="p">),</span> <span class="n">name</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">param</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Cannot assign non-leaf Tensor to parameter &#39;</span><span class="si">{0}</span><span class="s2">&#39;. Model &quot;</span>
            <span class="s2">&quot;parameters must be created explicitly. To express &#39;</span><span class="si">{0}</span><span class="s2">&#39; &quot;</span>
            <span class="s2">&quot;as a function of another Tensor, compute the value in &quot;</span>
            <span class="s2">&quot;the forward() method.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_parameters</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">param</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.requires_grad_">
    <p>def <span class="ident">requires_grad_</span>(</p><p>self: ~T, requires_grad: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Change if autograd should record operations on parameters in this
module.</p>
<p>This method sets the parameters' :attr:<code>requires_grad</code> attributes
in-place.</p>
<p>This method is helpful for freezing part of the module for finetuning
or training parts of a model individually (e.g., GAN training).</p>
<p>See :ref:<code>locally-disable-grad-doc</code> for a comparison between
<code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.</p>
<p>Args:
    requires_grad (bool): whether autograd should record operations on
                          parameters in this module. Default: <code>True</code>.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.requires_grad_', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.requires_grad_" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Change if autograd should record operations on parameters in this</span>
<span class="sd">    module.</span>
<span class="sd">    This method sets the parameters&#39; :attr:`requires_grad` attributes</span>
<span class="sd">    in-place.</span>
<span class="sd">    This method is helpful for freezing part of the module for finetuning</span>
<span class="sd">    or training parts of a model individually (e.g., GAN training).</span>
<span class="sd">    See :ref:`locally-disable-grad-doc` for a comparison between</span>
<span class="sd">    `.requires_grad_()` and several similar mechanisms that may be confused with it.</span>
<span class="sd">    Args:</span>
<span class="sd">        requires_grad (bool): whether autograd should record operations on</span>
<span class="sd">                              parameters in this module. Default: ``True``.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="n">p</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="n">requires_grad</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.set_extra_state">
    <p>def <span class="ident">set_extra_state</span>(</p><p>self, state: Any)</p>
    </div>
    

    
  
    <div class="desc"><p>This function is called from :func:<code>load_state_dict</code> to handle any extra state
found within the <code>state_dict</code>. Implement this function and a corresponding
:func:<code>get_extra_state</code> for your module if you need to store extra state within its
<code>state_dict</code>.</p>
<p>Args:
    state (dict): Extra state from the <code>state_dict</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.set_extra_state', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.set_extra_state" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">set_extra_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function is called from :func:`load_state_dict` to handle any extra state</span>
<span class="sd">    found within the `state_dict`. Implement this function and a corresponding</span>
<span class="sd">    :func:`get_extra_state` for your module if you need to store extra state within its</span>
<span class="sd">    `state_dict`.</span>
<span class="sd">    Args:</span>
<span class="sd">        state (dict): Extra state from the `state_dict`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
        <span class="s2">&quot;Reached a code path in Module.set_extra_state() that should never be called. &quot;</span>
        <span class="s2">&quot;Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md &quot;</span>
        <span class="s2">&quot;to report this bug.&quot;</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.share_memory">
    <p>def <span class="ident">share_memory</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>See :meth:<code>torch.Tensor.share_memory_</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.share_memory', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.share_memory" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">share_memory</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;See :meth:`torch.Tensor.share_memory_`&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">share_memory_</span><span class="p">())</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.state_dict">
    <p>def <span class="ident">state_dict</span>(</p><p>self, destination=None, prefix=&#39;&#39;, keep_vars=False)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns a dictionary containing a whole state of the module.</p>
<p>Both parameters and persistent buffers (e.g. running averages) are
included. Keys are corresponding parameter and buffer names.
Parameters and buffers set to <code>None</code> are not included.</p>
<p>Returns:
    dict:
        a dictionary containing a whole state of the module</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; module.state_dict().keys()
['bias', 'weight']
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.state_dict', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.state_dict" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">destination</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">keep_vars</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns a dictionary containing a whole state of the module.</span>
<span class="sd">    Both parameters and persistent buffers (e.g. running averages) are</span>
<span class="sd">    included. Keys are corresponding parameter and buffer names.</span>
<span class="sd">    Parameters and buffers set to ``None`` are not included.</span>
<span class="sd">    Returns:</span>
<span class="sd">        dict:</span>
<span class="sd">            a dictionary containing a whole state of the module</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; module.state_dict().keys()</span>
<span class="sd">        [&#39;bias&#39;, &#39;weight&#39;]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">destination</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">destination</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="n">destination</span><span class="o">.</span><span class="n">_metadata</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
    <span class="n">destination</span><span class="o">.</span><span class="n">_metadata</span><span class="p">[</span><span class="n">prefix</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">local_metadata</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">version</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_version</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_save_to_state_dict</span><span class="p">(</span><span class="n">destination</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">keep_vars</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(</span><span class="n">destination</span><span class="p">,</span> <span class="n">prefix</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">keep_vars</span><span class="o">=</span><span class="n">keep_vars</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">hook</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_dict_hooks</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
        <span class="n">hook_result</span> <span class="o">=</span> <span class="n">hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">destination</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">local_metadata</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">hook_result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">destination</span> <span class="o">=</span> <span class="n">hook_result</span>
    <span class="k">return</span> <span class="n">destination</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.to">
    <p>def <span class="ident">to</span>(</p><p>self, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>Moves and/or casts the parameters and buffers.</p>
<p>This can be called as</p>
<p>.. function:: to(device=None, dtype=None, non_blocking=False)
   :noindex:</p>
<p>.. function:: to(dtype, non_blocking=False)
   :noindex:</p>
<p>.. function:: to(tensor, non_blocking=False)
   :noindex:</p>
<p>.. function:: to(memory_format=torch.channels_last)
   :noindex:</p>
<p>Its signature is similar to :meth:<code>torch.Tensor.to</code>, but only accepts
floating point or complex :attr:<code>dtype</code>\ s. In addition, this method will
only cast the floating point or complex parameters and buffers to :attr:<code>dtype</code>
(if given). The integral parameters and buffers will be moved
:attr:<code>device</code>, if that is given, but with dtypes unchanged. When
:attr:<code>non_blocking</code> is set, it tries to convert/move asynchronously
with respect to the host if possible, e.g., moving CPU Tensors with
pinned memory to CUDA devices.</p>
<p>See below for examples.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Args:
    device (:class:<code>torch.device</code>): the desired device of the parameters
        and buffers in this module
    dtype (:class:<code>torch.dtype</code>): the desired floating point or complex dtype of
        the parameters and buffers in this module
    tensor (torch.Tensor): Tensor whose dtype and device are the desired
        dtype and device for all parameters and buffers in this module
    memory_format (:class:<code>torch.memory_format</code>): the desired memory
        format for 4D parameters and buffers in this module (keyword
        only argument)</p>
<p>Returns:
    Module: self</p>
<p>Examples::</p>
<pre><code>&gt;&gt;&gt; linear = nn.Linear(2, 2)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1913, -0.3420],
        [-0.5113, -0.2325]])
&gt;&gt;&gt; linear.to(torch.double)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1913, -0.3420],
        [-0.5113, -0.2325]], dtype=torch.float64)
&gt;&gt;&gt; gpu1 = torch.device("cuda:1")
&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1914, -0.3420],
        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')
&gt;&gt;&gt; cpu = torch.device("cpu")
&gt;&gt;&gt; linear.to(cpu)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1914, -0.3420],
        [-0.5112, -0.2324]], dtype=torch.float16)

&gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.3741+0.j,  0.2382+0.j],
        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)
&gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))
tensor([[0.6122+0.j, 0.1150+0.j],
        [0.6122+0.j, 0.1150+0.j],
        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.to', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.to" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves and/or casts the parameters and buffers.</span>
<span class="sd">    This can be called as</span>
<span class="sd">    .. function:: to(device=None, dtype=None, non_blocking=False)</span>
<span class="sd">       :noindex:</span>
<span class="sd">    .. function:: to(dtype, non_blocking=False)</span>
<span class="sd">       :noindex:</span>
<span class="sd">    .. function:: to(tensor, non_blocking=False)</span>
<span class="sd">       :noindex:</span>
<span class="sd">    .. function:: to(memory_format=torch.channels_last)</span>
<span class="sd">       :noindex:</span>
<span class="sd">    Its signature is similar to :meth:`torch.Tensor.to`, but only accepts</span>
<span class="sd">    floating point or complex :attr:`dtype`\ s. In addition, this method will</span>
<span class="sd">    only cast the floating point or complex parameters and buffers to :attr:`dtype`</span>
<span class="sd">    (if given). The integral parameters and buffers will be moved</span>
<span class="sd">    :attr:`device`, if that is given, but with dtypes unchanged. When</span>
<span class="sd">    :attr:`non_blocking` is set, it tries to convert/move asynchronously</span>
<span class="sd">    with respect to the host if possible, e.g., moving CPU Tensors with</span>
<span class="sd">    pinned memory to CUDA devices.</span>
<span class="sd">    See below for examples.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Args:</span>
<span class="sd">        device (:class:`torch.device`): the desired device of the parameters</span>
<span class="sd">            and buffers in this module</span>
<span class="sd">        dtype (:class:`torch.dtype`): the desired floating point or complex dtype of</span>
<span class="sd">            the parameters and buffers in this module</span>
<span class="sd">        tensor (torch.Tensor): Tensor whose dtype and device are the desired</span>
<span class="sd">            dtype and device for all parameters and buffers in this module</span>
<span class="sd">        memory_format (:class:`torch.memory_format`): the desired memory</span>
<span class="sd">            format for 4D parameters and buffers in this module (keyword</span>
<span class="sd">            only argument)</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    Examples::</span>
<span class="sd">        &gt;&gt;&gt; linear = nn.Linear(2, 2)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.1913, -0.3420],</span>
<span class="sd">                [-0.5113, -0.2325]])</span>
<span class="sd">        &gt;&gt;&gt; linear.to(torch.double)</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.1913, -0.3420],</span>
<span class="sd">                [-0.5113, -0.2325]], dtype=torch.float64)</span>
<span class="sd">        &gt;&gt;&gt; gpu1 = torch.device(&quot;cuda:1&quot;)</span>
<span class="sd">        &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.1914, -0.3420],</span>
<span class="sd">                [-0.5112, -0.2324]], dtype=torch.float16, device=&#39;cuda:1&#39;)</span>
<span class="sd">        &gt;&gt;&gt; cpu = torch.device(&quot;cpu&quot;)</span>
<span class="sd">        &gt;&gt;&gt; linear.to(cpu)</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.1914, -0.3420],</span>
<span class="sd">                [-0.5112, -0.2324]], dtype=torch.float16)</span>
<span class="sd">        &gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.3741+0.j,  0.2382+0.j],</span>
<span class="sd">                [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)</span>
<span class="sd">        &gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))</span>
<span class="sd">        tensor([[0.6122+0.j, 0.1150+0.j],</span>
<span class="sd">                [0.6122+0.j, 0.1150+0.j],</span>
<span class="sd">                [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">,</span> <span class="n">convert_to_format</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">_parse_to</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_floating_point</span> <span class="ow">or</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;nn.Module.to only accepts floating point or complex &#39;</span>
                            <span class="s1">&#39;dtypes, but got desired dtype=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dtype</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Complex modules are a new feature under active development whose design may change, &quot;</span>
                <span class="s2">&quot;and some modules might not work as expected when using complex tensors as parameters or buffers. &quot;</span>
                <span class="s2">&quot;Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md &quot;</span>
                <span class="s2">&quot;if a complex module does not work as expected.&quot;</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">convert</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">convert_to_format</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">t</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">t</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="ow">or</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
                        <span class="n">non_blocking</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">convert_to_format</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">t</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="ow">or</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="n">convert</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.to_empty">
    <p>def <span class="ident">to_empty</span>(</p><p>self: ~T, *, device: Union[str, torch.device])</p>
    </div>
    

    
  
    <div class="desc"><p>Moves the parameters and buffers to the specified device without copying storage.</p>
<p>Args:
    device (:class:<code>torch.device</code>): The desired device of the parameters
        and buffers in this module.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.to_empty', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.to_empty" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">to_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves the parameters and buffers to the specified device without copying storage.</span>
<span class="sd">    Args:</span>
<span class="sd">        device (:class:`torch.device`): The desired device of the parameters</span>
<span class="sd">            and buffers in this module.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">))</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.train">
    <p>def <span class="ident">train</span>(</p><p>self: ~T, mode: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Sets the module in training mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>,
etc.</p>
<p>Args:
    mode (bool): whether to set training mode (<code>True</code>) or evaluation
                 mode (<code>False</code>). Default: <code>True</code>.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.train', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.train" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets the module in training mode.</span>
<span class="sd">    This has any effect only on certain modules. See documentations of</span>
<span class="sd">    particular modules for details of their behaviors in training/evaluation</span>
<span class="sd">    mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,</span>
<span class="sd">    etc.</span>
<span class="sd">    Args:</span>
<span class="sd">        mode (bool): whether to set training mode (``True``) or evaluation</span>
<span class="sd">                     mode (``False``). Default: ``True``.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;training mode is expected to be boolean&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="o">=</span> <span class="n">mode</span>
    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">():</span>
        <span class="n">module</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.type">
    <p>def <span class="ident">type</span>(</p><p>self: ~T, dst_type: Union[torch.dtype, str])</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Args:
    dst_type (type or string): the desired type</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.type', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.type" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">type</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">dst_type</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all parameters and buffers to :attr:`dst_type`.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Args:</span>
<span class="sd">        dst_type (type or string): the desired type</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">dst_type</span><span class="p">))</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.xpu">
    <p>def <span class="ident">xpu</span>(</p><p>self: ~T, device: Union[int, torch.device, NoneType] = None)</p>
    </div>
    

    
  
    <div class="desc"><p>Moves all model parameters and buffers to the XPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on XPU while being optimized.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Arguments:
    device (int, optional): if specified, all parameters will be
        copied to that device</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.xpu', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.xpu" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">xpu</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves all model parameters and buffers to the XPU.</span>
<span class="sd">    This also makes associated parameters and buffers different objects. So</span>
<span class="sd">    it should be called before constructing optimizer if the module will</span>
<span class="sd">    live on XPU while being optimized.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Arguments:</span>
<span class="sd">        device (int, optional): if specified, all parameters will be</span>
<span class="sd">            copied to that device</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">xpu</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.MultiHeadAttentionLayer.zero_grad">
    <p>def <span class="ident">zero_grad</span>(</p><p>self, set_to_none: bool = False)</p>
    </div>
    

    
  
    <div class="desc"><p>Sets gradients of all model parameters to zero. See similar function
under :class:<code>torch.optim.Optimizer</code> for more context.</p>
<p>Args:
    set_to_none (bool): instead of setting to zero, set the grads to None.
        See :meth:<code>torch.optim.Optimizer.zero_grad</code> for details.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.MultiHeadAttentionLayer.zero_grad', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.MultiHeadAttentionLayer.zero_grad" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">set_to_none</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets gradients of all model parameters to zero. See similar function</span>
<span class="sd">    under :class:`torch.optim.Optimizer` for more context.</span>
<span class="sd">    Args:</span>
<span class="sd">        set_to_none (bool): instead of setting to zero, set the grads to None.</span>
<span class="sd">            See :meth:`torch.optim.Optimizer.zero_grad` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_is_replica&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;Calling .zero_grad() from a module created with nn.DataParallel() has no effect. &quot;</span>
            <span class="s2">&quot;The parameters are copied (in a differentiable manner) from the original module. &quot;</span>
            <span class="s2">&quot;This means they are not leaf nodes in autograd and so don&#39;t accumulate gradients. &quot;</span>
            <span class="s2">&quot;If you need gradients in your forward method, consider using autograd.grad instead.&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">set_to_none</span><span class="p">:</span>
                <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">grad_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">detach_</span><span class="p">()</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
                <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</pre></div>

  </div>
</div>

  </div>
  
      </div>
      </div>
      
      <div class="item">
      <p id="seq2seq.PositionwiseFeedforwardLayer" class="name">class <span class="ident">PositionwiseFeedforwardLayer</span></p>
      
  
    <div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p></div>
  <div class="source_cont">
</div>


      <div class="class">
          <h3>Ancestors (in MRO)</h3>
          <ul class="class_list">
          <li><a href="/torch.nn.modules.module.Module.ext">torch.nn.modules.module.Module</a></li>
          </ul>
          <h3>Class variables</h3>
            <div class="item">
            <p id="seq2seq.PositionwiseFeedforwardLayer.T_destination" class="name">var <span class="ident">T_destination</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="seq2seq.PositionwiseFeedforwardLayer.dump_patches" class="name">var <span class="ident">dump_patches</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
          <h3>Methods</h3>
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.add_module">
    <p>def <span class="ident">add_module</span>(</p><p>self, name: str, module: Union[ForwardRef(&#39;Module&#39;), NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Adds a child module to the current module.</p>
<p>The module can be accessed as an attribute using the given name.</p>
<p>Args:
    name (string): name of the child module. The child module can be
        accessed from this module using the given name
    module (Module): child module to be added to the module.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.add_module', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.add_module" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">add_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s1">&#39;Module&#39;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Adds a child module to the current module.</span>
<span class="sd">    The module can be accessed as an attribute using the given name.</span>
<span class="sd">    Args:</span>
<span class="sd">        name (string): name of the child module. The child module can be</span>
<span class="sd">            accessed from this module using the given name</span>
<span class="sd">        module (Module): child module to be added to the module.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">Module</span><span class="p">)</span> <span class="ow">and</span> <span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> is not a Module subclass&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">module</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_six</span><span class="o">.</span><span class="n">string_classes</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;module name should be a string. Got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">name</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span> <span class="ow">and</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;attribute &#39;</span><span class="si">{}</span><span class="s2">&#39; already exists&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">elif</span> <span class="s1">&#39;.&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;module name can&#39;t contain </span><span class="se">\&quot;</span><span class="s2">.</span><span class="se">\&quot;</span><span class="s2">, got: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;module name can&#39;t be empty string </span><span class="se">\&quot;\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.apply">
    <p>def <span class="ident">apply</span>(</p><p>self: ~T, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>)
as well as self. Typical use includes initializing the parameters of a model
(see also :ref:<code>nn-init-doc</code>).</p>
<p>Args:
    fn (:class:<code>Module</code> -&gt; None): function to be applied to each submodule</p>
<p>Returns:
    Module: self</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; @torch.no_grad()
&gt;&gt;&gt; def init_weights(m):
&gt;&gt;&gt;     print(m)
&gt;&gt;&gt;     if type(m) == nn.Linear:
&gt;&gt;&gt;         m.weight.fill_(1.0)
&gt;&gt;&gt;         print(m.weight)
&gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))
&gt;&gt;&gt; net.apply(init_weights)
Linear(in_features=2, out_features=2, bias=True)
Parameter containing:
tensor([[ 1.,  1.],
        [ 1.,  1.]])
Linear(in_features=2, out_features=2, bias=True)
Parameter containing:
tensor([[ 1.,  1.],
        [ 1.,  1.]])
Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.apply', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.apply" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="s1">&#39;Module&#39;</span><span class="p">],</span> <span class="kc">None</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies ``fn`` recursively to every submodule (as returned by ``.children()``)</span>
<span class="sd">    as well as self. Typical use includes initializing the parameters of a model</span>
<span class="sd">    (see also :ref:`nn-init-doc`).</span>
<span class="sd">    Args:</span>
<span class="sd">        fn (:class:`Module` -&gt; None): function to be applied to each submodule</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; @torch.no_grad()</span>
<span class="sd">        &gt;&gt;&gt; def init_weights(m):</span>
<span class="sd">        &gt;&gt;&gt;     print(m)</span>
<span class="sd">        &gt;&gt;&gt;     if type(m) == nn.Linear:</span>
<span class="sd">        &gt;&gt;&gt;         m.weight.fill_(1.0)</span>
<span class="sd">        &gt;&gt;&gt;         print(m.weight)</span>
<span class="sd">        &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))</span>
<span class="sd">        &gt;&gt;&gt; net.apply(init_weights)</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 1.,  1.],</span>
<span class="sd">                [ 1.,  1.]])</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 1.,  1.],</span>
<span class="sd">                [ 1.,  1.]])</span>
<span class="sd">        Sequential(</span>
<span class="sd">          (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">          (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        )</span>
<span class="sd">        Sequential(</span>
<span class="sd">          (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">          (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        )</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">():</span>
        <span class="n">module</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
    <span class="n">fn</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.bfloat16">
    <p>def <span class="ident">bfloat16</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.bfloat16', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.bfloat16" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">bfloat16</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all floating point parameters and buffers to ``bfloat16`` datatype.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">()</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="k">else</span> <span class="n">t</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.buffers">
    <p>def <span class="ident">buffers</span>(</p><p>self, recurse: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over module buffers.</p>
<p>Args:
    recurse (bool): if True, then yields buffers of this module
        and all submodules. Otherwise, yields only buffers that
        are direct members of this module.</p>
<p>Yields:
    torch.Tensor: module buffer</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for buf in model.buffers():
&gt;&gt;&gt;     print(type(buf), buf.size())
&lt;class 'torch.Tensor'&gt; (20L,)
&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.buffers', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.buffers" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over module buffers.</span>
<span class="sd">    Args:</span>
<span class="sd">        recurse (bool): if True, then yields buffers of this module</span>
<span class="sd">            and all submodules. Otherwise, yields only buffers that</span>
<span class="sd">            are direct members of this module.</span>
<span class="sd">    Yields:</span>
<span class="sd">        torch.Tensor: module buffer</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for buf in model.buffers():</span>
<span class="sd">        &gt;&gt;&gt;     print(type(buf), buf.size())</span>
<span class="sd">        &lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="sd">        &lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">buf</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">buf</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.children">
    <p>def <span class="ident">children</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over immediate children modules.</p>
<p>Yields:
    Module: a child module</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.children', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.children" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">children</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="s1">&#39;Module&#39;</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over immediate children modules.</span>
<span class="sd">    Yields:</span>
<span class="sd">        Module: a child module</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
        <span class="k">yield</span> <span class="n">module</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.cpu">
    <p>def <span class="ident">cpu</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Moves all model parameters and buffers to the CPU.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.cpu', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.cpu" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves all model parameters and buffers to the CPU.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.cuda">
    <p>def <span class="ident">cuda</span>(</p><p>self: ~T, device: Union[int, torch.device, NoneType] = None)</p>
    </div>
    

    
  
    <div class="desc"><p>Moves all model parameters and buffers to the GPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on GPU while being optimized.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Args:
    device (int, optional): if specified, all parameters will be
        copied to that device</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.cuda', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.cuda" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">cuda</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves all model parameters and buffers to the GPU.</span>
<span class="sd">    This also makes associated parameters and buffers different objects. So</span>
<span class="sd">    it should be called before constructing optimizer if the module will</span>
<span class="sd">    live on GPU while being optimized.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Args:</span>
<span class="sd">        device (int, optional): if specified, all parameters will be</span>
<span class="sd">            copied to that device</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.double">
    <p>def <span class="ident">double</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.double', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.double" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">double</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all floating point parameters and buffers to ``double`` datatype.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">double</span><span class="p">()</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="k">else</span> <span class="n">t</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.eval">
    <p>def <span class="ident">eval</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Sets the module in evaluation mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>,
etc.</p>
<p>This is equivalent with :meth:<code>self.train(False) &lt;torch.nn.Module.train&gt;</code>.</p>
<p>See :ref:<code>locally-disable-grad-doc</code> for a comparison between
<code>.eval()</code> and several similar mechanisms that may be confused with it.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.eval', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.eval" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets the module in evaluation mode.</span>
<span class="sd">    This has any effect only on certain modules. See documentations of</span>
<span class="sd">    particular modules for details of their behaviors in training/evaluation</span>
<span class="sd">    mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,</span>
<span class="sd">    etc.</span>
<span class="sd">    This is equivalent with :meth:`self.train(False) &lt;torch.nn.Module.train&gt;`.</span>
<span class="sd">    See :ref:`locally-disable-grad-doc` for a comparison between</span>
<span class="sd">    `.eval()` and several similar mechanisms that may be confused with it.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.extra_repr">
    <p>def <span class="ident">extra_repr</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.extra_repr', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.extra_repr" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Set the extra representation of the module</span>
<span class="sd">    To print customized extra information, you should re-implement</span>
<span class="sd">    this method in your own modules. Both single-line and multi-line</span>
<span class="sd">    strings are acceptable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="s1">&#39;&#39;</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.float">
    <p>def <span class="ident">float</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.float', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.float" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">float</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all floating point parameters and buffers to ``float`` datatype.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="k">else</span> <span class="n">t</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.forward">
    <p>def <span class="ident">forward</span>(</p><p>self, x)</p>
    </div>
    

    
  
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.forward', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.forward" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    
    <span class="c1">#x = [batch size, seq len, hid dim]</span>
    
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc_1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
    
    <span class="c1">#x = [batch size, seq len, pf dim]</span>
    
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
    <span class="c1">#x = [batch size, seq len, hid dim]</span>
    
    <span class="k">return</span> <span class="n">x</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.get_buffer">
    <p>def <span class="ident">get_buffer</span>(</p><p>self, target: str)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the buffer given by <code>target</code> if it exists,
otherwise throws an error.</p>
<p>See the docstring for <code>get_submodule</code> for a more detailed
explanation of this method's functionality as well as how to
correctly specify <code>target</code>.</p>
<p>Args:
    target: The fully-qualified string name of the buffer
        to look for. (See <code>get_submodule</code> for how to specify a
        fully-qualified string.)</p>
<p>Returns:
    torch.Tensor: The buffer referenced by <code>target</code></p>
<p>Raises:
    AttributeError: If the target string references an invalid
        path or resolves to something that is not a
        buffer</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.get_buffer', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.get_buffer" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_buffer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the buffer given by ``target`` if it exists,</span>
<span class="sd">    otherwise throws an error.</span>
<span class="sd">    See the docstring for ``get_submodule`` for a more detailed</span>
<span class="sd">    explanation of this method&#39;s functionality as well as how to</span>
<span class="sd">    correctly specify ``target``.</span>
<span class="sd">    Args:</span>
<span class="sd">        target: The fully-qualified string name of the buffer</span>
<span class="sd">            to look for. (See ``get_submodule`` for how to specify a</span>
<span class="sd">            fully-qualified string.)</span>
<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: The buffer referenced by ``target``</span>
<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: If the target string references an invalid</span>
<span class="sd">            path or resolves to something that is not a</span>
<span class="sd">            buffer</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">module_path</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">buffer_name</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">rpartition</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
    <span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_submodule</span><span class="p">(</span><span class="n">module_path</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">buffer_name</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">_get_name</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot; has no attribute `&quot;</span>
                             <span class="o">+</span> <span class="n">buffer_name</span> <span class="o">+</span> <span class="s2">&quot;`&quot;</span><span class="p">)</span>
    <span class="n">buffer</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">buffer_name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">buffer_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">mod</span><span class="o">.</span><span class="n">_buffers</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s2">&quot;`&quot;</span> <span class="o">+</span> <span class="n">buffer_name</span> <span class="o">+</span> <span class="s2">&quot;` is not a buffer&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">buffer</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.get_extra_state">
    <p>def <span class="ident">get_extra_state</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns any extra state to include in the module's state_dict.
Implement this and a corresponding :func:<code>set_extra_state</code> for your module
if you need to store extra state. This function is called when building the
module's <code>state_dict()</code>.</p>
<p>Note that extra state should be pickleable to ensure working serialization
of the state_dict. We only provide provide backwards compatibility guarantees
for serializing Tensors; other objects may break backwards compatibility if
their serialized pickled form changes.</p>
<p>Returns:
    object: Any extra state to store in the module's state_dict</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.get_extra_state', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.get_extra_state" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_extra_state</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns any extra state to include in the module&#39;s state_dict.</span>
<span class="sd">    Implement this and a corresponding :func:`set_extra_state` for your module</span>
<span class="sd">    if you need to store extra state. This function is called when building the</span>
<span class="sd">    module&#39;s `state_dict()`.</span>
<span class="sd">    Note that extra state should be pickleable to ensure working serialization</span>
<span class="sd">    of the state_dict. We only provide provide backwards compatibility guarantees</span>
<span class="sd">    for serializing Tensors; other objects may break backwards compatibility if</span>
<span class="sd">    their serialized pickled form changes.</span>
<span class="sd">    Returns:</span>
<span class="sd">        object: Any extra state to store in the module&#39;s state_dict</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
        <span class="s2">&quot;Reached a code path in Module.get_extra_state() that should never be called. &quot;</span>
        <span class="s2">&quot;Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md &quot;</span>
        <span class="s2">&quot;to report this bug.&quot;</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.get_parameter">
    <p>def <span class="ident">get_parameter</span>(</p><p>self, target: str)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the parameter given by <code>target</code> if it exists,
otherwise throws an error.</p>
<p>See the docstring for <code>get_submodule</code> for a more detailed
explanation of this method's functionality as well as how to
correctly specify <code>target</code>.</p>
<p>Args:
    target: The fully-qualified string name of the Parameter
        to look for. (See <code>get_submodule</code> for how to specify a
        fully-qualified string.)</p>
<p>Returns:
    torch.nn.Parameter: The Parameter referenced by <code>target</code></p>
<p>Raises:
    AttributeError: If the target string references an invalid
        path or resolves to something that is not an
        <code>nn.Parameter</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.get_parameter', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.get_parameter" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_parameter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Parameter&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the parameter given by ``target`` if it exists,</span>
<span class="sd">    otherwise throws an error.</span>
<span class="sd">    See the docstring for ``get_submodule`` for a more detailed</span>
<span class="sd">    explanation of this method&#39;s functionality as well as how to</span>
<span class="sd">    correctly specify ``target``.</span>
<span class="sd">    Args:</span>
<span class="sd">        target: The fully-qualified string name of the Parameter</span>
<span class="sd">            to look for. (See ``get_submodule`` for how to specify a</span>
<span class="sd">            fully-qualified string.)</span>
<span class="sd">    Returns:</span>
<span class="sd">        torch.nn.Parameter: The Parameter referenced by ``target``</span>
<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: If the target string references an invalid</span>
<span class="sd">            path or resolves to something that is not an</span>
<span class="sd">            ``nn.Parameter``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">module_path</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">param_name</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">rpartition</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
    <span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_submodule</span><span class="p">(</span><span class="n">module_path</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">param_name</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">_get_name</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot; has no attribute `&quot;</span>
                             <span class="o">+</span> <span class="n">param_name</span> <span class="o">+</span> <span class="s2">&quot;`&quot;</span><span class="p">)</span>
    <span class="n">param</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">param_name</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s2">&quot;`&quot;</span> <span class="o">+</span> <span class="n">param_name</span> <span class="o">+</span> <span class="s2">&quot;` is not an &quot;</span>
                             <span class="s2">&quot;nn.Parameter&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">param</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.get_submodule">
    <p>def <span class="ident">get_submodule</span>(</p><p>self, target: str)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the submodule given by <code>target</code> if it exists,
otherwise throws an error.</p>
<p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that
looks like this:</p>
<p>.. code-block::text</p>
<pre><code>A(
    (net_b): Module(
        (net_c): Module(
            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))
        )
        (linear): Linear(in_features=100, out_features=200, bias=True)
    )
)
</code></pre>
<p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested
submodule <code>net_b</code>, which itself has two submodules <code>net_c</code>
and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p>
<p>To check whether or not we have the <code>linear</code> submodule, we
would call <code>get_submodule("net_b.linear")</code>. To check whether
we have the <code>conv</code> submodule, we would call
<code>get_submodule("net_b.net_c.conv")</code>.</p>
<p>The runtime of <code>get_submodule</code> is bounded by the degree
of module nesting in <code>target</code>. A query against
<code>named_modules</code> achieves the same result, but it is O(N) in
the number of transitive modules. So, for a simple check to see
if some submodule exists, <code>get_submodule</code> should always be
used.</p>
<p>Args:
    target: The fully-qualified string name of the submodule
        to look for. (See above example for how to specify a
        fully-qualified string.)</p>
<p>Returns:
    torch.nn.Module: The submodule referenced by <code>target</code></p>
<p>Raises:
    AttributeError: If the target string references an invalid
        path or resolves to something that is not an
        <code>nn.Module</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.get_submodule', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.get_submodule" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_submodule</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Module&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the submodule given by ``target`` if it exists,</span>
<span class="sd">    otherwise throws an error.</span>
<span class="sd">    For example, let&#39;s say you have an ``nn.Module`` ``A`` that</span>
<span class="sd">    looks like this:</span>
<span class="sd">    .. code-block::text</span>
<span class="sd">        A(</span>
<span class="sd">            (net_b): Module(</span>
<span class="sd">                (net_c): Module(</span>
<span class="sd">                    (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))</span>
<span class="sd">                )</span>
<span class="sd">                (linear): Linear(in_features=100, out_features=200, bias=True)</span>
<span class="sd">            )</span>
<span class="sd">        )</span>
<span class="sd">    (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested</span>
<span class="sd">    submodule ``net_b``, which itself has two submodules ``net_c``</span>
<span class="sd">    and ``linear``. ``net_c`` then has a submodule ``conv``.)</span>
<span class="sd">    To check whether or not we have the ``linear`` submodule, we</span>
<span class="sd">    would call ``get_submodule(&quot;net_b.linear&quot;)``. To check whether</span>
<span class="sd">    we have the ``conv`` submodule, we would call</span>
<span class="sd">    ``get_submodule(&quot;net_b.net_c.conv&quot;)``.</span>
<span class="sd">    The runtime of ``get_submodule`` is bounded by the degree</span>
<span class="sd">    of module nesting in ``target``. A query against</span>
<span class="sd">    ``named_modules`` achieves the same result, but it is O(N) in</span>
<span class="sd">    the number of transitive modules. So, for a simple check to see</span>
<span class="sd">    if some submodule exists, ``get_submodule`` should always be</span>
<span class="sd">    used.</span>
<span class="sd">    Args:</span>
<span class="sd">        target: The fully-qualified string name of the submodule</span>
<span class="sd">            to look for. (See above example for how to specify a</span>
<span class="sd">            fully-qualified string.)</span>
<span class="sd">    Returns:</span>
<span class="sd">        torch.nn.Module: The submodule referenced by ``target``</span>
<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: If the target string references an invalid</span>
<span class="sd">            path or resolves to something that is not an</span>
<span class="sd">            ``nn.Module``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">target</span> <span class="o">==</span> <span class="s2">&quot;&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span>
    <span class="n">atoms</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
    <span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="bp">self</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">atoms</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">_get_name</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot; has no &quot;</span>
                                 <span class="s2">&quot;attribute `&quot;</span> <span class="o">+</span> <span class="n">item</span> <span class="o">+</span> <span class="s2">&quot;`&quot;</span><span class="p">)</span>
        <span class="n">mod</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">item</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s2">&quot;`&quot;</span> <span class="o">+</span> <span class="n">item</span> <span class="o">+</span> <span class="s2">&quot;` is not &quot;</span>
                                 <span class="s2">&quot;an nn.Module&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mod</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.half">
    <p>def <span class="ident">half</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.half', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.half" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">half</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all floating point parameters and buffers to ``half`` datatype.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">half</span><span class="p">()</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="k">else</span> <span class="n">t</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.load_state_dict">
    <p>def <span class="ident">load_state_dict</span>(</p><p>self, state_dict: &#39;OrderedDict[str, Tensor]&#39;, strict: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Copies parameters and buffers from :attr:<code>state_dict</code> into
this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then
the keys of :attr:<code>state_dict</code> must exactly match the keys returned
by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.</p>
<p>Args:
    state_dict (dict): a dict containing parameters and
        persistent buffers.
    strict (bool, optional): whether to strictly enforce that the keys
        in :attr:<code>state_dict</code> match the keys returned by this module's
        :meth:<code>~torch.nn.Module.state_dict</code> function. Default: <code>True</code></p>
<p>Returns:
    <code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:
        * <strong>missing_keys</strong> is a list of str containing the missing keys
        * <strong>unexpected_keys</strong> is a list of str containing the unexpected keys</p>
<p>Note:
    If a parameter or buffer is registered as <code>None</code> and its corresponding key
    exists in :attr:<code>state_dict</code>, :meth:<code>load_state_dict</code> will raise a
    <code>RuntimeError</code>.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.load_state_dict', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.load_state_dict" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="s1">&#39;OrderedDict[str, Tensor]&#39;</span><span class="p">,</span>
                    <span class="n">strict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Copies parameters and buffers from :attr:`state_dict` into</span>
<span class="sd">    this module and its descendants. If :attr:`strict` is ``True``, then</span>
<span class="sd">    the keys of :attr:`state_dict` must exactly match the keys returned</span>
<span class="sd">    by this module&#39;s :meth:`~torch.nn.Module.state_dict` function.</span>
<span class="sd">    Args:</span>
<span class="sd">        state_dict (dict): a dict containing parameters and</span>
<span class="sd">            persistent buffers.</span>
<span class="sd">        strict (bool, optional): whether to strictly enforce that the keys</span>
<span class="sd">            in :attr:`state_dict` match the keys returned by this module&#39;s</span>
<span class="sd">            :meth:`~torch.nn.Module.state_dict` function. Default: ``True``</span>
<span class="sd">    Returns:</span>
<span class="sd">        ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:</span>
<span class="sd">            * **missing_keys** is a list of str containing the missing keys</span>
<span class="sd">            * **unexpected_keys** is a list of str containing the unexpected keys</span>
<span class="sd">    Note:</span>
<span class="sd">        If a parameter or buffer is registered as ``None`` and its corresponding key</span>
<span class="sd">        exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a</span>
<span class="sd">        ``RuntimeError``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">missing_keys</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">unexpected_keys</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">error_msgs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># copy state_dict so _load_from_state_dict can modify it</span>
    <span class="n">metadata</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="s1">&#39;_metadata&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># mypy isn&#39;t aware that &quot;_metadata&quot; exists in state_dict</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">_metadata</span> <span class="o">=</span> <span class="n">metadata</span>  <span class="c1"># type: ignore[attr-defined]</span>
    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
        <span class="n">local_metadata</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="n">metadata</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">metadata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">prefix</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">{})</span>
        <span class="n">module</span><span class="o">.</span><span class="n">_load_from_state_dict</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">local_metadata</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">missing_keys</span><span class="p">,</span> <span class="n">unexpected_keys</span><span class="p">,</span> <span class="n">error_msgs</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">child</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">load</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">prefix</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>
    <span class="n">load</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">load</span>
    <span class="k">if</span> <span class="n">strict</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">unexpected_keys</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">error_msgs</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span>
                <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Unexpected key(s) in state_dict: </span><span class="si">{}</span><span class="s1">. &#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;&quot;</span><span class="si">{}</span><span class="s1">&quot;&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">unexpected_keys</span><span class="p">)))</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">missing_keys</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">error_msgs</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span>
                <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Missing key(s) in state_dict: </span><span class="si">{}</span><span class="s1">. &#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;&quot;</span><span class="si">{}</span><span class="s1">&quot;&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">missing_keys</span><span class="p">)))</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">error_msgs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Error(s) in loading state_dict for </span><span class="si">{}</span><span class="s1">:</span><span class="se">\n\t</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                           <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n\t</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">error_msgs</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">_IncompatibleKeys</span><span class="p">(</span><span class="n">missing_keys</span><span class="p">,</span> <span class="n">unexpected_keys</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.modules">
    <p>def <span class="ident">modules</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over all modules in the network.</p>
<p>Yields:
    Module: a module in the network</p>
<p>Note:
    Duplicate modules are returned only once. In the following
    example, <code>l</code> will be returned only once.</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; l = nn.Linear(2, 2)
&gt;&gt;&gt; net = nn.Sequential(l, l)
&gt;&gt;&gt; for idx, m in enumerate(net.modules()):
        print(idx, '-&gt;', m)

0 -&gt; Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
1 -&gt; Linear(in_features=2, out_features=2, bias=True)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.modules', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.modules" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">modules</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="s1">&#39;Module&#39;</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over all modules in the network.</span>
<span class="sd">    Yields:</span>
<span class="sd">        Module: a module in the network</span>
<span class="sd">    Note:</span>
<span class="sd">        Duplicate modules are returned only once. In the following</span>
<span class="sd">        example, ``l`` will be returned only once.</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; l = nn.Linear(2, 2)</span>
<span class="sd">        &gt;&gt;&gt; net = nn.Sequential(l, l)</span>
<span class="sd">        &gt;&gt;&gt; for idx, m in enumerate(net.modules()):</span>
<span class="sd">                print(idx, &#39;-&gt;&#39;, m)</span>
<span class="sd">        0 -&gt; Sequential(</span>
<span class="sd">          (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">          (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        )</span>
<span class="sd">        1 -&gt; Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
        <span class="k">yield</span> <span class="n">module</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.named_buffers">
    <p>def <span class="ident">named_buffers</span>(</p><p>self, prefix: str = &#39;&#39;, recurse: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over module buffers, yielding both the
name of the buffer as well as the buffer itself.</p>
<p>Args:
    prefix (str): prefix to prepend to all buffer names.
    recurse (bool): if True, then yields buffers of this module
        and all submodules. Otherwise, yields only buffers that
        are direct members of this module.</p>
<p>Yields:
    (string, torch.Tensor): Tuple containing the name and buffer</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for name, buf in self.named_buffers():
&gt;&gt;&gt;    if name in ['running_var']:
&gt;&gt;&gt;        print(buf.size())
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.named_buffers', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.named_buffers" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">named_buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over module buffers, yielding both the</span>
<span class="sd">    name of the buffer as well as the buffer itself.</span>
<span class="sd">    Args:</span>
<span class="sd">        prefix (str): prefix to prepend to all buffer names.</span>
<span class="sd">        recurse (bool): if True, then yields buffers of this module</span>
<span class="sd">            and all submodules. Otherwise, yields only buffers that</span>
<span class="sd">            are direct members of this module.</span>
<span class="sd">    Yields:</span>
<span class="sd">        (string, torch.Tensor): Tuple containing the name and buffer</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for name, buf in self.named_buffers():</span>
<span class="sd">        &gt;&gt;&gt;    if name in [&#39;running_var&#39;]:</span>
<span class="sd">        &gt;&gt;&gt;        print(buf.size())</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">gen</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_named_members</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">module</span><span class="p">:</span> <span class="n">module</span><span class="o">.</span><span class="n">_buffers</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span>
        <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span> <span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">gen</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">elem</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.named_children">
    <p>def <span class="ident">named_children</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over immediate children modules, yielding both
the name of the module as well as the module itself.</p>
<p>Yields:
    (string, Module): Tuple containing a name and child module</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for name, module in model.named_children():
&gt;&gt;&gt;     if name in ['conv4', 'conv5']:
&gt;&gt;&gt;         print(module)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.named_children', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.named_children" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">named_children</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s1">&#39;Module&#39;</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over immediate children modules, yielding both</span>
<span class="sd">    the name of the module as well as the module itself.</span>
<span class="sd">    Yields:</span>
<span class="sd">        (string, Module): Tuple containing a name and child module</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for name, module in model.named_children():</span>
<span class="sd">        &gt;&gt;&gt;     if name in [&#39;conv4&#39;, &#39;conv5&#39;]:</span>
<span class="sd">        &gt;&gt;&gt;         print(module)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">memo</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">module</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">memo</span><span class="p">:</span>
            <span class="n">memo</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
            <span class="k">yield</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.named_modules">
    <p>def <span class="ident">named_modules</span>(</p><p>self, memo: Union[Set[ForwardRef(&#39;Module&#39;)], NoneType] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over all modules in the network, yielding
both the name of the module as well as the module itself.</p>
<p>Args:
    memo: a memo to store the set of modules already added to the result
    prefix: a prefix that will be added to the name of the module
    remove_duplicate: whether to remove the duplicated module instances in the result
    or not</p>
<p>Yields:
    (string, Module): Tuple of name and module</p>
<p>Note:
    Duplicate modules are returned only once. In the following
    example, <code>l</code> will be returned only once.</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; l = nn.Linear(2, 2)
&gt;&gt;&gt; net = nn.Sequential(l, l)
&gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):
        print(idx, '-&gt;', m)

0 -&gt; ('', Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
))
1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.named_modules', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.named_modules" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">named_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memo</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Set</span><span class="p">[</span><span class="s1">&#39;Module&#39;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">remove_duplicate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over all modules in the network, yielding</span>
<span class="sd">    both the name of the module as well as the module itself.</span>
<span class="sd">    Args:</span>
<span class="sd">        memo: a memo to store the set of modules already added to the result</span>
<span class="sd">        prefix: a prefix that will be added to the name of the module</span>
<span class="sd">        remove_duplicate: whether to remove the duplicated module instances in the result</span>
<span class="sd">        or not</span>
<span class="sd">    Yields:</span>
<span class="sd">        (string, Module): Tuple of name and module</span>
<span class="sd">    Note:</span>
<span class="sd">        Duplicate modules are returned only once. In the following</span>
<span class="sd">        example, ``l`` will be returned only once.</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; l = nn.Linear(2, 2)</span>
<span class="sd">        &gt;&gt;&gt; net = nn.Sequential(l, l)</span>
<span class="sd">        &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):</span>
<span class="sd">                print(idx, &#39;-&gt;&#39;, m)</span>
<span class="sd">        0 -&gt; (&#39;&#39;, Sequential(</span>
<span class="sd">          (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">          (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        ))</span>
<span class="sd">        1 -&gt; (&#39;0&#39;, Linear(in_features=2, out_features=2, bias=True))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">memo</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">memo</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">if</span> <span class="bp">self</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">memo</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">remove_duplicate</span><span class="p">:</span>
            <span class="n">memo</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">prefix</span><span class="p">,</span> <span class="bp">self</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">submodule_prefix</span> <span class="o">=</span> <span class="n">prefix</span> <span class="o">+</span> <span class="p">(</span><span class="s1">&#39;.&#39;</span> <span class="k">if</span> <span class="n">prefix</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span><span class="p">)</span> <span class="o">+</span> <span class="n">name</span>
            <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_modules</span><span class="p">(</span><span class="n">memo</span><span class="p">,</span> <span class="n">submodule_prefix</span><span class="p">,</span> <span class="n">remove_duplicate</span><span class="p">):</span>
                <span class="k">yield</span> <span class="n">m</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.named_parameters">
    <p>def <span class="ident">named_parameters</span>(</p><p>self, prefix: str = &#39;&#39;, recurse: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over module parameters, yielding both the
name of the parameter as well as the parameter itself.</p>
<p>Args:
    prefix (str): prefix to prepend to all parameter names.
    recurse (bool): if True, then yields parameters of this module
        and all submodules. Otherwise, yields only parameters that
        are direct members of this module.</p>
<p>Yields:
    (string, Parameter): Tuple containing the name and parameter</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for name, param in self.named_parameters():
&gt;&gt;&gt;    if name in ['bias']:
&gt;&gt;&gt;        print(param.size())
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.named_parameters', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.named_parameters" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">named_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over module parameters, yielding both the</span>
<span class="sd">    name of the parameter as well as the parameter itself.</span>
<span class="sd">    Args:</span>
<span class="sd">        prefix (str): prefix to prepend to all parameter names.</span>
<span class="sd">        recurse (bool): if True, then yields parameters of this module</span>
<span class="sd">            and all submodules. Otherwise, yields only parameters that</span>
<span class="sd">            are direct members of this module.</span>
<span class="sd">    Yields:</span>
<span class="sd">        (string, Parameter): Tuple containing the name and parameter</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for name, param in self.named_parameters():</span>
<span class="sd">        &gt;&gt;&gt;    if name in [&#39;bias&#39;]:</span>
<span class="sd">        &gt;&gt;&gt;        print(param.size())</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">gen</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_named_members</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">module</span><span class="p">:</span> <span class="n">module</span><span class="o">.</span><span class="n">_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span>
        <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span> <span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">gen</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">elem</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.parameters">
    <p>def <span class="ident">parameters</span>(</p><p>self, recurse: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over module parameters.</p>
<p>This is typically passed to an optimizer.</p>
<p>Args:
    recurse (bool): if True, then yields parameters of this module
        and all submodules. Otherwise, yields only parameters that
        are direct members of this module.</p>
<p>Yields:
    Parameter: module parameter</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for param in model.parameters():
&gt;&gt;&gt;     print(type(param), param.size())
&lt;class 'torch.Tensor'&gt; (20L,)
&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.parameters', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.parameters" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Parameter</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over module parameters.</span>
<span class="sd">    This is typically passed to an optimizer.</span>
<span class="sd">    Args:</span>
<span class="sd">        recurse (bool): if True, then yields parameters of this module</span>
<span class="sd">            and all submodules. Otherwise, yields only parameters that</span>
<span class="sd">            are direct members of this module.</span>
<span class="sd">    Yields:</span>
<span class="sd">        Parameter: module parameter</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for param in model.parameters():</span>
<span class="sd">        &gt;&gt;&gt;     print(type(param), param.size())</span>
<span class="sd">        &lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="sd">        &lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">param</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.register_backward_hook">
    <p>def <span class="ident">register_backward_hook</span>(</p><p>self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]])</p>
    </div>
    

    
  
    <div class="desc"><p>Registers a backward hook on the module.</p>
<p>This function is deprecated in favor of :meth:<code>~torch.nn.Module.register_full_backward_hook</code> and
the behavior of this function will change in future versions.</p>
<p>Returns:
    :class:<code>torch.utils.hooks.RemovableHandle</code>:
        a handle that can be used to remove the added hook by calling
        <code>handle.remove()</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.register_backward_hook', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.register_backward_hook" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_backward_hook</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="s1">&#39;Module&#39;</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">],</span> <span class="n">Union</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a backward hook on the module.</span>
<span class="sd">    This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and</span>
<span class="sd">    the behavior of this function will change in future versions.</span>
<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_full_backward_hook</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Cannot use both regular backward hooks and full backward hooks on a &quot;</span>
                           <span class="s2">&quot;single Module. Please use only one of them.&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_is_full_backward_hook</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.register_buffer">
    <p>def <span class="ident">register_buffer</span>(</p><p>self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Adds a buffer to the module.</p>
<p>This is typically used to register a buffer that should not to be
considered a model parameter. For example, BatchNorm's <code>running_mean</code>
is not a parameter, but is part of the module's state. Buffers, by
default, are persistent and will be saved alongside parameters. This
behavior can be changed by setting :attr:<code>persistent</code> to <code>False</code>. The
only difference between a persistent buffer and a non-persistent buffer
is that the latter will not be a part of this module's
:attr:<code>state_dict</code>.</p>
<p>Buffers can be accessed as attributes using given names.</p>
<p>Args:
    name (string): name of the buffer. The buffer can be accessed
        from this module using the given name
    tensor (Tensor or None): buffer to be registered. If <code>None</code>, then operations
        that run on buffers, such as :attr:<code>cuda</code>, are ignored. If <code>None</code>,
        the buffer is <strong>not</strong> included in the module's :attr:<code>state_dict</code>.
    persistent (bool): whether the buffer is part of this module's
        :attr:<code>state_dict</code>.</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.register_buffer', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.register_buffer" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_buffer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">persistent</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Adds a buffer to the module.</span>
<span class="sd">    This is typically used to register a buffer that should not to be</span>
<span class="sd">    considered a model parameter. For example, BatchNorm&#39;s ``running_mean``</span>
<span class="sd">    is not a parameter, but is part of the module&#39;s state. Buffers, by</span>
<span class="sd">    default, are persistent and will be saved alongside parameters. This</span>
<span class="sd">    behavior can be changed by setting :attr:`persistent` to ``False``. The</span>
<span class="sd">    only difference between a persistent buffer and a non-persistent buffer</span>
<span class="sd">    is that the latter will not be a part of this module&#39;s</span>
<span class="sd">    :attr:`state_dict`.</span>
<span class="sd">    Buffers can be accessed as attributes using given names.</span>
<span class="sd">    Args:</span>
<span class="sd">        name (string): name of the buffer. The buffer can be accessed</span>
<span class="sd">            from this module using the given name</span>
<span class="sd">        tensor (Tensor or None): buffer to be registered. If ``None``, then operations</span>
<span class="sd">            that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,</span>
<span class="sd">            the buffer is **not** included in the module&#39;s :attr:`state_dict`.</span>
<span class="sd">        persistent (bool): whether the buffer is part of this module&#39;s</span>
<span class="sd">            :attr:`state_dict`.</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; self.register_buffer(&#39;running_mean&#39;, torch.zeros(num_features))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">persistent</span> <span class="ow">is</span> <span class="kc">False</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ScriptModule</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;ScriptModule does not support non-persistent buffers&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="s1">&#39;_buffers&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
            <span class="s2">&quot;cannot assign buffer before Module.__init__() call&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_six</span><span class="o">.</span><span class="n">string_classes</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;buffer name should be a string. &quot;</span>
                        <span class="s2">&quot;Got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">name</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="s1">&#39;.&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;buffer name can&#39;t contain </span><span class="se">\&quot;</span><span class="s2">.</span><span class="se">\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;buffer name can&#39;t be empty string </span><span class="se">\&quot;\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span> <span class="ow">and</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffers</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;attribute &#39;</span><span class="si">{}</span><span class="s2">&#39; already exists&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;cannot assign &#39;</span><span class="si">{}</span><span class="s2">&#39; object to buffer &#39;</span><span class="si">{}</span><span class="s2">&#39; &quot;</span>
                        <span class="s2">&quot;(torch Tensor or None required)&quot;</span>
                        <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">tensor</span><span class="p">),</span> <span class="n">name</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_buffers</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">tensor</span>
        <span class="k">if</span> <span class="n">persistent</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_non_persistent_buffers_set</span><span class="o">.</span><span class="n">discard</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_non_persistent_buffers_set</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.register_forward_hook">
    <p>def <span class="ident">register_forward_hook</span>(</p><p>self, hook: Callable[..., NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Registers a forward hook on the module.</p>
<p>The hook will be called every time after :func:<code>forward</code> has computed an output.
It should have the following signature::</p>
<pre><code>hook(module, input, output) -&gt; None or modified output
</code></pre>
<p>The input contains only the positional arguments given to the module.
Keyword arguments won't be passed to the hooks and only to the <code>forward</code>.
The hook can modify the output. It can modify the input inplace but
it will not have effect on forward since this is called after
:func:<code>forward</code> is called.</p>
<p>Returns:
    :class:<code>torch.utils.hooks.RemovableHandle</code>:
        a handle that can be used to remove the added hook by calling
        <code>handle.remove()</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.register_forward_hook', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.register_forward_hook" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_forward_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a forward hook on the module.</span>
<span class="sd">    The hook will be called every time after :func:`forward` has computed an output.</span>
<span class="sd">    It should have the following signature::</span>
<span class="sd">        hook(module, input, output) -&gt; None or modified output</span>
<span class="sd">    The input contains only the positional arguments given to the module.</span>
<span class="sd">    Keyword arguments won&#39;t be passed to the hooks and only to the ``forward``.</span>
<span class="sd">    The hook can modify the output. It can modify the input inplace but</span>
<span class="sd">    it will not have effect on forward since this is called after</span>
<span class="sd">    :func:`forward` is called.</span>
<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.register_forward_pre_hook">
    <p>def <span class="ident">register_forward_pre_hook</span>(</p><p>self, hook: Callable[..., NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Registers a forward pre-hook on the module.</p>
<p>The hook will be called every time before :func:<code>forward</code> is invoked.
It should have the following signature::</p>
<pre><code>hook(module, input) -&gt; None or modified input
</code></pre>
<p>The input contains only the positional arguments given to the module.
Keyword arguments won't be passed to the hooks and only to the <code>forward</code>.
The hook can modify the input. User can either return a tuple or a
single modified value in the hook. We will wrap the value into a tuple
if a single value is returned(unless that value is already a tuple).</p>
<p>Returns:
    :class:<code>torch.utils.hooks.RemovableHandle</code>:
        a handle that can be used to remove the added hook by calling
        <code>handle.remove()</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.register_forward_pre_hook', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.register_forward_pre_hook" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_forward_pre_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a forward pre-hook on the module.</span>
<span class="sd">    The hook will be called every time before :func:`forward` is invoked.</span>
<span class="sd">    It should have the following signature::</span>
<span class="sd">        hook(module, input) -&gt; None or modified input</span>
<span class="sd">    The input contains only the positional arguments given to the module.</span>
<span class="sd">    Keyword arguments won&#39;t be passed to the hooks and only to the ``forward``.</span>
<span class="sd">    The hook can modify the input. User can either return a tuple or a</span>
<span class="sd">    single modified value in the hook. We will wrap the value into a tuple</span>
<span class="sd">    if a single value is returned(unless that value is already a tuple).</span>
<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.register_full_backward_hook">
    <p>def <span class="ident">register_full_backward_hook</span>(</p><p>self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]])</p>
    </div>
    

    
  
    <div class="desc"><p>Registers a backward hook on the module.</p>
<p>The hook will be called every time the gradients with respect to module
inputs are computed. The hook should have the following signature::</p>
<pre><code>hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None
</code></pre>
<p>The :attr:<code>grad_input</code> and :attr:<code>grad_output</code> are tuples that contain the gradients
with respect to the inputs and outputs respectively. The hook should
not modify its arguments, but it can optionally return a new gradient with
respect to the input that will be used in place of :attr:<code>grad_input</code> in
subsequent computations. :attr:<code>grad_input</code> will only correspond to the inputs given
as positional arguments and all kwarg arguments are ignored. Entries
in :attr:<code>grad_input</code> and :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor
arguments.</p>
<p>For technical reasons, when this hook is applied to a Module, its forward function will
receive a view of each Tensor passed to the Module. Similarly the caller will receive a view
of each Tensor returned by the Module's forward function.</p>
<p>.. warning ::
    Modifying inputs or outputs inplace is not allowed when using backward hooks and
    will raise an error.</p>
<p>Returns:
    :class:<code>torch.utils.hooks.RemovableHandle</code>:
        a handle that can be used to remove the added hook by calling
        <code>handle.remove()</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.register_full_backward_hook', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.register_full_backward_hook" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_full_backward_hook</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="s1">&#39;Module&#39;</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">],</span> <span class="n">Union</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a backward hook on the module.</span>
<span class="sd">    The hook will be called every time the gradients with respect to module</span>
<span class="sd">    inputs are computed. The hook should have the following signature::</span>
<span class="sd">        hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None</span>
<span class="sd">    The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients</span>
<span class="sd">    with respect to the inputs and outputs respectively. The hook should</span>
<span class="sd">    not modify its arguments, but it can optionally return a new gradient with</span>
<span class="sd">    respect to the input that will be used in place of :attr:`grad_input` in</span>
<span class="sd">    subsequent computations. :attr:`grad_input` will only correspond to the inputs given</span>
<span class="sd">    as positional arguments and all kwarg arguments are ignored. Entries</span>
<span class="sd">    in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor</span>
<span class="sd">    arguments.</span>
<span class="sd">    For technical reasons, when this hook is applied to a Module, its forward function will</span>
<span class="sd">    receive a view of each Tensor passed to the Module. Similarly the caller will receive a view</span>
<span class="sd">    of each Tensor returned by the Module&#39;s forward function.</span>
<span class="sd">    .. warning ::</span>
<span class="sd">        Modifying inputs or outputs inplace is not allowed when using backward hooks and</span>
<span class="sd">        will raise an error.</span>
<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_full_backward_hook</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Cannot use both regular backward hooks and full backward hooks on a &quot;</span>
                           <span class="s2">&quot;single Module. Please use only one of them.&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_is_full_backward_hook</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.register_parameter">
    <p>def <span class="ident">register_parameter</span>(</p><p>self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Adds a parameter to the module.</p>
<p>The parameter can be accessed as an attribute using given name.</p>
<p>Args:
    name (string): name of the parameter. The parameter can be accessed
        from this module using the given name
    param (Parameter or None): parameter to be added to the module. If
        <code>None</code>, then operations that run on parameters, such as :attr:<code>cuda</code>,
        are ignored. If <code>None</code>, the parameter is <strong>not</strong> included in the
        module's :attr:<code>state_dict</code>.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.register_parameter', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.register_parameter" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_parameter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">param</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Parameter</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Adds a parameter to the module.</span>
<span class="sd">    The parameter can be accessed as an attribute using given name.</span>
<span class="sd">    Args:</span>
<span class="sd">        name (string): name of the parameter. The parameter can be accessed</span>
<span class="sd">            from this module using the given name</span>
<span class="sd">        param (Parameter or None): parameter to be added to the module. If</span>
<span class="sd">            ``None``, then operations that run on parameters, such as :attr:`cuda`,</span>
<span class="sd">            are ignored. If ``None``, the parameter is **not** included in the</span>
<span class="sd">            module&#39;s :attr:`state_dict`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="s1">&#39;_parameters&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
            <span class="s2">&quot;cannot assign parameter before Module.__init__() call&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_six</span><span class="o">.</span><span class="n">string_classes</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;parameter name should be a string. &quot;</span>
                        <span class="s2">&quot;Got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">name</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="s1">&#39;.&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;parameter name can&#39;t contain </span><span class="se">\&quot;</span><span class="s2">.</span><span class="se">\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;parameter name can&#39;t be empty string </span><span class="se">\&quot;\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span> <span class="ow">and</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parameters</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;attribute &#39;</span><span class="si">{}</span><span class="s2">&#39; already exists&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">param</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_parameters</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;cannot assign &#39;</span><span class="si">{}</span><span class="s2">&#39; object to parameter &#39;</span><span class="si">{}</span><span class="s2">&#39; &quot;</span>
                        <span class="s2">&quot;(torch.nn.Parameter or None required)&quot;</span>
                        <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">param</span><span class="p">),</span> <span class="n">name</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">param</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Cannot assign non-leaf Tensor to parameter &#39;</span><span class="si">{0}</span><span class="s2">&#39;. Model &quot;</span>
            <span class="s2">&quot;parameters must be created explicitly. To express &#39;</span><span class="si">{0}</span><span class="s2">&#39; &quot;</span>
            <span class="s2">&quot;as a function of another Tensor, compute the value in &quot;</span>
            <span class="s2">&quot;the forward() method.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_parameters</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">param</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.requires_grad_">
    <p>def <span class="ident">requires_grad_</span>(</p><p>self: ~T, requires_grad: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Change if autograd should record operations on parameters in this
module.</p>
<p>This method sets the parameters' :attr:<code>requires_grad</code> attributes
in-place.</p>
<p>This method is helpful for freezing part of the module for finetuning
or training parts of a model individually (e.g., GAN training).</p>
<p>See :ref:<code>locally-disable-grad-doc</code> for a comparison between
<code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.</p>
<p>Args:
    requires_grad (bool): whether autograd should record operations on
                          parameters in this module. Default: <code>True</code>.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.requires_grad_', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.requires_grad_" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Change if autograd should record operations on parameters in this</span>
<span class="sd">    module.</span>
<span class="sd">    This method sets the parameters&#39; :attr:`requires_grad` attributes</span>
<span class="sd">    in-place.</span>
<span class="sd">    This method is helpful for freezing part of the module for finetuning</span>
<span class="sd">    or training parts of a model individually (e.g., GAN training).</span>
<span class="sd">    See :ref:`locally-disable-grad-doc` for a comparison between</span>
<span class="sd">    `.requires_grad_()` and several similar mechanisms that may be confused with it.</span>
<span class="sd">    Args:</span>
<span class="sd">        requires_grad (bool): whether autograd should record operations on</span>
<span class="sd">                              parameters in this module. Default: ``True``.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="n">p</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="n">requires_grad</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.set_extra_state">
    <p>def <span class="ident">set_extra_state</span>(</p><p>self, state: Any)</p>
    </div>
    

    
  
    <div class="desc"><p>This function is called from :func:<code>load_state_dict</code> to handle any extra state
found within the <code>state_dict</code>. Implement this function and a corresponding
:func:<code>get_extra_state</code> for your module if you need to store extra state within its
<code>state_dict</code>.</p>
<p>Args:
    state (dict): Extra state from the <code>state_dict</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.set_extra_state', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.set_extra_state" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">set_extra_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function is called from :func:`load_state_dict` to handle any extra state</span>
<span class="sd">    found within the `state_dict`. Implement this function and a corresponding</span>
<span class="sd">    :func:`get_extra_state` for your module if you need to store extra state within its</span>
<span class="sd">    `state_dict`.</span>
<span class="sd">    Args:</span>
<span class="sd">        state (dict): Extra state from the `state_dict`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
        <span class="s2">&quot;Reached a code path in Module.set_extra_state() that should never be called. &quot;</span>
        <span class="s2">&quot;Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md &quot;</span>
        <span class="s2">&quot;to report this bug.&quot;</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.share_memory">
    <p>def <span class="ident">share_memory</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>See :meth:<code>torch.Tensor.share_memory_</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.share_memory', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.share_memory" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">share_memory</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;See :meth:`torch.Tensor.share_memory_`&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">share_memory_</span><span class="p">())</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.state_dict">
    <p>def <span class="ident">state_dict</span>(</p><p>self, destination=None, prefix=&#39;&#39;, keep_vars=False)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns a dictionary containing a whole state of the module.</p>
<p>Both parameters and persistent buffers (e.g. running averages) are
included. Keys are corresponding parameter and buffer names.
Parameters and buffers set to <code>None</code> are not included.</p>
<p>Returns:
    dict:
        a dictionary containing a whole state of the module</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; module.state_dict().keys()
['bias', 'weight']
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.state_dict', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.state_dict" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">destination</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">keep_vars</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns a dictionary containing a whole state of the module.</span>
<span class="sd">    Both parameters and persistent buffers (e.g. running averages) are</span>
<span class="sd">    included. Keys are corresponding parameter and buffer names.</span>
<span class="sd">    Parameters and buffers set to ``None`` are not included.</span>
<span class="sd">    Returns:</span>
<span class="sd">        dict:</span>
<span class="sd">            a dictionary containing a whole state of the module</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; module.state_dict().keys()</span>
<span class="sd">        [&#39;bias&#39;, &#39;weight&#39;]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">destination</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">destination</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="n">destination</span><span class="o">.</span><span class="n">_metadata</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
    <span class="n">destination</span><span class="o">.</span><span class="n">_metadata</span><span class="p">[</span><span class="n">prefix</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">local_metadata</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">version</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_version</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_save_to_state_dict</span><span class="p">(</span><span class="n">destination</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">keep_vars</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(</span><span class="n">destination</span><span class="p">,</span> <span class="n">prefix</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">keep_vars</span><span class="o">=</span><span class="n">keep_vars</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">hook</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_dict_hooks</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
        <span class="n">hook_result</span> <span class="o">=</span> <span class="n">hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">destination</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">local_metadata</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">hook_result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">destination</span> <span class="o">=</span> <span class="n">hook_result</span>
    <span class="k">return</span> <span class="n">destination</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.to">
    <p>def <span class="ident">to</span>(</p><p>self, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>Moves and/or casts the parameters and buffers.</p>
<p>This can be called as</p>
<p>.. function:: to(device=None, dtype=None, non_blocking=False)
   :noindex:</p>
<p>.. function:: to(dtype, non_blocking=False)
   :noindex:</p>
<p>.. function:: to(tensor, non_blocking=False)
   :noindex:</p>
<p>.. function:: to(memory_format=torch.channels_last)
   :noindex:</p>
<p>Its signature is similar to :meth:<code>torch.Tensor.to</code>, but only accepts
floating point or complex :attr:<code>dtype</code>\ s. In addition, this method will
only cast the floating point or complex parameters and buffers to :attr:<code>dtype</code>
(if given). The integral parameters and buffers will be moved
:attr:<code>device</code>, if that is given, but with dtypes unchanged. When
:attr:<code>non_blocking</code> is set, it tries to convert/move asynchronously
with respect to the host if possible, e.g., moving CPU Tensors with
pinned memory to CUDA devices.</p>
<p>See below for examples.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Args:
    device (:class:<code>torch.device</code>): the desired device of the parameters
        and buffers in this module
    dtype (:class:<code>torch.dtype</code>): the desired floating point or complex dtype of
        the parameters and buffers in this module
    tensor (torch.Tensor): Tensor whose dtype and device are the desired
        dtype and device for all parameters and buffers in this module
    memory_format (:class:<code>torch.memory_format</code>): the desired memory
        format for 4D parameters and buffers in this module (keyword
        only argument)</p>
<p>Returns:
    Module: self</p>
<p>Examples::</p>
<pre><code>&gt;&gt;&gt; linear = nn.Linear(2, 2)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1913, -0.3420],
        [-0.5113, -0.2325]])
&gt;&gt;&gt; linear.to(torch.double)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1913, -0.3420],
        [-0.5113, -0.2325]], dtype=torch.float64)
&gt;&gt;&gt; gpu1 = torch.device("cuda:1")
&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1914, -0.3420],
        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')
&gt;&gt;&gt; cpu = torch.device("cpu")
&gt;&gt;&gt; linear.to(cpu)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1914, -0.3420],
        [-0.5112, -0.2324]], dtype=torch.float16)

&gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.3741+0.j,  0.2382+0.j],
        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)
&gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))
tensor([[0.6122+0.j, 0.1150+0.j],
        [0.6122+0.j, 0.1150+0.j],
        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.to', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.to" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves and/or casts the parameters and buffers.</span>
<span class="sd">    This can be called as</span>
<span class="sd">    .. function:: to(device=None, dtype=None, non_blocking=False)</span>
<span class="sd">       :noindex:</span>
<span class="sd">    .. function:: to(dtype, non_blocking=False)</span>
<span class="sd">       :noindex:</span>
<span class="sd">    .. function:: to(tensor, non_blocking=False)</span>
<span class="sd">       :noindex:</span>
<span class="sd">    .. function:: to(memory_format=torch.channels_last)</span>
<span class="sd">       :noindex:</span>
<span class="sd">    Its signature is similar to :meth:`torch.Tensor.to`, but only accepts</span>
<span class="sd">    floating point or complex :attr:`dtype`\ s. In addition, this method will</span>
<span class="sd">    only cast the floating point or complex parameters and buffers to :attr:`dtype`</span>
<span class="sd">    (if given). The integral parameters and buffers will be moved</span>
<span class="sd">    :attr:`device`, if that is given, but with dtypes unchanged. When</span>
<span class="sd">    :attr:`non_blocking` is set, it tries to convert/move asynchronously</span>
<span class="sd">    with respect to the host if possible, e.g., moving CPU Tensors with</span>
<span class="sd">    pinned memory to CUDA devices.</span>
<span class="sd">    See below for examples.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Args:</span>
<span class="sd">        device (:class:`torch.device`): the desired device of the parameters</span>
<span class="sd">            and buffers in this module</span>
<span class="sd">        dtype (:class:`torch.dtype`): the desired floating point or complex dtype of</span>
<span class="sd">            the parameters and buffers in this module</span>
<span class="sd">        tensor (torch.Tensor): Tensor whose dtype and device are the desired</span>
<span class="sd">            dtype and device for all parameters and buffers in this module</span>
<span class="sd">        memory_format (:class:`torch.memory_format`): the desired memory</span>
<span class="sd">            format for 4D parameters and buffers in this module (keyword</span>
<span class="sd">            only argument)</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    Examples::</span>
<span class="sd">        &gt;&gt;&gt; linear = nn.Linear(2, 2)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.1913, -0.3420],</span>
<span class="sd">                [-0.5113, -0.2325]])</span>
<span class="sd">        &gt;&gt;&gt; linear.to(torch.double)</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.1913, -0.3420],</span>
<span class="sd">                [-0.5113, -0.2325]], dtype=torch.float64)</span>
<span class="sd">        &gt;&gt;&gt; gpu1 = torch.device(&quot;cuda:1&quot;)</span>
<span class="sd">        &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.1914, -0.3420],</span>
<span class="sd">                [-0.5112, -0.2324]], dtype=torch.float16, device=&#39;cuda:1&#39;)</span>
<span class="sd">        &gt;&gt;&gt; cpu = torch.device(&quot;cpu&quot;)</span>
<span class="sd">        &gt;&gt;&gt; linear.to(cpu)</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.1914, -0.3420],</span>
<span class="sd">                [-0.5112, -0.2324]], dtype=torch.float16)</span>
<span class="sd">        &gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.3741+0.j,  0.2382+0.j],</span>
<span class="sd">                [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)</span>
<span class="sd">        &gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))</span>
<span class="sd">        tensor([[0.6122+0.j, 0.1150+0.j],</span>
<span class="sd">                [0.6122+0.j, 0.1150+0.j],</span>
<span class="sd">                [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">,</span> <span class="n">convert_to_format</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">_parse_to</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_floating_point</span> <span class="ow">or</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;nn.Module.to only accepts floating point or complex &#39;</span>
                            <span class="s1">&#39;dtypes, but got desired dtype=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dtype</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Complex modules are a new feature under active development whose design may change, &quot;</span>
                <span class="s2">&quot;and some modules might not work as expected when using complex tensors as parameters or buffers. &quot;</span>
                <span class="s2">&quot;Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md &quot;</span>
                <span class="s2">&quot;if a complex module does not work as expected.&quot;</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">convert</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">convert_to_format</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">t</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">t</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="ow">or</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
                        <span class="n">non_blocking</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">convert_to_format</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">t</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="ow">or</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="n">convert</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.to_empty">
    <p>def <span class="ident">to_empty</span>(</p><p>self: ~T, *, device: Union[str, torch.device])</p>
    </div>
    

    
  
    <div class="desc"><p>Moves the parameters and buffers to the specified device without copying storage.</p>
<p>Args:
    device (:class:<code>torch.device</code>): The desired device of the parameters
        and buffers in this module.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.to_empty', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.to_empty" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">to_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves the parameters and buffers to the specified device without copying storage.</span>
<span class="sd">    Args:</span>
<span class="sd">        device (:class:`torch.device`): The desired device of the parameters</span>
<span class="sd">            and buffers in this module.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">))</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.train">
    <p>def <span class="ident">train</span>(</p><p>self: ~T, mode: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Sets the module in training mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>,
etc.</p>
<p>Args:
    mode (bool): whether to set training mode (<code>True</code>) or evaluation
                 mode (<code>False</code>). Default: <code>True</code>.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.train', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.train" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets the module in training mode.</span>
<span class="sd">    This has any effect only on certain modules. See documentations of</span>
<span class="sd">    particular modules for details of their behaviors in training/evaluation</span>
<span class="sd">    mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,</span>
<span class="sd">    etc.</span>
<span class="sd">    Args:</span>
<span class="sd">        mode (bool): whether to set training mode (``True``) or evaluation</span>
<span class="sd">                     mode (``False``). Default: ``True``.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;training mode is expected to be boolean&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="o">=</span> <span class="n">mode</span>
    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">():</span>
        <span class="n">module</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.type">
    <p>def <span class="ident">type</span>(</p><p>self: ~T, dst_type: Union[torch.dtype, str])</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Args:
    dst_type (type or string): the desired type</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.type', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.type" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">type</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">dst_type</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all parameters and buffers to :attr:`dst_type`.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Args:</span>
<span class="sd">        dst_type (type or string): the desired type</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">dst_type</span><span class="p">))</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.xpu">
    <p>def <span class="ident">xpu</span>(</p><p>self: ~T, device: Union[int, torch.device, NoneType] = None)</p>
    </div>
    

    
  
    <div class="desc"><p>Moves all model parameters and buffers to the XPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on XPU while being optimized.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Arguments:
    device (int, optional): if specified, all parameters will be
        copied to that device</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.xpu', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.xpu" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">xpu</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves all model parameters and buffers to the XPU.</span>
<span class="sd">    This also makes associated parameters and buffers different objects. So</span>
<span class="sd">    it should be called before constructing optimizer if the module will</span>
<span class="sd">    live on XPU while being optimized.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Arguments:</span>
<span class="sd">        device (int, optional): if specified, all parameters will be</span>
<span class="sd">            copied to that device</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">xpu</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.PositionwiseFeedforwardLayer.zero_grad">
    <p>def <span class="ident">zero_grad</span>(</p><p>self, set_to_none: bool = False)</p>
    </div>
    

    
  
    <div class="desc"><p>Sets gradients of all model parameters to zero. See similar function
under :class:<code>torch.optim.Optimizer</code> for more context.</p>
<p>Args:
    set_to_none (bool): instead of setting to zero, set the grads to None.
        See :meth:<code>torch.optim.Optimizer.zero_grad</code> for details.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.PositionwiseFeedforwardLayer.zero_grad', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.PositionwiseFeedforwardLayer.zero_grad" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">set_to_none</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets gradients of all model parameters to zero. See similar function</span>
<span class="sd">    under :class:`torch.optim.Optimizer` for more context.</span>
<span class="sd">    Args:</span>
<span class="sd">        set_to_none (bool): instead of setting to zero, set the grads to None.</span>
<span class="sd">            See :meth:`torch.optim.Optimizer.zero_grad` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_is_replica&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;Calling .zero_grad() from a module created with nn.DataParallel() has no effect. &quot;</span>
            <span class="s2">&quot;The parameters are copied (in a differentiable manner) from the original module. &quot;</span>
            <span class="s2">&quot;This means they are not leaf nodes in autograd and so don&#39;t accumulate gradients. &quot;</span>
            <span class="s2">&quot;If you need gradients in your forward method, consider using autograd.grad instead.&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">set_to_none</span><span class="p">:</span>
                <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">grad_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">detach_</span><span class="p">()</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
                <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</pre></div>

  </div>
</div>

  </div>
  
      </div>
      </div>
      
      <div class="item">
      <p id="seq2seq.Seq2Seq" class="name">class <span class="ident">Seq2Seq</span></p>
      
  
    <div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p></div>
  <div class="source_cont">
</div>


      <div class="class">
          <h3>Ancestors (in MRO)</h3>
          <ul class="class_list">
          <li><a href="/torch.nn.modules.module.Module.ext">torch.nn.modules.module.Module</a></li>
          </ul>
          <h3>Class variables</h3>
            <div class="item">
            <p id="seq2seq.Seq2Seq.T_destination" class="name">var <span class="ident">T_destination</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="seq2seq.Seq2Seq.dump_patches" class="name">var <span class="ident">dump_patches</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
          <h3>Methods</h3>
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.add_module">
    <p>def <span class="ident">add_module</span>(</p><p>self, name: str, module: Union[ForwardRef(&#39;Module&#39;), NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Adds a child module to the current module.</p>
<p>The module can be accessed as an attribute using the given name.</p>
<p>Args:
    name (string): name of the child module. The child module can be
        accessed from this module using the given name
    module (Module): child module to be added to the module.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.add_module', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.add_module" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">add_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s1">&#39;Module&#39;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Adds a child module to the current module.</span>
<span class="sd">    The module can be accessed as an attribute using the given name.</span>
<span class="sd">    Args:</span>
<span class="sd">        name (string): name of the child module. The child module can be</span>
<span class="sd">            accessed from this module using the given name</span>
<span class="sd">        module (Module): child module to be added to the module.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">Module</span><span class="p">)</span> <span class="ow">and</span> <span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> is not a Module subclass&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">module</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_six</span><span class="o">.</span><span class="n">string_classes</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;module name should be a string. Got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">name</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span> <span class="ow">and</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;attribute &#39;</span><span class="si">{}</span><span class="s2">&#39; already exists&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">elif</span> <span class="s1">&#39;.&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;module name can&#39;t contain </span><span class="se">\&quot;</span><span class="s2">.</span><span class="se">\&quot;</span><span class="s2">, got: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;module name can&#39;t be empty string </span><span class="se">\&quot;\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.apply">
    <p>def <span class="ident">apply</span>(</p><p>self: ~T, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>)
as well as self. Typical use includes initializing the parameters of a model
(see also :ref:<code>nn-init-doc</code>).</p>
<p>Args:
    fn (:class:<code>Module</code> -&gt; None): function to be applied to each submodule</p>
<p>Returns:
    Module: self</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; @torch.no_grad()
&gt;&gt;&gt; def init_weights(m):
&gt;&gt;&gt;     print(m)
&gt;&gt;&gt;     if type(m) == nn.Linear:
&gt;&gt;&gt;         m.weight.fill_(1.0)
&gt;&gt;&gt;         print(m.weight)
&gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))
&gt;&gt;&gt; net.apply(init_weights)
Linear(in_features=2, out_features=2, bias=True)
Parameter containing:
tensor([[ 1.,  1.],
        [ 1.,  1.]])
Linear(in_features=2, out_features=2, bias=True)
Parameter containing:
tensor([[ 1.,  1.],
        [ 1.,  1.]])
Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.apply', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.apply" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="s1">&#39;Module&#39;</span><span class="p">],</span> <span class="kc">None</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies ``fn`` recursively to every submodule (as returned by ``.children()``)</span>
<span class="sd">    as well as self. Typical use includes initializing the parameters of a model</span>
<span class="sd">    (see also :ref:`nn-init-doc`).</span>
<span class="sd">    Args:</span>
<span class="sd">        fn (:class:`Module` -&gt; None): function to be applied to each submodule</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; @torch.no_grad()</span>
<span class="sd">        &gt;&gt;&gt; def init_weights(m):</span>
<span class="sd">        &gt;&gt;&gt;     print(m)</span>
<span class="sd">        &gt;&gt;&gt;     if type(m) == nn.Linear:</span>
<span class="sd">        &gt;&gt;&gt;         m.weight.fill_(1.0)</span>
<span class="sd">        &gt;&gt;&gt;         print(m.weight)</span>
<span class="sd">        &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))</span>
<span class="sd">        &gt;&gt;&gt; net.apply(init_weights)</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 1.,  1.],</span>
<span class="sd">                [ 1.,  1.]])</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 1.,  1.],</span>
<span class="sd">                [ 1.,  1.]])</span>
<span class="sd">        Sequential(</span>
<span class="sd">          (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">          (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        )</span>
<span class="sd">        Sequential(</span>
<span class="sd">          (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">          (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        )</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">():</span>
        <span class="n">module</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
    <span class="n">fn</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.bfloat16">
    <p>def <span class="ident">bfloat16</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.bfloat16', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.bfloat16" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">bfloat16</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all floating point parameters and buffers to ``bfloat16`` datatype.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">()</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="k">else</span> <span class="n">t</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.buffers">
    <p>def <span class="ident">buffers</span>(</p><p>self, recurse: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over module buffers.</p>
<p>Args:
    recurse (bool): if True, then yields buffers of this module
        and all submodules. Otherwise, yields only buffers that
        are direct members of this module.</p>
<p>Yields:
    torch.Tensor: module buffer</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for buf in model.buffers():
&gt;&gt;&gt;     print(type(buf), buf.size())
&lt;class 'torch.Tensor'&gt; (20L,)
&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.buffers', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.buffers" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over module buffers.</span>
<span class="sd">    Args:</span>
<span class="sd">        recurse (bool): if True, then yields buffers of this module</span>
<span class="sd">            and all submodules. Otherwise, yields only buffers that</span>
<span class="sd">            are direct members of this module.</span>
<span class="sd">    Yields:</span>
<span class="sd">        torch.Tensor: module buffer</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for buf in model.buffers():</span>
<span class="sd">        &gt;&gt;&gt;     print(type(buf), buf.size())</span>
<span class="sd">        &lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="sd">        &lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">buf</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">buf</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.children">
    <p>def <span class="ident">children</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over immediate children modules.</p>
<p>Yields:
    Module: a child module</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.children', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.children" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">children</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="s1">&#39;Module&#39;</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over immediate children modules.</span>
<span class="sd">    Yields:</span>
<span class="sd">        Module: a child module</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
        <span class="k">yield</span> <span class="n">module</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.cpu">
    <p>def <span class="ident">cpu</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Moves all model parameters and buffers to the CPU.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.cpu', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.cpu" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves all model parameters and buffers to the CPU.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.cuda">
    <p>def <span class="ident">cuda</span>(</p><p>self: ~T, device: Union[int, torch.device, NoneType] = None)</p>
    </div>
    

    
  
    <div class="desc"><p>Moves all model parameters and buffers to the GPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on GPU while being optimized.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Args:
    device (int, optional): if specified, all parameters will be
        copied to that device</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.cuda', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.cuda" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">cuda</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves all model parameters and buffers to the GPU.</span>
<span class="sd">    This also makes associated parameters and buffers different objects. So</span>
<span class="sd">    it should be called before constructing optimizer if the module will</span>
<span class="sd">    live on GPU while being optimized.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Args:</span>
<span class="sd">        device (int, optional): if specified, all parameters will be</span>
<span class="sd">            copied to that device</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.double">
    <p>def <span class="ident">double</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.double', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.double" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">double</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all floating point parameters and buffers to ``double`` datatype.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">double</span><span class="p">()</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="k">else</span> <span class="n">t</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.eval">
    <p>def <span class="ident">eval</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Sets the module in evaluation mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>,
etc.</p>
<p>This is equivalent with :meth:<code>self.train(False) &lt;torch.nn.Module.train&gt;</code>.</p>
<p>See :ref:<code>locally-disable-grad-doc</code> for a comparison between
<code>.eval()</code> and several similar mechanisms that may be confused with it.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.eval', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.eval" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets the module in evaluation mode.</span>
<span class="sd">    This has any effect only on certain modules. See documentations of</span>
<span class="sd">    particular modules for details of their behaviors in training/evaluation</span>
<span class="sd">    mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,</span>
<span class="sd">    etc.</span>
<span class="sd">    This is equivalent with :meth:`self.train(False) &lt;torch.nn.Module.train&gt;`.</span>
<span class="sd">    See :ref:`locally-disable-grad-doc` for a comparison between</span>
<span class="sd">    `.eval()` and several similar mechanisms that may be confused with it.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.extra_repr">
    <p>def <span class="ident">extra_repr</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.extra_repr', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.extra_repr" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Set the extra representation of the module</span>
<span class="sd">    To print customized extra information, you should re-implement</span>
<span class="sd">    this method in your own modules. Both single-line and multi-line</span>
<span class="sd">    strings are acceptable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="s1">&#39;&#39;</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.float">
    <p>def <span class="ident">float</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.float', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.float" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">float</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all floating point parameters and buffers to ``float`` datatype.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="k">else</span> <span class="n">t</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.forward">
    <p>def <span class="ident">forward</span>(</p><p>self, src, trg)</p>
    </div>
    

    
  
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.forward', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.forward" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">trg</span><span class="p">):</span>
    
    <span class="c1">#src = [batch size, src len]</span>
    <span class="c1">#trg = [batch size, trg len]</span>
            
    <span class="n">src_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">make_src_mask</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
    <span class="n">trg_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">make_trg_mask</span><span class="p">(</span><span class="n">trg</span><span class="p">)</span>
    
    <span class="c1">#src_mask = [batch size, 1, 1, src len]</span>
    <span class="c1">#trg_mask = [batch size, 1, trg len, trg len]</span>
    
    <span class="n">enc_src</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
    
    <span class="c1">#enc_src = [batch size, src len, hid dim]</span>
            
    <span class="n">output</span><span class="p">,</span> <span class="n">attention</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">trg</span><span class="p">,</span> <span class="n">enc_src</span><span class="p">,</span> <span class="n">trg_mask</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
    
    <span class="c1">#output = [batch size, trg len, output dim]</span>
    <span class="c1">#attention = [batch size, n heads, trg len, src len]</span>
    
    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attention</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.get_buffer">
    <p>def <span class="ident">get_buffer</span>(</p><p>self, target: str)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the buffer given by <code>target</code> if it exists,
otherwise throws an error.</p>
<p>See the docstring for <code>get_submodule</code> for a more detailed
explanation of this method's functionality as well as how to
correctly specify <code>target</code>.</p>
<p>Args:
    target: The fully-qualified string name of the buffer
        to look for. (See <code>get_submodule</code> for how to specify a
        fully-qualified string.)</p>
<p>Returns:
    torch.Tensor: The buffer referenced by <code>target</code></p>
<p>Raises:
    AttributeError: If the target string references an invalid
        path or resolves to something that is not a
        buffer</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.get_buffer', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.get_buffer" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_buffer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the buffer given by ``target`` if it exists,</span>
<span class="sd">    otherwise throws an error.</span>
<span class="sd">    See the docstring for ``get_submodule`` for a more detailed</span>
<span class="sd">    explanation of this method&#39;s functionality as well as how to</span>
<span class="sd">    correctly specify ``target``.</span>
<span class="sd">    Args:</span>
<span class="sd">        target: The fully-qualified string name of the buffer</span>
<span class="sd">            to look for. (See ``get_submodule`` for how to specify a</span>
<span class="sd">            fully-qualified string.)</span>
<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: The buffer referenced by ``target``</span>
<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: If the target string references an invalid</span>
<span class="sd">            path or resolves to something that is not a</span>
<span class="sd">            buffer</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">module_path</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">buffer_name</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">rpartition</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
    <span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_submodule</span><span class="p">(</span><span class="n">module_path</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">buffer_name</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">_get_name</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot; has no attribute `&quot;</span>
                             <span class="o">+</span> <span class="n">buffer_name</span> <span class="o">+</span> <span class="s2">&quot;`&quot;</span><span class="p">)</span>
    <span class="n">buffer</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">buffer_name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">buffer_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">mod</span><span class="o">.</span><span class="n">_buffers</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s2">&quot;`&quot;</span> <span class="o">+</span> <span class="n">buffer_name</span> <span class="o">+</span> <span class="s2">&quot;` is not a buffer&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">buffer</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.get_extra_state">
    <p>def <span class="ident">get_extra_state</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns any extra state to include in the module's state_dict.
Implement this and a corresponding :func:<code>set_extra_state</code> for your module
if you need to store extra state. This function is called when building the
module's <code>state_dict()</code>.</p>
<p>Note that extra state should be pickleable to ensure working serialization
of the state_dict. We only provide provide backwards compatibility guarantees
for serializing Tensors; other objects may break backwards compatibility if
their serialized pickled form changes.</p>
<p>Returns:
    object: Any extra state to store in the module's state_dict</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.get_extra_state', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.get_extra_state" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_extra_state</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns any extra state to include in the module&#39;s state_dict.</span>
<span class="sd">    Implement this and a corresponding :func:`set_extra_state` for your module</span>
<span class="sd">    if you need to store extra state. This function is called when building the</span>
<span class="sd">    module&#39;s `state_dict()`.</span>
<span class="sd">    Note that extra state should be pickleable to ensure working serialization</span>
<span class="sd">    of the state_dict. We only provide provide backwards compatibility guarantees</span>
<span class="sd">    for serializing Tensors; other objects may break backwards compatibility if</span>
<span class="sd">    their serialized pickled form changes.</span>
<span class="sd">    Returns:</span>
<span class="sd">        object: Any extra state to store in the module&#39;s state_dict</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
        <span class="s2">&quot;Reached a code path in Module.get_extra_state() that should never be called. &quot;</span>
        <span class="s2">&quot;Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md &quot;</span>
        <span class="s2">&quot;to report this bug.&quot;</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.get_parameter">
    <p>def <span class="ident">get_parameter</span>(</p><p>self, target: str)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the parameter given by <code>target</code> if it exists,
otherwise throws an error.</p>
<p>See the docstring for <code>get_submodule</code> for a more detailed
explanation of this method's functionality as well as how to
correctly specify <code>target</code>.</p>
<p>Args:
    target: The fully-qualified string name of the Parameter
        to look for. (See <code>get_submodule</code> for how to specify a
        fully-qualified string.)</p>
<p>Returns:
    torch.nn.Parameter: The Parameter referenced by <code>target</code></p>
<p>Raises:
    AttributeError: If the target string references an invalid
        path or resolves to something that is not an
        <code>nn.Parameter</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.get_parameter', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.get_parameter" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_parameter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Parameter&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the parameter given by ``target`` if it exists,</span>
<span class="sd">    otherwise throws an error.</span>
<span class="sd">    See the docstring for ``get_submodule`` for a more detailed</span>
<span class="sd">    explanation of this method&#39;s functionality as well as how to</span>
<span class="sd">    correctly specify ``target``.</span>
<span class="sd">    Args:</span>
<span class="sd">        target: The fully-qualified string name of the Parameter</span>
<span class="sd">            to look for. (See ``get_submodule`` for how to specify a</span>
<span class="sd">            fully-qualified string.)</span>
<span class="sd">    Returns:</span>
<span class="sd">        torch.nn.Parameter: The Parameter referenced by ``target``</span>
<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: If the target string references an invalid</span>
<span class="sd">            path or resolves to something that is not an</span>
<span class="sd">            ``nn.Parameter``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">module_path</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">param_name</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">rpartition</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
    <span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_submodule</span><span class="p">(</span><span class="n">module_path</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">param_name</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">_get_name</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot; has no attribute `&quot;</span>
                             <span class="o">+</span> <span class="n">param_name</span> <span class="o">+</span> <span class="s2">&quot;`&quot;</span><span class="p">)</span>
    <span class="n">param</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">param_name</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s2">&quot;`&quot;</span> <span class="o">+</span> <span class="n">param_name</span> <span class="o">+</span> <span class="s2">&quot;` is not an &quot;</span>
                             <span class="s2">&quot;nn.Parameter&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">param</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.get_submodule">
    <p>def <span class="ident">get_submodule</span>(</p><p>self, target: str)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the submodule given by <code>target</code> if it exists,
otherwise throws an error.</p>
<p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that
looks like this:</p>
<p>.. code-block::text</p>
<pre><code>A(
    (net_b): Module(
        (net_c): Module(
            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))
        )
        (linear): Linear(in_features=100, out_features=200, bias=True)
    )
)
</code></pre>
<p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested
submodule <code>net_b</code>, which itself has two submodules <code>net_c</code>
and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p>
<p>To check whether or not we have the <code>linear</code> submodule, we
would call <code>get_submodule("net_b.linear")</code>. To check whether
we have the <code>conv</code> submodule, we would call
<code>get_submodule("net_b.net_c.conv")</code>.</p>
<p>The runtime of <code>get_submodule</code> is bounded by the degree
of module nesting in <code>target</code>. A query against
<code>named_modules</code> achieves the same result, but it is O(N) in
the number of transitive modules. So, for a simple check to see
if some submodule exists, <code>get_submodule</code> should always be
used.</p>
<p>Args:
    target: The fully-qualified string name of the submodule
        to look for. (See above example for how to specify a
        fully-qualified string.)</p>
<p>Returns:
    torch.nn.Module: The submodule referenced by <code>target</code></p>
<p>Raises:
    AttributeError: If the target string references an invalid
        path or resolves to something that is not an
        <code>nn.Module</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.get_submodule', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.get_submodule" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_submodule</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Module&quot;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the submodule given by ``target`` if it exists,</span>
<span class="sd">    otherwise throws an error.</span>
<span class="sd">    For example, let&#39;s say you have an ``nn.Module`` ``A`` that</span>
<span class="sd">    looks like this:</span>
<span class="sd">    .. code-block::text</span>
<span class="sd">        A(</span>
<span class="sd">            (net_b): Module(</span>
<span class="sd">                (net_c): Module(</span>
<span class="sd">                    (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))</span>
<span class="sd">                )</span>
<span class="sd">                (linear): Linear(in_features=100, out_features=200, bias=True)</span>
<span class="sd">            )</span>
<span class="sd">        )</span>
<span class="sd">    (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested</span>
<span class="sd">    submodule ``net_b``, which itself has two submodules ``net_c``</span>
<span class="sd">    and ``linear``. ``net_c`` then has a submodule ``conv``.)</span>
<span class="sd">    To check whether or not we have the ``linear`` submodule, we</span>
<span class="sd">    would call ``get_submodule(&quot;net_b.linear&quot;)``. To check whether</span>
<span class="sd">    we have the ``conv`` submodule, we would call</span>
<span class="sd">    ``get_submodule(&quot;net_b.net_c.conv&quot;)``.</span>
<span class="sd">    The runtime of ``get_submodule`` is bounded by the degree</span>
<span class="sd">    of module nesting in ``target``. A query against</span>
<span class="sd">    ``named_modules`` achieves the same result, but it is O(N) in</span>
<span class="sd">    the number of transitive modules. So, for a simple check to see</span>
<span class="sd">    if some submodule exists, ``get_submodule`` should always be</span>
<span class="sd">    used.</span>
<span class="sd">    Args:</span>
<span class="sd">        target: The fully-qualified string name of the submodule</span>
<span class="sd">            to look for. (See above example for how to specify a</span>
<span class="sd">            fully-qualified string.)</span>
<span class="sd">    Returns:</span>
<span class="sd">        torch.nn.Module: The submodule referenced by ``target``</span>
<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: If the target string references an invalid</span>
<span class="sd">            path or resolves to something that is not an</span>
<span class="sd">            ``nn.Module``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">target</span> <span class="o">==</span> <span class="s2">&quot;&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span>
    <span class="n">atoms</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
    <span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="bp">self</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">atoms</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">_get_name</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot; has no &quot;</span>
                                 <span class="s2">&quot;attribute `&quot;</span> <span class="o">+</span> <span class="n">item</span> <span class="o">+</span> <span class="s2">&quot;`&quot;</span><span class="p">)</span>
        <span class="n">mod</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">item</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s2">&quot;`&quot;</span> <span class="o">+</span> <span class="n">item</span> <span class="o">+</span> <span class="s2">&quot;` is not &quot;</span>
                                 <span class="s2">&quot;an nn.Module&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mod</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.half">
    <p>def <span class="ident">half</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.half', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.half" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">half</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all floating point parameters and buffers to ``half`` datatype.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">half</span><span class="p">()</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="k">else</span> <span class="n">t</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.load_state_dict">
    <p>def <span class="ident">load_state_dict</span>(</p><p>self, state_dict: &#39;OrderedDict[str, Tensor]&#39;, strict: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Copies parameters and buffers from :attr:<code>state_dict</code> into
this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then
the keys of :attr:<code>state_dict</code> must exactly match the keys returned
by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.</p>
<p>Args:
    state_dict (dict): a dict containing parameters and
        persistent buffers.
    strict (bool, optional): whether to strictly enforce that the keys
        in :attr:<code>state_dict</code> match the keys returned by this module's
        :meth:<code>~torch.nn.Module.state_dict</code> function. Default: <code>True</code></p>
<p>Returns:
    <code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:
        * <strong>missing_keys</strong> is a list of str containing the missing keys
        * <strong>unexpected_keys</strong> is a list of str containing the unexpected keys</p>
<p>Note:
    If a parameter or buffer is registered as <code>None</code> and its corresponding key
    exists in :attr:<code>state_dict</code>, :meth:<code>load_state_dict</code> will raise a
    <code>RuntimeError</code>.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.load_state_dict', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.load_state_dict" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="s1">&#39;OrderedDict[str, Tensor]&#39;</span><span class="p">,</span>
                    <span class="n">strict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Copies parameters and buffers from :attr:`state_dict` into</span>
<span class="sd">    this module and its descendants. If :attr:`strict` is ``True``, then</span>
<span class="sd">    the keys of :attr:`state_dict` must exactly match the keys returned</span>
<span class="sd">    by this module&#39;s :meth:`~torch.nn.Module.state_dict` function.</span>
<span class="sd">    Args:</span>
<span class="sd">        state_dict (dict): a dict containing parameters and</span>
<span class="sd">            persistent buffers.</span>
<span class="sd">        strict (bool, optional): whether to strictly enforce that the keys</span>
<span class="sd">            in :attr:`state_dict` match the keys returned by this module&#39;s</span>
<span class="sd">            :meth:`~torch.nn.Module.state_dict` function. Default: ``True``</span>
<span class="sd">    Returns:</span>
<span class="sd">        ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:</span>
<span class="sd">            * **missing_keys** is a list of str containing the missing keys</span>
<span class="sd">            * **unexpected_keys** is a list of str containing the unexpected keys</span>
<span class="sd">    Note:</span>
<span class="sd">        If a parameter or buffer is registered as ``None`` and its corresponding key</span>
<span class="sd">        exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a</span>
<span class="sd">        ``RuntimeError``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">missing_keys</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">unexpected_keys</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">error_msgs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># copy state_dict so _load_from_state_dict can modify it</span>
    <span class="n">metadata</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="s1">&#39;_metadata&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># mypy isn&#39;t aware that &quot;_metadata&quot; exists in state_dict</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">_metadata</span> <span class="o">=</span> <span class="n">metadata</span>  <span class="c1"># type: ignore[attr-defined]</span>
    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
        <span class="n">local_metadata</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="n">metadata</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">metadata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">prefix</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">{})</span>
        <span class="n">module</span><span class="o">.</span><span class="n">_load_from_state_dict</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">local_metadata</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">missing_keys</span><span class="p">,</span> <span class="n">unexpected_keys</span><span class="p">,</span> <span class="n">error_msgs</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">child</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">load</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">prefix</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>
    <span class="n">load</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">load</span>
    <span class="k">if</span> <span class="n">strict</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">unexpected_keys</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">error_msgs</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span>
                <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Unexpected key(s) in state_dict: </span><span class="si">{}</span><span class="s1">. &#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;&quot;</span><span class="si">{}</span><span class="s1">&quot;&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">unexpected_keys</span><span class="p">)))</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">missing_keys</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">error_msgs</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span>
                <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Missing key(s) in state_dict: </span><span class="si">{}</span><span class="s1">. &#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;&quot;</span><span class="si">{}</span><span class="s1">&quot;&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">missing_keys</span><span class="p">)))</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">error_msgs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Error(s) in loading state_dict for </span><span class="si">{}</span><span class="s1">:</span><span class="se">\n\t</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                           <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n\t</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">error_msgs</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">_IncompatibleKeys</span><span class="p">(</span><span class="n">missing_keys</span><span class="p">,</span> <span class="n">unexpected_keys</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.make_src_mask">
    <p>def <span class="ident">make_src_mask</span>(</p><p>self, src)</p>
    </div>
    

    
  
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.make_src_mask', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.make_src_mask" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">make_src_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">):</span>
    
    <span class="c1">#src = [batch size, src len]</span>
    
    <span class="n">src_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">src</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_pad_idx</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="c1">#src_mask = [batch size, 1, 1, src len]</span>
    <span class="k">return</span> <span class="n">src_mask</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.make_trg_mask">
    <p>def <span class="ident">make_trg_mask</span>(</p><p>self, trg)</p>
    </div>
    

    
  
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.make_trg_mask', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.make_trg_mask" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">make_trg_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trg</span><span class="p">):</span>
    
    <span class="c1">#trg = [batch size, trg len]</span>
    
    <span class="n">trg_pad_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">trg</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trg_pad_idx</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    
    <span class="c1">#trg_pad_mask = [batch size, 1, 1, trg len]</span>
    
    <span class="n">trg_len</span> <span class="o">=</span> <span class="n">trg</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="n">trg_sub_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">trg_len</span><span class="p">,</span> <span class="n">trg_len</span><span class="p">),</span> <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">))</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
    
    <span class="c1">#trg_sub_mask = [trg len, trg len]</span>
        
    <span class="n">trg_mask</span> <span class="o">=</span> <span class="n">trg_pad_mask</span> <span class="o">&amp;</span> <span class="n">trg_sub_mask</span>
    
    <span class="c1">#trg_mask = [batch size, 1, trg len, trg len]</span>
    
    <span class="k">return</span> <span class="n">trg_mask</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.modules">
    <p>def <span class="ident">modules</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over all modules in the network.</p>
<p>Yields:
    Module: a module in the network</p>
<p>Note:
    Duplicate modules are returned only once. In the following
    example, <code>l</code> will be returned only once.</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; l = nn.Linear(2, 2)
&gt;&gt;&gt; net = nn.Sequential(l, l)
&gt;&gt;&gt; for idx, m in enumerate(net.modules()):
        print(idx, '-&gt;', m)

0 -&gt; Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
1 -&gt; Linear(in_features=2, out_features=2, bias=True)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.modules', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.modules" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">modules</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="s1">&#39;Module&#39;</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over all modules in the network.</span>
<span class="sd">    Yields:</span>
<span class="sd">        Module: a module in the network</span>
<span class="sd">    Note:</span>
<span class="sd">        Duplicate modules are returned only once. In the following</span>
<span class="sd">        example, ``l`` will be returned only once.</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; l = nn.Linear(2, 2)</span>
<span class="sd">        &gt;&gt;&gt; net = nn.Sequential(l, l)</span>
<span class="sd">        &gt;&gt;&gt; for idx, m in enumerate(net.modules()):</span>
<span class="sd">                print(idx, &#39;-&gt;&#39;, m)</span>
<span class="sd">        0 -&gt; Sequential(</span>
<span class="sd">          (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">          (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        )</span>
<span class="sd">        1 -&gt; Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
        <span class="k">yield</span> <span class="n">module</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.named_buffers">
    <p>def <span class="ident">named_buffers</span>(</p><p>self, prefix: str = &#39;&#39;, recurse: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over module buffers, yielding both the
name of the buffer as well as the buffer itself.</p>
<p>Args:
    prefix (str): prefix to prepend to all buffer names.
    recurse (bool): if True, then yields buffers of this module
        and all submodules. Otherwise, yields only buffers that
        are direct members of this module.</p>
<p>Yields:
    (string, torch.Tensor): Tuple containing the name and buffer</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for name, buf in self.named_buffers():
&gt;&gt;&gt;    if name in ['running_var']:
&gt;&gt;&gt;        print(buf.size())
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.named_buffers', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.named_buffers" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">named_buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over module buffers, yielding both the</span>
<span class="sd">    name of the buffer as well as the buffer itself.</span>
<span class="sd">    Args:</span>
<span class="sd">        prefix (str): prefix to prepend to all buffer names.</span>
<span class="sd">        recurse (bool): if True, then yields buffers of this module</span>
<span class="sd">            and all submodules. Otherwise, yields only buffers that</span>
<span class="sd">            are direct members of this module.</span>
<span class="sd">    Yields:</span>
<span class="sd">        (string, torch.Tensor): Tuple containing the name and buffer</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for name, buf in self.named_buffers():</span>
<span class="sd">        &gt;&gt;&gt;    if name in [&#39;running_var&#39;]:</span>
<span class="sd">        &gt;&gt;&gt;        print(buf.size())</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">gen</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_named_members</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">module</span><span class="p">:</span> <span class="n">module</span><span class="o">.</span><span class="n">_buffers</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span>
        <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span> <span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">gen</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">elem</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.named_children">
    <p>def <span class="ident">named_children</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over immediate children modules, yielding both
the name of the module as well as the module itself.</p>
<p>Yields:
    (string, Module): Tuple containing a name and child module</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for name, module in model.named_children():
&gt;&gt;&gt;     if name in ['conv4', 'conv5']:
&gt;&gt;&gt;         print(module)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.named_children', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.named_children" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">named_children</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s1">&#39;Module&#39;</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over immediate children modules, yielding both</span>
<span class="sd">    the name of the module as well as the module itself.</span>
<span class="sd">    Yields:</span>
<span class="sd">        (string, Module): Tuple containing a name and child module</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for name, module in model.named_children():</span>
<span class="sd">        &gt;&gt;&gt;     if name in [&#39;conv4&#39;, &#39;conv5&#39;]:</span>
<span class="sd">        &gt;&gt;&gt;         print(module)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">memo</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">module</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">memo</span><span class="p">:</span>
            <span class="n">memo</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
            <span class="k">yield</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.named_modules">
    <p>def <span class="ident">named_modules</span>(</p><p>self, memo: Union[Set[ForwardRef(&#39;Module&#39;)], NoneType] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over all modules in the network, yielding
both the name of the module as well as the module itself.</p>
<p>Args:
    memo: a memo to store the set of modules already added to the result
    prefix: a prefix that will be added to the name of the module
    remove_duplicate: whether to remove the duplicated module instances in the result
    or not</p>
<p>Yields:
    (string, Module): Tuple of name and module</p>
<p>Note:
    Duplicate modules are returned only once. In the following
    example, <code>l</code> will be returned only once.</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; l = nn.Linear(2, 2)
&gt;&gt;&gt; net = nn.Sequential(l, l)
&gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):
        print(idx, '-&gt;', m)

0 -&gt; ('', Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
))
1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.named_modules', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.named_modules" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">named_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memo</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Set</span><span class="p">[</span><span class="s1">&#39;Module&#39;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">remove_duplicate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over all modules in the network, yielding</span>
<span class="sd">    both the name of the module as well as the module itself.</span>
<span class="sd">    Args:</span>
<span class="sd">        memo: a memo to store the set of modules already added to the result</span>
<span class="sd">        prefix: a prefix that will be added to the name of the module</span>
<span class="sd">        remove_duplicate: whether to remove the duplicated module instances in the result</span>
<span class="sd">        or not</span>
<span class="sd">    Yields:</span>
<span class="sd">        (string, Module): Tuple of name and module</span>
<span class="sd">    Note:</span>
<span class="sd">        Duplicate modules are returned only once. In the following</span>
<span class="sd">        example, ``l`` will be returned only once.</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; l = nn.Linear(2, 2)</span>
<span class="sd">        &gt;&gt;&gt; net = nn.Sequential(l, l)</span>
<span class="sd">        &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):</span>
<span class="sd">                print(idx, &#39;-&gt;&#39;, m)</span>
<span class="sd">        0 -&gt; (&#39;&#39;, Sequential(</span>
<span class="sd">          (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">          (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        ))</span>
<span class="sd">        1 -&gt; (&#39;0&#39;, Linear(in_features=2, out_features=2, bias=True))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">memo</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">memo</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">if</span> <span class="bp">self</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">memo</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">remove_duplicate</span><span class="p">:</span>
            <span class="n">memo</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">prefix</span><span class="p">,</span> <span class="bp">self</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">submodule_prefix</span> <span class="o">=</span> <span class="n">prefix</span> <span class="o">+</span> <span class="p">(</span><span class="s1">&#39;.&#39;</span> <span class="k">if</span> <span class="n">prefix</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span><span class="p">)</span> <span class="o">+</span> <span class="n">name</span>
            <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_modules</span><span class="p">(</span><span class="n">memo</span><span class="p">,</span> <span class="n">submodule_prefix</span><span class="p">,</span> <span class="n">remove_duplicate</span><span class="p">):</span>
                <span class="k">yield</span> <span class="n">m</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.named_parameters">
    <p>def <span class="ident">named_parameters</span>(</p><p>self, prefix: str = &#39;&#39;, recurse: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over module parameters, yielding both the
name of the parameter as well as the parameter itself.</p>
<p>Args:
    prefix (str): prefix to prepend to all parameter names.
    recurse (bool): if True, then yields parameters of this module
        and all submodules. Otherwise, yields only parameters that
        are direct members of this module.</p>
<p>Yields:
    (string, Parameter): Tuple containing the name and parameter</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for name, param in self.named_parameters():
&gt;&gt;&gt;    if name in ['bias']:
&gt;&gt;&gt;        print(param.size())
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.named_parameters', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.named_parameters" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">named_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over module parameters, yielding both the</span>
<span class="sd">    name of the parameter as well as the parameter itself.</span>
<span class="sd">    Args:</span>
<span class="sd">        prefix (str): prefix to prepend to all parameter names.</span>
<span class="sd">        recurse (bool): if True, then yields parameters of this module</span>
<span class="sd">            and all submodules. Otherwise, yields only parameters that</span>
<span class="sd">            are direct members of this module.</span>
<span class="sd">    Yields:</span>
<span class="sd">        (string, Parameter): Tuple containing the name and parameter</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for name, param in self.named_parameters():</span>
<span class="sd">        &gt;&gt;&gt;    if name in [&#39;bias&#39;]:</span>
<span class="sd">        &gt;&gt;&gt;        print(param.size())</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">gen</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_named_members</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">module</span><span class="p">:</span> <span class="n">module</span><span class="o">.</span><span class="n">_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span>
        <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span> <span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">gen</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">elem</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.parameters">
    <p>def <span class="ident">parameters</span>(</p><p>self, recurse: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns an iterator over module parameters.</p>
<p>This is typically passed to an optimizer.</p>
<p>Args:
    recurse (bool): if True, then yields parameters of this module
        and all submodules. Otherwise, yields only parameters that
        are direct members of this module.</p>
<p>Yields:
    Parameter: module parameter</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; for param in model.parameters():
&gt;&gt;&gt;     print(type(param), param.size())
&lt;class 'torch.Tensor'&gt; (20L,)
&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.parameters', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.parameters" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Parameter</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over module parameters.</span>
<span class="sd">    This is typically passed to an optimizer.</span>
<span class="sd">    Args:</span>
<span class="sd">        recurse (bool): if True, then yields parameters of this module</span>
<span class="sd">            and all submodules. Otherwise, yields only parameters that</span>
<span class="sd">            are direct members of this module.</span>
<span class="sd">    Yields:</span>
<span class="sd">        Parameter: module parameter</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; for param in model.parameters():</span>
<span class="sd">        &gt;&gt;&gt;     print(type(param), param.size())</span>
<span class="sd">        &lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="sd">        &lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">param</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.register_backward_hook">
    <p>def <span class="ident">register_backward_hook</span>(</p><p>self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]])</p>
    </div>
    

    
  
    <div class="desc"><p>Registers a backward hook on the module.</p>
<p>This function is deprecated in favor of :meth:<code>~torch.nn.Module.register_full_backward_hook</code> and
the behavior of this function will change in future versions.</p>
<p>Returns:
    :class:<code>torch.utils.hooks.RemovableHandle</code>:
        a handle that can be used to remove the added hook by calling
        <code>handle.remove()</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.register_backward_hook', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.register_backward_hook" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_backward_hook</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="s1">&#39;Module&#39;</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">],</span> <span class="n">Union</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a backward hook on the module.</span>
<span class="sd">    This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and</span>
<span class="sd">    the behavior of this function will change in future versions.</span>
<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_full_backward_hook</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Cannot use both regular backward hooks and full backward hooks on a &quot;</span>
                           <span class="s2">&quot;single Module. Please use only one of them.&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_is_full_backward_hook</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.register_buffer">
    <p>def <span class="ident">register_buffer</span>(</p><p>self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Adds a buffer to the module.</p>
<p>This is typically used to register a buffer that should not to be
considered a model parameter. For example, BatchNorm's <code>running_mean</code>
is not a parameter, but is part of the module's state. Buffers, by
default, are persistent and will be saved alongside parameters. This
behavior can be changed by setting :attr:<code>persistent</code> to <code>False</code>. The
only difference between a persistent buffer and a non-persistent buffer
is that the latter will not be a part of this module's
:attr:<code>state_dict</code>.</p>
<p>Buffers can be accessed as attributes using given names.</p>
<p>Args:
    name (string): name of the buffer. The buffer can be accessed
        from this module using the given name
    tensor (Tensor or None): buffer to be registered. If <code>None</code>, then operations
        that run on buffers, such as :attr:<code>cuda</code>, are ignored. If <code>None</code>,
        the buffer is <strong>not</strong> included in the module's :attr:<code>state_dict</code>.
    persistent (bool): whether the buffer is part of this module's
        :attr:<code>state_dict</code>.</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.register_buffer', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.register_buffer" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_buffer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">persistent</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Adds a buffer to the module.</span>
<span class="sd">    This is typically used to register a buffer that should not to be</span>
<span class="sd">    considered a model parameter. For example, BatchNorm&#39;s ``running_mean``</span>
<span class="sd">    is not a parameter, but is part of the module&#39;s state. Buffers, by</span>
<span class="sd">    default, are persistent and will be saved alongside parameters. This</span>
<span class="sd">    behavior can be changed by setting :attr:`persistent` to ``False``. The</span>
<span class="sd">    only difference between a persistent buffer and a non-persistent buffer</span>
<span class="sd">    is that the latter will not be a part of this module&#39;s</span>
<span class="sd">    :attr:`state_dict`.</span>
<span class="sd">    Buffers can be accessed as attributes using given names.</span>
<span class="sd">    Args:</span>
<span class="sd">        name (string): name of the buffer. The buffer can be accessed</span>
<span class="sd">            from this module using the given name</span>
<span class="sd">        tensor (Tensor or None): buffer to be registered. If ``None``, then operations</span>
<span class="sd">            that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,</span>
<span class="sd">            the buffer is **not** included in the module&#39;s :attr:`state_dict`.</span>
<span class="sd">        persistent (bool): whether the buffer is part of this module&#39;s</span>
<span class="sd">            :attr:`state_dict`.</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; self.register_buffer(&#39;running_mean&#39;, torch.zeros(num_features))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">persistent</span> <span class="ow">is</span> <span class="kc">False</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ScriptModule</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;ScriptModule does not support non-persistent buffers&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="s1">&#39;_buffers&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
            <span class="s2">&quot;cannot assign buffer before Module.__init__() call&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_six</span><span class="o">.</span><span class="n">string_classes</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;buffer name should be a string. &quot;</span>
                        <span class="s2">&quot;Got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">name</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="s1">&#39;.&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;buffer name can&#39;t contain </span><span class="se">\&quot;</span><span class="s2">.</span><span class="se">\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;buffer name can&#39;t be empty string </span><span class="se">\&quot;\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span> <span class="ow">and</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffers</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;attribute &#39;</span><span class="si">{}</span><span class="s2">&#39; already exists&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;cannot assign &#39;</span><span class="si">{}</span><span class="s2">&#39; object to buffer &#39;</span><span class="si">{}</span><span class="s2">&#39; &quot;</span>
                        <span class="s2">&quot;(torch Tensor or None required)&quot;</span>
                        <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">tensor</span><span class="p">),</span> <span class="n">name</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_buffers</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">tensor</span>
        <span class="k">if</span> <span class="n">persistent</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_non_persistent_buffers_set</span><span class="o">.</span><span class="n">discard</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_non_persistent_buffers_set</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.register_forward_hook">
    <p>def <span class="ident">register_forward_hook</span>(</p><p>self, hook: Callable[..., NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Registers a forward hook on the module.</p>
<p>The hook will be called every time after :func:<code>forward</code> has computed an output.
It should have the following signature::</p>
<pre><code>hook(module, input, output) -&gt; None or modified output
</code></pre>
<p>The input contains only the positional arguments given to the module.
Keyword arguments won't be passed to the hooks and only to the <code>forward</code>.
The hook can modify the output. It can modify the input inplace but
it will not have effect on forward since this is called after
:func:<code>forward</code> is called.</p>
<p>Returns:
    :class:<code>torch.utils.hooks.RemovableHandle</code>:
        a handle that can be used to remove the added hook by calling
        <code>handle.remove()</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.register_forward_hook', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.register_forward_hook" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_forward_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a forward hook on the module.</span>
<span class="sd">    The hook will be called every time after :func:`forward` has computed an output.</span>
<span class="sd">    It should have the following signature::</span>
<span class="sd">        hook(module, input, output) -&gt; None or modified output</span>
<span class="sd">    The input contains only the positional arguments given to the module.</span>
<span class="sd">    Keyword arguments won&#39;t be passed to the hooks and only to the ``forward``.</span>
<span class="sd">    The hook can modify the output. It can modify the input inplace but</span>
<span class="sd">    it will not have effect on forward since this is called after</span>
<span class="sd">    :func:`forward` is called.</span>
<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.register_forward_pre_hook">
    <p>def <span class="ident">register_forward_pre_hook</span>(</p><p>self, hook: Callable[..., NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Registers a forward pre-hook on the module.</p>
<p>The hook will be called every time before :func:<code>forward</code> is invoked.
It should have the following signature::</p>
<pre><code>hook(module, input) -&gt; None or modified input
</code></pre>
<p>The input contains only the positional arguments given to the module.
Keyword arguments won't be passed to the hooks and only to the <code>forward</code>.
The hook can modify the input. User can either return a tuple or a
single modified value in the hook. We will wrap the value into a tuple
if a single value is returned(unless that value is already a tuple).</p>
<p>Returns:
    :class:<code>torch.utils.hooks.RemovableHandle</code>:
        a handle that can be used to remove the added hook by calling
        <code>handle.remove()</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.register_forward_pre_hook', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.register_forward_pre_hook" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_forward_pre_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a forward pre-hook on the module.</span>
<span class="sd">    The hook will be called every time before :func:`forward` is invoked.</span>
<span class="sd">    It should have the following signature::</span>
<span class="sd">        hook(module, input) -&gt; None or modified input</span>
<span class="sd">    The input contains only the positional arguments given to the module.</span>
<span class="sd">    Keyword arguments won&#39;t be passed to the hooks and only to the ``forward``.</span>
<span class="sd">    The hook can modify the input. User can either return a tuple or a</span>
<span class="sd">    single modified value in the hook. We will wrap the value into a tuple</span>
<span class="sd">    if a single value is returned(unless that value is already a tuple).</span>
<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.register_full_backward_hook">
    <p>def <span class="ident">register_full_backward_hook</span>(</p><p>self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]])</p>
    </div>
    

    
  
    <div class="desc"><p>Registers a backward hook on the module.</p>
<p>The hook will be called every time the gradients with respect to module
inputs are computed. The hook should have the following signature::</p>
<pre><code>hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None
</code></pre>
<p>The :attr:<code>grad_input</code> and :attr:<code>grad_output</code> are tuples that contain the gradients
with respect to the inputs and outputs respectively. The hook should
not modify its arguments, but it can optionally return a new gradient with
respect to the input that will be used in place of :attr:<code>grad_input</code> in
subsequent computations. :attr:<code>grad_input</code> will only correspond to the inputs given
as positional arguments and all kwarg arguments are ignored. Entries
in :attr:<code>grad_input</code> and :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor
arguments.</p>
<p>For technical reasons, when this hook is applied to a Module, its forward function will
receive a view of each Tensor passed to the Module. Similarly the caller will receive a view
of each Tensor returned by the Module's forward function.</p>
<p>.. warning ::
    Modifying inputs or outputs inplace is not allowed when using backward hooks and
    will raise an error.</p>
<p>Returns:
    :class:<code>torch.utils.hooks.RemovableHandle</code>:
        a handle that can be used to remove the added hook by calling
        <code>handle.remove()</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.register_full_backward_hook', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.register_full_backward_hook" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_full_backward_hook</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="s1">&#39;Module&#39;</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">],</span> <span class="n">Union</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a backward hook on the module.</span>
<span class="sd">    The hook will be called every time the gradients with respect to module</span>
<span class="sd">    inputs are computed. The hook should have the following signature::</span>
<span class="sd">        hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None</span>
<span class="sd">    The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients</span>
<span class="sd">    with respect to the inputs and outputs respectively. The hook should</span>
<span class="sd">    not modify its arguments, but it can optionally return a new gradient with</span>
<span class="sd">    respect to the input that will be used in place of :attr:`grad_input` in</span>
<span class="sd">    subsequent computations. :attr:`grad_input` will only correspond to the inputs given</span>
<span class="sd">    as positional arguments and all kwarg arguments are ignored. Entries</span>
<span class="sd">    in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor</span>
<span class="sd">    arguments.</span>
<span class="sd">    For technical reasons, when this hook is applied to a Module, its forward function will</span>
<span class="sd">    receive a view of each Tensor passed to the Module. Similarly the caller will receive a view</span>
<span class="sd">    of each Tensor returned by the Module&#39;s forward function.</span>
<span class="sd">    .. warning ::</span>
<span class="sd">        Modifying inputs or outputs inplace is not allowed when using backward hooks and</span>
<span class="sd">        will raise an error.</span>
<span class="sd">    Returns:</span>
<span class="sd">        :class:`torch.utils.hooks.RemovableHandle`:</span>
<span class="sd">            a handle that can be used to remove the added hook by calling</span>
<span class="sd">            ``handle.remove()``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_full_backward_hook</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Cannot use both regular backward hooks and full backward hooks on a &quot;</span>
                           <span class="s2">&quot;single Module. Please use only one of them.&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_is_full_backward_hook</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
    <span class="k">return</span> <span class="n">handle</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.register_parameter">
    <p>def <span class="ident">register_parameter</span>(</p><p>self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType])</p>
    </div>
    

    
  
    <div class="desc"><p>Adds a parameter to the module.</p>
<p>The parameter can be accessed as an attribute using given name.</p>
<p>Args:
    name (string): name of the parameter. The parameter can be accessed
        from this module using the given name
    param (Parameter or None): parameter to be added to the module. If
        <code>None</code>, then operations that run on parameters, such as :attr:<code>cuda</code>,
        are ignored. If <code>None</code>, the parameter is <strong>not</strong> included in the
        module's :attr:<code>state_dict</code>.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.register_parameter', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.register_parameter" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_parameter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">param</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Parameter</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Adds a parameter to the module.</span>
<span class="sd">    The parameter can be accessed as an attribute using given name.</span>
<span class="sd">    Args:</span>
<span class="sd">        name (string): name of the parameter. The parameter can be accessed</span>
<span class="sd">            from this module using the given name</span>
<span class="sd">        param (Parameter or None): parameter to be added to the module. If</span>
<span class="sd">            ``None``, then operations that run on parameters, such as :attr:`cuda`,</span>
<span class="sd">            are ignored. If ``None``, the parameter is **not** included in the</span>
<span class="sd">            module&#39;s :attr:`state_dict`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="s1">&#39;_parameters&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
            <span class="s2">&quot;cannot assign parameter before Module.__init__() call&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_six</span><span class="o">.</span><span class="n">string_classes</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;parameter name should be a string. &quot;</span>
                        <span class="s2">&quot;Got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">name</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="s1">&#39;.&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;parameter name can&#39;t contain </span><span class="se">\&quot;</span><span class="s2">.</span><span class="se">\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;parameter name can&#39;t be empty string </span><span class="se">\&quot;\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span> <span class="ow">and</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parameters</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;attribute &#39;</span><span class="si">{}</span><span class="s2">&#39; already exists&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">param</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_parameters</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;cannot assign &#39;</span><span class="si">{}</span><span class="s2">&#39; object to parameter &#39;</span><span class="si">{}</span><span class="s2">&#39; &quot;</span>
                        <span class="s2">&quot;(torch.nn.Parameter or None required)&quot;</span>
                        <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">param</span><span class="p">),</span> <span class="n">name</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">param</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Cannot assign non-leaf Tensor to parameter &#39;</span><span class="si">{0}</span><span class="s2">&#39;. Model &quot;</span>
            <span class="s2">&quot;parameters must be created explicitly. To express &#39;</span><span class="si">{0}</span><span class="s2">&#39; &quot;</span>
            <span class="s2">&quot;as a function of another Tensor, compute the value in &quot;</span>
            <span class="s2">&quot;the forward() method.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_parameters</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">param</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.requires_grad_">
    <p>def <span class="ident">requires_grad_</span>(</p><p>self: ~T, requires_grad: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Change if autograd should record operations on parameters in this
module.</p>
<p>This method sets the parameters' :attr:<code>requires_grad</code> attributes
in-place.</p>
<p>This method is helpful for freezing part of the module for finetuning
or training parts of a model individually (e.g., GAN training).</p>
<p>See :ref:<code>locally-disable-grad-doc</code> for a comparison between
<code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.</p>
<p>Args:
    requires_grad (bool): whether autograd should record operations on
                          parameters in this module. Default: <code>True</code>.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.requires_grad_', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.requires_grad_" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Change if autograd should record operations on parameters in this</span>
<span class="sd">    module.</span>
<span class="sd">    This method sets the parameters&#39; :attr:`requires_grad` attributes</span>
<span class="sd">    in-place.</span>
<span class="sd">    This method is helpful for freezing part of the module for finetuning</span>
<span class="sd">    or training parts of a model individually (e.g., GAN training).</span>
<span class="sd">    See :ref:`locally-disable-grad-doc` for a comparison between</span>
<span class="sd">    `.requires_grad_()` and several similar mechanisms that may be confused with it.</span>
<span class="sd">    Args:</span>
<span class="sd">        requires_grad (bool): whether autograd should record operations on</span>
<span class="sd">                              parameters in this module. Default: ``True``.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="n">p</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="n">requires_grad</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.set_extra_state">
    <p>def <span class="ident">set_extra_state</span>(</p><p>self, state: Any)</p>
    </div>
    

    
  
    <div class="desc"><p>This function is called from :func:<code>load_state_dict</code> to handle any extra state
found within the <code>state_dict</code>. Implement this function and a corresponding
:func:<code>get_extra_state</code> for your module if you need to store extra state within its
<code>state_dict</code>.</p>
<p>Args:
    state (dict): Extra state from the <code>state_dict</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.set_extra_state', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.set_extra_state" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">set_extra_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function is called from :func:`load_state_dict` to handle any extra state</span>
<span class="sd">    found within the `state_dict`. Implement this function and a corresponding</span>
<span class="sd">    :func:`get_extra_state` for your module if you need to store extra state within its</span>
<span class="sd">    `state_dict`.</span>
<span class="sd">    Args:</span>
<span class="sd">        state (dict): Extra state from the `state_dict`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
        <span class="s2">&quot;Reached a code path in Module.set_extra_state() that should never be called. &quot;</span>
        <span class="s2">&quot;Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md &quot;</span>
        <span class="s2">&quot;to report this bug.&quot;</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.share_memory">
    <p>def <span class="ident">share_memory</span>(</p><p>self: ~T)</p>
    </div>
    

    
  
    <div class="desc"><p>See :meth:<code>torch.Tensor.share_memory_</code></p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.share_memory', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.share_memory" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">share_memory</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;See :meth:`torch.Tensor.share_memory_`&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">share_memory_</span><span class="p">())</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.state_dict">
    <p>def <span class="ident">state_dict</span>(</p><p>self, destination=None, prefix=&#39;&#39;, keep_vars=False)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns a dictionary containing a whole state of the module.</p>
<p>Both parameters and persistent buffers (e.g. running averages) are
included. Keys are corresponding parameter and buffer names.
Parameters and buffers set to <code>None</code> are not included.</p>
<p>Returns:
    dict:
        a dictionary containing a whole state of the module</p>
<p>Example::</p>
<pre><code>&gt;&gt;&gt; module.state_dict().keys()
['bias', 'weight']
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.state_dict', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.state_dict" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">destination</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">keep_vars</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns a dictionary containing a whole state of the module.</span>
<span class="sd">    Both parameters and persistent buffers (e.g. running averages) are</span>
<span class="sd">    included. Keys are corresponding parameter and buffer names.</span>
<span class="sd">    Parameters and buffers set to ``None`` are not included.</span>
<span class="sd">    Returns:</span>
<span class="sd">        dict:</span>
<span class="sd">            a dictionary containing a whole state of the module</span>
<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; module.state_dict().keys()</span>
<span class="sd">        [&#39;bias&#39;, &#39;weight&#39;]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">destination</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">destination</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="n">destination</span><span class="o">.</span><span class="n">_metadata</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
    <span class="n">destination</span><span class="o">.</span><span class="n">_metadata</span><span class="p">[</span><span class="n">prefix</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">local_metadata</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">version</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_version</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_save_to_state_dict</span><span class="p">(</span><span class="n">destination</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">keep_vars</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(</span><span class="n">destination</span><span class="p">,</span> <span class="n">prefix</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">keep_vars</span><span class="o">=</span><span class="n">keep_vars</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">hook</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_dict_hooks</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
        <span class="n">hook_result</span> <span class="o">=</span> <span class="n">hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">destination</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">local_metadata</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">hook_result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">destination</span> <span class="o">=</span> <span class="n">hook_result</span>
    <span class="k">return</span> <span class="n">destination</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.to">
    <p>def <span class="ident">to</span>(</p><p>self, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>Moves and/or casts the parameters and buffers.</p>
<p>This can be called as</p>
<p>.. function:: to(device=None, dtype=None, non_blocking=False)
   :noindex:</p>
<p>.. function:: to(dtype, non_blocking=False)
   :noindex:</p>
<p>.. function:: to(tensor, non_blocking=False)
   :noindex:</p>
<p>.. function:: to(memory_format=torch.channels_last)
   :noindex:</p>
<p>Its signature is similar to :meth:<code>torch.Tensor.to</code>, but only accepts
floating point or complex :attr:<code>dtype</code>\ s. In addition, this method will
only cast the floating point or complex parameters and buffers to :attr:<code>dtype</code>
(if given). The integral parameters and buffers will be moved
:attr:<code>device</code>, if that is given, but with dtypes unchanged. When
:attr:<code>non_blocking</code> is set, it tries to convert/move asynchronously
with respect to the host if possible, e.g., moving CPU Tensors with
pinned memory to CUDA devices.</p>
<p>See below for examples.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Args:
    device (:class:<code>torch.device</code>): the desired device of the parameters
        and buffers in this module
    dtype (:class:<code>torch.dtype</code>): the desired floating point or complex dtype of
        the parameters and buffers in this module
    tensor (torch.Tensor): Tensor whose dtype and device are the desired
        dtype and device for all parameters and buffers in this module
    memory_format (:class:<code>torch.memory_format</code>): the desired memory
        format for 4D parameters and buffers in this module (keyword
        only argument)</p>
<p>Returns:
    Module: self</p>
<p>Examples::</p>
<pre><code>&gt;&gt;&gt; linear = nn.Linear(2, 2)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1913, -0.3420],
        [-0.5113, -0.2325]])
&gt;&gt;&gt; linear.to(torch.double)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1913, -0.3420],
        [-0.5113, -0.2325]], dtype=torch.float64)
&gt;&gt;&gt; gpu1 = torch.device("cuda:1")
&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1914, -0.3420],
        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')
&gt;&gt;&gt; cpu = torch.device("cpu")
&gt;&gt;&gt; linear.to(cpu)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1914, -0.3420],
        [-0.5112, -0.2324]], dtype=torch.float16)

&gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.3741+0.j,  0.2382+0.j],
        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)
&gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))
tensor([[0.6122+0.j, 0.1150+0.j],
        [0.6122+0.j, 0.1150+0.j],
        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)
</code></pre></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.to', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.to" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves and/or casts the parameters and buffers.</span>
<span class="sd">    This can be called as</span>
<span class="sd">    .. function:: to(device=None, dtype=None, non_blocking=False)</span>
<span class="sd">       :noindex:</span>
<span class="sd">    .. function:: to(dtype, non_blocking=False)</span>
<span class="sd">       :noindex:</span>
<span class="sd">    .. function:: to(tensor, non_blocking=False)</span>
<span class="sd">       :noindex:</span>
<span class="sd">    .. function:: to(memory_format=torch.channels_last)</span>
<span class="sd">       :noindex:</span>
<span class="sd">    Its signature is similar to :meth:`torch.Tensor.to`, but only accepts</span>
<span class="sd">    floating point or complex :attr:`dtype`\ s. In addition, this method will</span>
<span class="sd">    only cast the floating point or complex parameters and buffers to :attr:`dtype`</span>
<span class="sd">    (if given). The integral parameters and buffers will be moved</span>
<span class="sd">    :attr:`device`, if that is given, but with dtypes unchanged. When</span>
<span class="sd">    :attr:`non_blocking` is set, it tries to convert/move asynchronously</span>
<span class="sd">    with respect to the host if possible, e.g., moving CPU Tensors with</span>
<span class="sd">    pinned memory to CUDA devices.</span>
<span class="sd">    See below for examples.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Args:</span>
<span class="sd">        device (:class:`torch.device`): the desired device of the parameters</span>
<span class="sd">            and buffers in this module</span>
<span class="sd">        dtype (:class:`torch.dtype`): the desired floating point or complex dtype of</span>
<span class="sd">            the parameters and buffers in this module</span>
<span class="sd">        tensor (torch.Tensor): Tensor whose dtype and device are the desired</span>
<span class="sd">            dtype and device for all parameters and buffers in this module</span>
<span class="sd">        memory_format (:class:`torch.memory_format`): the desired memory</span>
<span class="sd">            format for 4D parameters and buffers in this module (keyword</span>
<span class="sd">            only argument)</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    Examples::</span>
<span class="sd">        &gt;&gt;&gt; linear = nn.Linear(2, 2)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.1913, -0.3420],</span>
<span class="sd">                [-0.5113, -0.2325]])</span>
<span class="sd">        &gt;&gt;&gt; linear.to(torch.double)</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.1913, -0.3420],</span>
<span class="sd">                [-0.5113, -0.2325]], dtype=torch.float64)</span>
<span class="sd">        &gt;&gt;&gt; gpu1 = torch.device(&quot;cuda:1&quot;)</span>
<span class="sd">        &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.1914, -0.3420],</span>
<span class="sd">                [-0.5112, -0.2324]], dtype=torch.float16, device=&#39;cuda:1&#39;)</span>
<span class="sd">        &gt;&gt;&gt; cpu = torch.device(&quot;cpu&quot;)</span>
<span class="sd">        &gt;&gt;&gt; linear.to(cpu)</span>
<span class="sd">        Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.1914, -0.3420],</span>
<span class="sd">                [-0.5112, -0.2324]], dtype=torch.float16)</span>
<span class="sd">        &gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)</span>
<span class="sd">        &gt;&gt;&gt; linear.weight</span>
<span class="sd">        Parameter containing:</span>
<span class="sd">        tensor([[ 0.3741+0.j,  0.2382+0.j],</span>
<span class="sd">                [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)</span>
<span class="sd">        &gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))</span>
<span class="sd">        tensor([[0.6122+0.j, 0.1150+0.j],</span>
<span class="sd">                [0.6122+0.j, 0.1150+0.j],</span>
<span class="sd">                [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">,</span> <span class="n">convert_to_format</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">_parse_to</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_floating_point</span> <span class="ow">or</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;nn.Module.to only accepts floating point or complex &#39;</span>
                            <span class="s1">&#39;dtypes, but got desired dtype=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dtype</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Complex modules are a new feature under active development whose design may change, &quot;</span>
                <span class="s2">&quot;and some modules might not work as expected when using complex tensors as parameters or buffers. &quot;</span>
                <span class="s2">&quot;Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md &quot;</span>
                <span class="s2">&quot;if a complex module does not work as expected.&quot;</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">convert</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">convert_to_format</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">t</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">t</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="ow">or</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
                        <span class="n">non_blocking</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">convert_to_format</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">t</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="ow">or</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="n">convert</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.to_empty">
    <p>def <span class="ident">to_empty</span>(</p><p>self: ~T, *, device: Union[str, torch.device])</p>
    </div>
    

    
  
    <div class="desc"><p>Moves the parameters and buffers to the specified device without copying storage.</p>
<p>Args:
    device (:class:<code>torch.device</code>): The desired device of the parameters
        and buffers in this module.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.to_empty', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.to_empty" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">to_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves the parameters and buffers to the specified device without copying storage.</span>
<span class="sd">    Args:</span>
<span class="sd">        device (:class:`torch.device`): The desired device of the parameters</span>
<span class="sd">            and buffers in this module.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">))</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.train">
    <p>def <span class="ident">train</span>(</p><p>self: ~T, mode: bool = True)</p>
    </div>
    

    
  
    <div class="desc"><p>Sets the module in training mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>,
etc.</p>
<p>Args:
    mode (bool): whether to set training mode (<code>True</code>) or evaluation
                 mode (<code>False</code>). Default: <code>True</code>.</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.train', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.train" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets the module in training mode.</span>
<span class="sd">    This has any effect only on certain modules. See documentations of</span>
<span class="sd">    particular modules for details of their behaviors in training/evaluation</span>
<span class="sd">    mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,</span>
<span class="sd">    etc.</span>
<span class="sd">    Args:</span>
<span class="sd">        mode (bool): whether to set training mode (``True``) or evaluation</span>
<span class="sd">                     mode (``False``). Default: ``True``.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;training mode is expected to be boolean&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="o">=</span> <span class="n">mode</span>
    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">():</span>
        <span class="n">module</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.type">
    <p>def <span class="ident">type</span>(</p><p>self: ~T, dst_type: Union[torch.dtype, str])</p>
    </div>
    

    
  
    <div class="desc"><p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Args:
    dst_type (type or string): the desired type</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.type', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.type" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">type</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">dst_type</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Casts all parameters and buffers to :attr:`dst_type`.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Args:</span>
<span class="sd">        dst_type (type or string): the desired type</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">dst_type</span><span class="p">))</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.xpu">
    <p>def <span class="ident">xpu</span>(</p><p>self: ~T, device: Union[int, torch.device, NoneType] = None)</p>
    </div>
    

    
  
    <div class="desc"><p>Moves all model parameters and buffers to the XPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on XPU while being optimized.</p>
<p>.. note::
    This method modifies the module in-place.</p>
<p>Arguments:
    device (int, optional): if specified, all parameters will be
        copied to that device</p>
<p>Returns:
    Module: self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.xpu', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.xpu" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">xpu</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves all model parameters and buffers to the XPU.</span>
<span class="sd">    This also makes associated parameters and buffers different objects. So</span>
<span class="sd">    it should be called before constructing optimizer if the module will</span>
<span class="sd">    live on XPU while being optimized.</span>
<span class="sd">    .. note::</span>
<span class="sd">        This method modifies the module in-place.</span>
<span class="sd">    Arguments:</span>
<span class="sd">        device (int, optional): if specified, all parameters will be</span>
<span class="sd">            copied to that device</span>
<span class="sd">    Returns:</span>
<span class="sd">        Module: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">xpu</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="seq2seq.Seq2Seq.zero_grad">
    <p>def <span class="ident">zero_grad</span>(</p><p>self, set_to_none: bool = False)</p>
    </div>
    

    
  
    <div class="desc"><p>Sets gradients of all model parameters to zero. See similar function
under :class:<code>torch.optim.Optimizer</code> for more context.</p>
<p>Args:
    set_to_none (bool): instead of setting to zero, set the grads to None.
        See :meth:<code>torch.optim.Optimizer.zero_grad</code> for details.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-seq2seq.Seq2Seq.zero_grad', this);">Show source &equiv;</a></p>
  <div id="source-seq2seq.Seq2Seq.zero_grad" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">set_to_none</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets gradients of all model parameters to zero. See similar function</span>
<span class="sd">    under :class:`torch.optim.Optimizer` for more context.</span>
<span class="sd">    Args:</span>
<span class="sd">        set_to_none (bool): instead of setting to zero, set the grads to None.</span>
<span class="sd">            See :meth:`torch.optim.Optimizer.zero_grad` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_is_replica&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;Calling .zero_grad() from a module created with nn.DataParallel() has no effect. &quot;</span>
            <span class="s2">&quot;The parameters are copied (in a differentiable manner) from the original module. &quot;</span>
            <span class="s2">&quot;This means they are not leaf nodes in autograd and so don&#39;t accumulate gradients. &quot;</span>
            <span class="s2">&quot;If you need gradients in your forward method, consider using autograd.grad instead.&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">set_to_none</span><span class="p">:</span>
                <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">grad_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">detach_</span><span class="p">()</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
                <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</pre></div>

  </div>
</div>

  </div>
  
      </div>
      </div>

  </section>
</article>
  <div class="clear"> </div>
  <footer id="footer">
    <p>
      Generated by <a href="https://github.com/timothycrosley/pdocs">pdocs 1.1.1</a>
    </p>
  </footer>
</div>
</body>
</html>